{
  "hash": "a079cc2f44028499824d601240506be8",
  "result": {
    "markdown": "---\ntitle: 'Fitness Tracker Modeling: ML'\nauthor: Mark Edney\ndate: '2022-01-29'\ncategories:\n  - Project\n  - GGPlot\n  - ML\n  - R\ndraft: false\ndescription: 'An analysis of Data collected by Fitness Trackers'\nimage: \"MachineLearningProject.png\"\ntoc: true\narchives:\n  - 2022/01\n---\n\n\n![](MachineLearningProject.png)\n\n> The original paper was written on 12/18/2020\n\n\n\n\n\n# Executive Summary\n\nThis report analyzes collected data on different users preforming barbell lifts performed at different levels of quality. A machine learning algorithm was used to create a model to determine the user's rating based on data collected from multiple accelerometers. More information on the project can be found [here](https://web.archive.org/web/20161224072740/https:/groupware.les.inf.puc-rio.br/har).\n\n# Analysis\n\n## Initialization\n\nThe following code was used to initialize the required R libraries, as well as downloading the required data and store it into memory. There are some columns of the data that were not required for modelling which were excluded (ex. usernames).\n\n\n::: {.cell hash='index_cache/html/Inialization_ffc808cebaf7b94c9142cda0913ad188'}\n\n```{.r .cell-code}\nlibrary(caret)\nlibrary(gbm)\nlibrary(dplyr)\nlibrary(randomForest)\nlibrary(ggplot2)\nset.seed(90210)\nNtree <- 200\n\ndownload.file(\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\", \"training.csv\")\ndownload.file(\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\", \"testing.csv\")\ntrain <- read.csv2(\"training.csv\", sep = \",\")[,-c(1:7)]\ntest <- read.csv2(\"testing.csv\", sep = \",\")[,-c(1:7)]\n```\n:::\n\n\n## Reducing predictors\n\nThe data contains way too many predictors (153 in total) to produce accurate and simple models. Some trimming is required. The first trim is performed with the near zero variance function from the caret library, which finds the predictors that exhibit near zero variation. These predictors would add little benefit to include in models.\n\n\n::: {.cell hash='index_cache/html/nearzero_a6712d2c2a3b3a0cd88341c59b700c22'}\n\n```{.r .cell-code}\nnz <- nearZeroVar(train)\ntrain <- train[,-nz]\ntest <- test[-nz]\n```\n:::\n\n\nFrom this step, the number of predictors is reduced to 94. There remains numerous NA values in the data. These values are examined in the next chunk of code.\n\n\n::: {.cell hash='index_cache/html/NA_c4f7747344573ee86cf382cc32b81926'}\n\n```{.r .cell-code}\nmaxi <- length(train) - 1\nvalna <- 1:maxi\n\nfor (i in 1:maxi) {\n        train[,i] <- as.numeric(train[,i])\n        test[,i] <- as.numeric(test[,i])\n        valna[i] <- mean(is.na(train[,i]))\n}\n\ntable(valna)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nvalna\n                0 0.979308938946081 \n               52                41 \n```\n:::\n:::\n\n\nThe code shows that there are 52 predictors that have no missing data and 41 predictors that are mostly missing values. These predictors would add little value to the modelling and are removed with the following code\n\n\n::: {.cell hash='index_cache/html/NA2_3639e16a12979195546b22d8fbdd8231'}\n\n```{.r .cell-code}\ntrain <- train[, valna == 0]\ntest <- test[, valna == 0]\n```\n:::\n\n\nThe training was then divided to create a validation set which will be used for cross validation. Note that the random forest algorithm has built in cross validation with the \"out of bag error\". About 1/3 of the data is used in a random forest.\n\n\n::: {.cell hash='index_cache/html/crossvalid_14f5ec43d1fcea1a6b5c385fb9c929ab'}\n\n```{.r .cell-code}\nValid <- createDataPartition(train$classe, p = 0.3)[[1]]\nvalid <- train[Valid,]\ntrain <- train[-Valid,]\n```\n:::\n\n\nThe next step is to utilize the variable of importance function in the caret library to reduce the number of predictors even further. The train data is still very large, but by making a sample set from the training data and modelling from that we can get a reasonable approximation of the variables of importance.\n\n\n::: {.cell hash='index_cache/html/sample_5003a0dc2cf31d37cd39b68e9cbc9ce8'}\n\n```{.r .cell-code}\nstrain <- rbind(sample_n(train[train$classe == \"A\",],round(mean(train$classe == \"A\")*200,0)),\n                sample_n(train[train$classe == \"B\",],round(mean(train$classe == \"B\")*200,0)),\n                sample_n(train[train$classe == \"C\",],round(mean(train$classe == \"C\")*200,0)),\n                sample_n(train[train$classe == \"D\",],round(mean(train$classe == \"D\")*200,0)),\n                sample_n(train[train$classe == \"E\",],round(mean(train$classe == \"E\")*200,0))\n)\n```\n:::\n\n\nThe sample set was set to ensure an accurate representation of the 'classe' variable in the training data. Two models were completed and their variables of importance were added together.\n\n\n::: {.cell hash='index_cache/html/varimp_ad881b2f02fd3558ec682d5b6297afb1'}\n\n```{.r .cell-code}\nmdl1 <- train(classe~., data = strain, method = \"rf\", ntree = Ntree)\nmdl2 <- train(classe~., data = strain, method = \"gbm\", verbose = FALSE)\nvar <- varImp(mdl1)$importance > 50 | varImp(mdl2)$importance > 50\nvarorder <- order(varImp(mdl1)$importance, decreasing = TRUE)\nVarimp <- row.names(varImp(mdl1)$importance)[varorder[1:2]]\n```\n:::\n\n\nA value of 50 was used for a cut-off value. The total number of predictors has been reduced to 4.\n\n\n::: {.cell hash='index_cache/html/datareduce_9ae4382d7a5a68a6b8d2e0e1f56fe934'}\n\n```{.r .cell-code}\nvalid <- valid[,var]\ntrain <- train[,var] %>% slice_sample(prop = 0.75)\ntest <- test[,var]\n```\n:::\n\n\n## Modelling\n\nWith the reduced predictors, the models can now be trained. Since these model will look at the entire training data set, it will require a lot of time. The models include:\\\n- Random forest\\\n- Generalized Boosted\\\n- Linear Discriminant\\\n- Combined\\\nThe randomForest function is used as it is more efficient than the train function. The data method is also more efficient than using the formula method.\n\n\n::: {.cell hash='index_cache/html/models_f82d0f0f59eca8aa89975baaf0e8300b'}\n\n```{.r .cell-code}\nmdl11 <- randomForest(x = train[,1:(ncol(train) - 1)], y = as.factor(train[,ncol(train)]), ntree = Ntree, proximity = TRUE)\nmdl21 <- train(classe~., data = train, method = \"gbm\", verbose = FALSE)\nmdl31 <- train(classe~., data = train, method = \"lda\")\n```\n:::\n\n\nThe following code constructs the combined model\n\n\n::: {.cell hash='index_cache/html/combined_cb68ad1b87cdf3b1903a1d502edd8f4d'}\n\n```{.r .cell-code}\npmdl11 <- predict(mdl11, valid)\npmdl21 <- predict(mdl21, valid)\npmdl31 <- predict(mdl31, valid)\njoin <- data.frame(pmdl11, pmdl21, pmdl31, classe = valid$classe)\njmdl <- randomForest(x = join[,1:3], y = as.factor(join$classe), ntree = Ntree)\n```\n:::\n\n\n## Model Evaluation\n\nThe new models will need to be examined against the validation data set. The out of bag error for the random forest models were not used, as the validation data provides a uniform comparison for all models. The following function was used to test the models:\n\n\n::: {.cell hash='index_cache/html/test_c651427ddd0ea799079df7c0f891ab60'}\n\n```{.r .cell-code}\nExacc <- function(mdl, test){\n        mean(predict(mdl,test) == test$classe)\n}\n```\n:::\n\n\nThe model's accuracy are summarized in the following dataframe when they are used to predict the results in the validation set:\n\n\n::: {.cell hash='index_cache/html/results_6f307b50738e9896d9c477971e339611'}\n::: {.cell-output .cell-output-stdout}\n```\n  Model  accuracy\n1 mdl11 0.8935303\n2 mdl21 0.7989472\n3 mdl31 0.3625403\n4 joint 0.8897945\n```\n:::\n:::\n\n\n# Conclusion\n\nFrom the results from the model testing, it is clear that the random forest and the combined are the most accurate models for the validation set. The combined model has approximately the same level of accuracy as the random forest, meaning it is the most heavily weighted model. It also means that the inclusion of the boosted and linear discriminant models do not contribute to its accuracy. For simplification, the random forest model is the best model.\n\n# Plot\n\nThe centres of the model can be found from the proximity data. The proximity data is compared to two predictors, the most important predictors. The two most significant predictors would sometimes vary, so the code was changed to be flexible to it.\n\n\n::: {.cell hash='index_cache/html/centers_6239bd018a2d411004b43e9d03c6da91'}\n\n```{.r .cell-code}\nindex <- names(train) %in% Varimp\nmdlp <- classCenter(train[index], train$classe, mdl11$proximity)\nmdlp <- as.data.frame(mdlp)\nmdlp$classe <- rownames(mdlp)\n```\n:::\n\n\nThis centre data can be included with the training data. There are distinctly different regions based off of the different classe values, but the other predictors also contribute to model accuracy.\n\n\n::: {.cell hash='index_cache/html/plot_b5a7de9d1033907f23af74e232808694'}\n\n```{.r .cell-code}\nindex <- names(train) %in% Varimp\nnames <- names(train[index])\n\nf <- function(name1, name2){\n        xval <- sym(name1)\n        yval <- sym(name2)\n        ggplot(data = train, aes_string(x = xval, y = yval, col = \"classe\")) +\n                geom_point() +\n                geom_point(aes_string(x = xval, y = yval, col = \"classe\"), size = 10, shape = 4, data = mdlp) +\n                labs(title = \"Model centers on two variables of importance\")\n        }\nf(names[1], names[2])\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}