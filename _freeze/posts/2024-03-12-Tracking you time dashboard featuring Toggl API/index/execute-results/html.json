{
  "hash": "b4670db9bb77193133ab4c641add8c2c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Tracking you time dashboard featuring Toggl API\nauthor: Mark Edney\ndate: 2024-03-12\ncategories: [How-to,Python,Dashboard]\ndraft: true\ndescription: 'A dashboard application that connects to the Toggl API to display tracked time.'\nimage: ''\narchives:\n  - 2024/03\ntoc: false\nengine: jupyter\nexecute:\n   enabled: true\nformat:\n  html:\n    code-fold: show\n    code-tools: true\n---\n\n# Introduction\n\nDo you lead a busy life? It sure feels like it, but how do you truly\nknow how you spend your time? Would you like to focus your time on\nspecific projects or sepcific tasks?\n\nThe solution is to start tracking you time. I personally, started\ntracking my time almost five years ago when I had multiple jobs. I was a\nfull-time graduate student with a part-time job. This created a demand\non my time and I needed to manage my time well.\n\nI learnt about Toggl, a free easy to use time tracking application. It\nis simple to use on android with additional feature on their website.\n\nI hadn't really done anything with the data I created tracking my time,\nso I thought it would be a great opportunity to learn how to create a\ndashboard in `python`.\n\n# Inialization\n\nAs always, we start with loading our python packages. There isn't really\nanything out of the ordinary, we will use the `requests` library to make\nour API call. You will need to create an account at the Toggl website\nand create a API key.\n\nI've saved the API key in my environmental variables, it is loaded into\nthe script with the `os.environ` function. You also need to create a\nheader for the API call, this is the part that caused the most\ndifficulty for me as it is very specific in the format.\n\n::: {#00fe8ac2 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport os\nimport base64\nimport requests\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nkey = os.environ['Toggl']\n\nstring=key+':api_token'\nheaders={\n    'Authorization':'Basic '+base64.b64encode(string.encode('ascii')).decode('utf-8'),\n    \"Content-Type\": \"application/json\"   \n}\n```\n:::\n\n\n# Toggl API\n\nThis section goes more into the details for the API. The call accepts\nthe following parameters: `since` for your starting date, `until` for\nyou finishing date, `user_agent` this is your user account email and\n`workspace_id` which is found on the Toggl website when you click on\nyour workspace. I have kept mine secret, you will need to replace them\nwith your own values.\n\n::: {#c4d366b1 .cell execution_count=2}\n``` {.python .cell-code}\nparams = {\n    'since':'2020-01-01',\n    'until':'2020-12-31',\n    'user_agent': 'email',\n    'workspace_id' : \"workspace\"\n}\n```\n:::\n\n\n\n\nWith the header and parameters for the API call setup, we can now use\nthe requests package to make the actual call. The API call returns a\njson document, for ease I will convert that json into a dataframe. We\ncan than analyze what the API call has returned.\n\n::: {#2dd9ebca .cell execution_count=4}\n``` {.python .cell-code}\nresponse = requests.get('https://api.track.toggl.com/reports/api/v2/details', headers = headers, params= params)\nmy_json = json.loads(response.content)\ndf = pd.DataFrame(my_json['data'])\nprint(df.columns, df.head(5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIndex(['id', 'pid', 'tid', 'uid', 'description', 'start', 'end', 'updated',\n       'dur', 'user', 'use_stop', 'client', 'project', 'project_color',\n       'project_hex_color', 'task', 'billable', 'is_billable', 'cur', 'tags'],\n      dtype='object')            id          pid   tid      uid    description  \\\n0  1825387165  162435592.0  None  4504525  Data Products   \n1  1825095042  162435592.0  None  4504525  Data Products   \n2  1824585854  150042491.0  None  4504525        Commute   \n3  1824350140  150042504.0  None  4504525   Walmart work   \n4  1824350131  150042491.0  None  4504525        Commute   \n\n                       start                        end  \\\n0  2020-12-31T20:26:49-05:00  2020-12-31T20:44:21-05:00   \n1  2020-12-31T16:56:40-05:00  2020-12-31T17:21:47-05:00   \n2  2020-12-30T22:55:00-05:00  2020-12-31T23:30:00-05:00   \n3  2020-12-30T14:24:04-05:00  2020-12-30T22:55:48-05:00   \n4  2020-12-30T14:24:02-05:00  2020-12-30T14:24:04-05:00   \n\n                     updated       dur     user  use_stop client  \\\n0  2021-01-01T20:08:38-05:00   1052000  M2edney     False   None   \n1  2020-12-31T20:26:49-05:00   1507000  M2edney     False   None   \n2  2020-12-31T01:06:18-05:00  88500000  M2edney     False   None   \n3  2020-12-30T22:55:49-05:00  30704000  M2edney     False   None   \n4  2020-12-30T14:24:05-05:00      2000  M2edney     False   None   \n\n              project project_color project_hex_color  task billable  \\\n0  Data Science Study             0           #9e5bd9  None     None   \n1  Data Science Study             0           #9e5bd9  None     None   \n2             Commute             0           #c7af14  None     None   \n3             Walmart             0           #d94182  None     None   \n4             Commute             0           #c7af14  None     None   \n\n   is_billable   cur tags  \n0        False  None   []  \n1        False  None   []  \n2        False  None   []  \n3        False  None   []  \n4        False  None   []  \n```\n:::\n:::\n\n\nThe issue with this method is that it only returns a limited number of\nvalues. We can see it only returned 50 time entries.\n\n::: {#acf57515 .cell execution_count=5}\n``` {.python .cell-code}\nprint(len(df))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n50\n```\n:::\n:::\n\n\nTo collect all entries within the time frame, we need to try something a\nlittle different. The json file does include two important attributes, the total count\nand the number of entries per page. With these two values, we can loop through all \nthe pages of entries and concatenate them into a single dataframe. The previous attempt\nalready retrieved the first page so we can start at the second. I also included some\nsimple data cleanup to remove null entries and change the duration values into hrs. \n\n::: {#9743fed2 .cell execution_count=6}\n``` {.python .cell-code}\npage_count = int(my_json['total_count']/my_json['per_page']) + (my_json['total_count'] % my_json['per_page'] > 0)\nfor page in range(1, page_count):\n    params['page']= str(page)\n    response = requests.get('https://api.track.toggl.com/reports/api/v2/details', headers = headers, params= params)\n    my_json = json.loads(response.content)\n    df =pd.concat([df, pd.DataFrame(my_json['data'])])\n    \ndf['dur'] = df['dur']/1000/60/60\ndf = df[~df['project'].isnull()]\nprint(len(df))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n863\n```\n:::\n:::\n\n\n# Simple Graphics\n\nBefore creating any complicated graphics for our dashboard, we should make a simple\ngraphics. In this graph we total the durations and split them by their labeled \nproject. I use the `sns.barplot` from the`seaborn` library because it is well designed\nand simple to use. \n\n::: {#5a45d43c .cell execution_count=7}\n``` {.python .cell-code}\nsns.barplot(x='project', y= 'dur', data = df, estimator = 'sum', hue= 'project')\nplt.xticks(rotation=45)\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```\n([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n [Text(0, 0, 'Data Science Study'),\n  Text(1, 0, 'Commute'),\n  Text(2, 0, 'Walmart'),\n  Text(3, 0, 'Job Seeking'),\n  Text(4, 0, 'Thesis work'),\n  Text(5, 0, 'CHE215'),\n  Text(6, 0, 'CHE615'),\n  Text(7, 0, 'Mentoring'),\n  Text(8, 0, 'CHE616'),\n  Text(9, 0, 'CE8201'),\n  Text(10, 0, 'Writing course'),\n  Text(11, 0, 'Administrative')])\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-2.png){width=619 height=518}\n:::\n:::\n\n\n# Conclusion\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}