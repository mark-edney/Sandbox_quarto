{
  "hash": "525ca582a14e92b938209b2a9e1f5ab0",
  "result": {
    "markdown": "---\ntitle: Job posting analysis\nauthor: Mark Edney\ndate: '2022-02-06'\ncategories:\n  - Project\n  - R\n  - Shiny App\ndraft: false\ndescription: A project used to study the occurance of keywords in a job posting.\nimage: jobsearch.jpg\ntoc: true\narchives:\n  - 2022/01\n---\n\n![](jobsearch.jpg)\n\nRecently, there was a post on medium about the use of Natural Language Processing (NLP) to study a job posting for keywords. I found that this article was very similar to R shiny App that I created a while ago. [^1]\n\n[^1]: [use-python-and-nlp-to-boost-your-resume](https://medium.com/data-marketing-philosophy/use-python-and-nlp-to-boost-your-resume-e4691a58bcc9)\n\n## Introduction\n\nTechnology has changed the job application process, making it easier and quicker to apply to jobs. As a result, the average job posting will receive around 250 resumes. [^2] So how can hiring managers handle spending their time looking through that many resumes for one posting? That's easy, they cheat.\n\n[^2]: [Resume Screening: A How-To Guide For Recruiters](https://ideal.com/resume-screening/)\n\nHiring Managers no longer look at individual resumes, but use automatic software called applicant tracking system (ATS). These programs filter resumes by a set of keywords, reducing the amount of resumes to a more manageable amount. So how can a job applicant make sure their resume is looked at? Well, they should cheat.\n\nThe medium article I mentioned uses Python and Natural Language Processing (NLP) to skim through the job posting to look for the most common words used. This is useful information, but not necessarily the keywords used by the ATS software. I propose the use of an R Shiny App to filter a job posting by a list of common keywords.\n\nAn R Shiny App is an interactive web based application that runs R code. The syntax for a Shiny App is a little different from R and requires some additional understanding. The product will be a basic, interactive program that can be hosted online. One free Shiny App hosting site that I recommend is [shinyapps.io](https://www.shinyapps.io).\n\n## Inialization\n\nThe shiny App will require the following libraries.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(shiny)\nlibrary(wordcloud2)\nlibrary(tidyverse)\nlibrary(XML)\nlibrary(rvest)\nlibrary(tidytext)\n```\n:::\n\n\nThe Shiny App will use a csv files which contains a set of keywords that ATS will look for. This list was found online, but I have modified by adding additional keywords as I see fit. The file can be downloaded [here](https://github.com/mark-edney/Resumes/blob/master/Word_Cloud/Keywords.csv) from my GitHub site. Here is a sample of some keywords:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nKeywords <- read_csv(\"Keywords.csv\") \nKeywords$Keys %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \".NET\"                \"account management\"  \"accounting\"         \n[4] \"accounts payable\"    \"accounts receivable\" \"acquisition\"        \n```\n:::\n:::\n\n\n## App Structure\n\nOne issue I found when developing this application was the use of keywords that are a combination of multiple words. This creates some complications, as a simple search of keywords would use only the first word and lose the context.\n\nThis challenge was met by breaking the website down into ngrams. An over simplification of a ngram is a group of n number of words. Wikipedia has a very good page that better explains ngrams.[^3] The website can then be split into ngrams of different lengths and the keywords searched for.\n\n[^3]: [Wiki: ngrams](https://en.wikipedia.org/wiki/N-gram)\n\nAs a example, the phrase:\n\n> The quick brown fox\n\nfor a ngram of length 1 would return:\n\n> (The) (quick) (brown) (fox)\n\nfor a ngram of length 2 would return:\n\n> (The quick) (quick brown) (brown fox)\n\nand for a ngram of length 3 would return:\n\n> (The quick brown) (quick brown fox)\n\n## Application Coding\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshinyApp(\n#This is the standard format for a shiny app\n        \n#The UI function controls all the frontend for the app\n        ui = fluidPage(\n                titlePanel(\"Job Posting Word Cloud\"),\n                sidebarLayout(\n                        sidebarPanel(\n#The user is asked for a url\n                                textInput(\"url\", \"input URL\", value = \"https://www.google.com/\")\n                                ),\n                        mainPanel(\n#The word cloud plot is displayed\n                                h4(\"Key-Word Cloud\"),\n                                wordcloud2Output(\"plot\")\n                                )\n                        )\n                ),\n        \n#The server function controls the backend for the app\n        server = function(input, output){\n                \n#The keywords are loaded and an index of how many words per keyword is created\n                Keywords <- read_csv(\"Keywords.csv\")\n                Keywords$Keys <- str_to_lower(Keywords$Keys) \n                index <- Keywords$Keys %>% str_count(\" \")\n                \n#The { brackets are used to create reactive functions which continuously run \n                data <- reactive({\n#The input variable is how the server side receives data from the ui side\n                url <- input$url\n#The text is read from the url provide by the user\n                data <- text %>%\n                        data.frame(text = .)\n#Since there are ngrams of length 1-3, there a three search's that are concatenated together\n                rbind(data %>%\n#the unnest_tolkens from the tidytext library is used to create the ngrams\n                              unnest_tokens(word, text, token = \"ngrams\", n= 1) %>%\n#A count is performed on each ngram in the website to find the most common ngrams \n                              count(word, name = 'freq', sort = TRUE) %>%\n#The ngram count is then filtered by the keywords of the same ngram length\n                              filter(word %in% Keywords$Keys[index == 0]),\n#The steps are repeated for bigrams (ngrams of length 2) and trigrams(ngrams of length 3)\n                      data %>%\n                              unnest_tokens(word, text, token = \"ngrams\", n= 2) %>%\n                              count(word, name = 'freq', sort = TRUE) %>%\n                              filter(word %in% Keywords$Keys[index == 1]),\n                      data %>%\n                              unnest_tokens(word, text, token = \"ngrams\", n= 3) %>%\n                              count(word, name = 'freq', sort = TRUE) %>%\n                              filter(word %in% Keywords$Keys[index == 2]))\n                        })\n                \n#The plot/wordcloud needs to be saved as an output value\n#The output variable is how the server sends data back to the UI\n                output$plot <- renderWordcloud2({\n#One part of the strange syntax of a shiny app is that the since the data is reactive\n#and changes with the user input, it is passed in a function so it needs to be called\n#as data ()\n                        wordcloud2(data())\n                        })\n        },\n\n  options = list(height = 500)\n)\n```\n:::\n\n\n## Shiny App\n\n\n::: {.cell}\n<iframe src=\"https://m2edney.shinyapps.io/Word_Cloud/?_ga=2.82297238.1842982076.1647479103-1341333380.1645206372\" width=\"672\" height=\"400px\" data-external=\"1\"></iframe>\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}