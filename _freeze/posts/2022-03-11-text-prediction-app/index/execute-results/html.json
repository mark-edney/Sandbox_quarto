{
  "hash": "22f0e7403ed3249770a4d06656a63290",
  "result": {
    "markdown": "---\ntitle: Text Prediction Shiny App pt 1\nauthor: Mark Edney\ndate: '2022-05-31'\nslug: []\ncategories:\n  - Project\n  - Shiny App\n  - NLP\n  - R\ndraft: false\ndescription: 'A predictive Text Shiny Application'\nimage: \"predict_text.jpg\"\narchives:\n  - 2022/05\nlatex: true\ntoc: true\n---\n\n\n![](predict_text.jpg)\n\n> This Shiny App was first written in May of 2021\n\n## Description\n\nThe goal of this project was to create an N-gram based model to predict the word to follow the user's input. This project was to complete the Capstone project for the Johns Hopkins University Data science program on Coursera. The data for this project was provided by Swiftkey.\n\nThis project will be broken down to multiple parts as the entire project is quite large. The first part will deal with the creation of the corpus. This corpus will require additional filtering to remove words that are not English, contractions and words that are considered profanity.\n\n## Initialization\n\nThe initial step that loads the required libraries and downloads the data sets if not all read on file.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(pryr)\n\n#downloads the corpus files, profanity filter and English dictionary\n\nurl <- \"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip\"\nurl2 <- \"https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip\"\nurl3 <- \"https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt\"\nurl4 <- \"https://raw.githubusercontent.com/mark-edney/Capestone/1c143b40dd71f0564c3248df2a8638d08af10440/data/contractions.txt\"\n\n# I have added this if statement for testing, if the files are found than they will not be downloaded again\nif(dir.exists(\"~/R/Capestone/data/\") == FALSE){\n       dir.create(\"~/R/Capestone/data/\")}\n\nif(file.exists(\"~/R/Capestone/data/data.zip\") == FALSE|\n   file.exists(\"~/R/Capestone/data/prof.zip\")==FALSE|\n   file.exists(\"~/R/Capestone/data/diction.txt\")==FALSE|\n    file.exists(\"~/R/Capestone/data/contractions.txt\")==FALSE){\n        download.file(url,destfile = \"~/R/Capestone/data/data.zip\")\n        download.file(url2,destfile = \"~/R/Capestone/data/prof.zip\")\n        download.file(url3,destfile = \"~/R/Capestone/data/diction.txt\")\n        download.file(url4,destfile = \"~/R/Capestone/data/contractions.txt\")\n        setwd(\"~/R/Capestone/data/\")\n        unzip(\"~/R/Capestone/data/prof.zip\")\n        unzip(\"~/R/Capestone/data/data.zip\")\n        setwd(\"~/R/Capestone\")\n}\n```\n:::\n\n\n## Creating a Corpus\n\nThe project requires a Corpus, or a large body of text, to create models. At this stage, the files are opened and joined. The Corpus is so large and requires so much ram that a sample of 10% is taken.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblog <- read_lines(\"~/R/Capestone/data/final/en_US/en_US.blogs.txt\")\nnews <- read_lines(\"~/R/Capestone/data/final/en_US/en_US.news.txt\")\ntwitter <- read_lines(\"~/R/Capestone/data/final/en_US/en_US.twitter.txt\")\nblog <- tibble(text = blog) \nnews <- tibble(text = news)\ntwitter <- tibble(text = twitter)\n\nset.seed(90210)\ncorpus <- bind_rows(blog,twitter,news) %>% \n        slice_sample(prop = 0.10) %>%\n        mutate(line = row_number())\n```\n:::\n\n\n## Corpus filtering\n\nHere, the corpus filter is created to remove profanity and any word that is not in the English dictionary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprof <- read_lines(\"~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt\")[15]\nprof <- prof %>% \n        str_split(\", \") %>% \n        flatten() %>% \n        unlist()\nprof <- tibble(\"word\" = prof)\n\nenglish <- read_lines(\"~/R/Capestone/data/diction.txt\")\nenglish <- tibble(\"word\" = english[!english==\"\"])\n\ncontract <- read_lines(\"~/R/Capestone/data/contractions.txt\")\ncontract <- tibble(\"word\" = contract) %>% unnest_tokens(word,word)\n```\n:::\n\n\n## Vocabulary\n\nA vocabulary of words is created from the unique words with the applied filters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#clean up ram\nrm(blog,news,twitter)\nvoc <- bind_rows(english, contract) %>% anti_join(prof)\n\nunigram <- corpus %>% unnest_tokens(ngram, text, token = \"ngrams\", n = 1) %>%\n        semi_join(voc, by = c(\"ngram\"=\"word\")) \n#decreases the voc size\nvoc <- tibble(word = unique(unigram$ngram))\n```\n:::\n\n\n## Corpus Exploration\n\n\n\n\n\nNow that the corpus is created, we can do some exploration into the text. There are some lines of text that have some odd behaviour, but on the whole it mostly makes sense.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus %>% \n        head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  text                                                                      line\n  <chr>                                                                    <int>\n1 no we don't just kidding yes we do d                                         1\n2 it sounds like a man walking on snow but it's my heartbeat cara on how …     2\n3 they thought about it but you're too short                                   3\n4 indeed dear to my heart want to go see                                       4\n5 i know its a tough world out there the secret to succeeding isn't stepp…     5\n6 we're wishing for the cure too good luck                                     6\n```\n:::\n:::\n\n\n## Vocabulary Exploration\n\nBy using the `arrange` function, we can sort the unigrams by their counts. This provides some insight on which words come up the most frequently. It is not surprisingly that the most common word is \"the\". These frequencies will play an important role in the test prediction, so it is important to consider them. It is very common to filter out \"Stop Words\" as they likely add little value to predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunigram %>%\n        arrange(desc(n)) %>%\n        head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  ngram      n\n  <chr>  <int>\n1 the   476750\n2 to    277081\n3 and   242032\n4 a     238301\n5 of    201539\n6 in    165645\n```\n:::\n:::\n\n\n> Photo by [Sandy Millar](https://unsplash.com/@sandym10?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/predictive-text?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}