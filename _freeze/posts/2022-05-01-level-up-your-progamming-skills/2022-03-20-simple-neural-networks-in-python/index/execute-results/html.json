{
  "hash": "d6cb0136c8a605cde0c0334ee3dac0dd",
  "result": {
    "markdown": "---\ntitle: Simple Neural Networks in Python\nauthor: Mark Edney\ndate: '2022-03-20'\nslug: []\ncategories: \n  - How-to\n  - Python\n  - NN\ndraft: false\ndescription: An outline for creating a simple Neural Network in Python\nimage: 'network.jpg'\narchives: 2022/03\ntoc: true\n---\n\n![](network.jpg) Neural Networks (NN) have become incredibly popular due to their high level of accuracy. The creation of a NN can be complicated and have a high level of customization. I wanted to explore just the simplest NN that you could create. A framework as a workhorse for developing new NN.\n\nThe `SciKitlearn` provides the easiest solution with the Multi-Layer Perceptron series of functions. It doesn't provide a bunch of the more advanced features of `TensorFlow`, like GPU support, but that is not what I'm looking for.\n\n## Initialization\n\nFor the demonstration, I decided to use a data set on faults found in [steel plates](https://www.openml.org/d/1504) from the OpenML website. The data set includes 27 features with 7 binary predictors.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv('https://www.openml.org/data/get_csv/1592296/php9xWOpn')\n\npredictors = ['V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'Class']\ndf['Class'] -= 1 \n```\n:::\n\n\nSince there are multiple binary predictors, I needed to create a single class variable to represent each class. The `Class` variable doesn't currently represent this, it represents all faults that don't fit in the categories of `V28` to `V33`. The single variable class was created with the `np.argmax` function which returns the index of the highest value between all the predictors.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ny = np.argmax(df[predictors].values, axis =1)\nX = df.drop(predictors, axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n```\n:::\n\n\n## Modelling\n\nThis is the most basic model that I would like to evaluate. I've used the `GridSearch` function, so all combinations of parameters are tested. The only parameter I wanted to examine was the size of the hidden layers. Each hidden layer provided is a tuple, where each number represents the number of nodes in a singled layer. Multiple numbers represent additional layers.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100), \n(100,100,100,100), \n(100,100,100,100,100), \n(100,100,100,100,100,100), \n(100,100,100,100,100,100,100),\n(100,100,100,100,100,100,100,100),\n(100,100,100,100,100,100,100,100,100),\n(100,100,100,100,100,100,100,100,100,100)]}\nmodel = MLPClassifier(random_state = 1,max_iter = 10000, \nsolver = 'adam', learning_rate = 'adaptive')\n\ngrid = GridSearchCV(estimator = model, param_grid = parameters)\ngrid.fit(X_train, y_train)\nprint(grid.best_score_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.4054982817869416\n```\n:::\n:::\n\n\nThe performance of the best model in the grid is not impressive. It took me awhile to realize that I had forgotten to scale the features. I included this error to show the importance of scaling on model performance.\n\n## Feature Scaling\n\nThe features are simply scaled with the `StandardScaler` function. The same model is used on the scaled features.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nscaler = sc.fit(X_train)\nX_train_sc = scaler.transform(X_train)\nX_test_sc = scaler.transform(X_test)\n\nparameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100), \n(100,100,100,100), \n(100,100,100,100,100), \n(100,100,100,100,100,100), \n(100,100,100,100,100,100,100),\n(100,100,100,100,100,100,100,100),\n(100,100,100,100,100,100,100,100,100),\n(100,100,100,100,100,100,100,100,100,100)]}\nmodel = MLPClassifier(random_state = 1,max_iter = 10000, \nsolver = 'adam', learning_rate = 'adaptive')\n\ngrid = GridSearchCV(estimator = model, param_grid = parameters, cv=3)\ngrid.fit(X_train_sc, y_train)\ngrid.best_score_\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n0.7553264604810996\n```\n:::\n:::\n\n\nThe performance of the scaled model is much more impressive. After the `GridSearch` function finds the parameters for the best model, it retrains the model on the entire dataset. This is because the function utilize cross validation, so some data was withheld for comparing the different models on test data.\n\n## Conclusion\n\nWith our model constructed, we can now test its performance on the original test set. It is important to remember to use the scaled test features, as that is what the model is expecting.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ngrid.score(X_test_sc, y_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n0.7304526748971193\n```\n:::\n:::\n\n\nThe results are pretty satisfactory. A decent level of accuracy without a lot of complicated code. Default values were used, whenever they were appropriate. Additional steps could be taken, but this remains a good foundation for future exploratory analysis.\n\n> Photo by [Alina Grubnyak](https://unsplash.com/@alinnnaaaa?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/neural-networks?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}