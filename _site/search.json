[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Mark Edney is the sole writer for all articles. He performs his own research, and all projects are those that he has completed from a selection of different Data Science certifications. He also manages the website.\nMark is a Engineer in Training located in Toronto Ontario Canada. He has studied data science since 2019 but has programmed for years before that. He is proficient in R, Python and Matlab but has previously learned some C and C++.\nThis website is created in R with the blogdown package. All assets are stored on Github and generously hosted by Netlify. For any inquires, please refer to the contact page.\n\nBlogroll\nA collection of interesting and related blogs:\n\nr-bloggers\npython-bloggers"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Aug 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-01-29-fitness-tracker-modeling-ml/index.html#initialization",
    "href": "posts/2022-01-29-fitness-tracker-modeling-ml/index.html#initialization",
    "title": "Fitness Tracker Modeling: ML",
    "section": "Initialization",
    "text": "Initialization\nThe following code was used to initialize the required R libraries, as well as downloading the required data and store it into memory. There are some columns of the data that were not required for modelling which were excluded (ex. usernames).\n\nlibrary(caret)\nlibrary(gbm)\nlibrary(dplyr)\nlibrary(randomForest)\nlibrary(ggplot2)\nset.seed(90210)\nNtree <- 200\n\ndownload.file(\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\", \"training.csv\")\ndownload.file(\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\", \"testing.csv\")\ntrain <- read.csv2(\"training.csv\", sep = \",\")[,-c(1:7)]\ntest <- read.csv2(\"testing.csv\", sep = \",\")[,-c(1:7)]"
  },
  {
    "objectID": "posts/2022-01-29-fitness-tracker-modeling-ml/index.html#reducing-predictors",
    "href": "posts/2022-01-29-fitness-tracker-modeling-ml/index.html#reducing-predictors",
    "title": "Fitness Tracker Modeling: ML",
    "section": "Reducing predictors",
    "text": "Reducing predictors\nThe data contains way too many predictors (153 in total) to produce accurate and simple models. Some trimming is required. The first trim is performed with the near zero variance function from the caret library, which finds the predictors that exhibit near zero variation. These predictors would add little benefit to include in models.\n\nnz <- nearZeroVar(train)\ntrain <- train[,-nz]\ntest <- test[-nz]\n\nFrom this step, the number of predictors is reduced to 94. There remains numerous NA values in the data. These values are examined in the next chunk of code.\n\nmaxi <- length(train) - 1\nvalna <- 1:maxi\n\nfor (i in 1:maxi) {\n        train[,i] <- as.numeric(train[,i])\n        test[,i] <- as.numeric(test[,i])\n        valna[i] <- mean(is.na(train[,i]))\n}\n\ntable(valna)\n\nvalna\n                0 0.979308938946081 \n               52                41 \n\n\nThe code shows that there are 52 predictors that have no missing data and 41 predictors that are mostly missing values. These predictors would add little value to the modelling and are removed with the following code\n\ntrain <- train[, valna == 0]\ntest <- test[, valna == 0]\n\nThe training was then divided to create a validation set which will be used for cross validation. Note that the random forest algorithm has built in cross validation with the “out of bag error”. About 1/3 of the data is used in a random forest.\n\nValid <- createDataPartition(train$classe, p = 0.3)[[1]]\nvalid <- train[Valid,]\ntrain <- train[-Valid,]\n\nThe next step is to utilize the variable of importance function in the caret library to reduce the number of predictors even further. The train data is still very large, but by making a sample set from the training data and modelling from that we can get a reasonable approximation of the variables of importance.\n\nstrain <- rbind(sample_n(train[train$classe == \"A\",],round(mean(train$classe == \"A\")*200,0)),\n                sample_n(train[train$classe == \"B\",],round(mean(train$classe == \"B\")*200,0)),\n                sample_n(train[train$classe == \"C\",],round(mean(train$classe == \"C\")*200,0)),\n                sample_n(train[train$classe == \"D\",],round(mean(train$classe == \"D\")*200,0)),\n                sample_n(train[train$classe == \"E\",],round(mean(train$classe == \"E\")*200,0))\n)\n\nThe sample set was set to ensure an accurate representation of the ‘classe’ variable in the training data. Two models were completed and their variables of importance were added together.\n\nmdl1 <- train(classe~., data = strain, method = \"rf\", ntree = Ntree)\nmdl2 <- train(classe~., data = strain, method = \"gbm\", verbose = FALSE)\nvar <- varImp(mdl1)$importance > 50 | varImp(mdl2)$importance > 50\nvarorder <- order(varImp(mdl1)$importance, decreasing = TRUE)\nVarimp <- row.names(varImp(mdl1)$importance)[varorder[1:2]]\n\nA value of 50 was used for a cut-off value. The total number of predictors has been reduced to 4.\n\nvalid <- valid[,var]\ntrain <- train[,var] %>% slice_sample(prop = 0.75)\ntest <- test[,var]"
  },
  {
    "objectID": "posts/2022-01-29-fitness-tracker-modeling-ml/index.html#modelling",
    "href": "posts/2022-01-29-fitness-tracker-modeling-ml/index.html#modelling",
    "title": "Fitness Tracker Modeling: ML",
    "section": "Modelling",
    "text": "Modelling\nWith the reduced predictors, the models can now be trained. Since these model will look at the entire training data set, it will require a lot of time. The models include:\n- Random forest\n- Generalized Boosted\n- Linear Discriminant\n- Combined\nThe randomForest function is used as it is more efficient than the train function. The data method is also more efficient than using the formula method.\n\nmdl11 <- randomForest(x = train[,1:(ncol(train) - 1)], y = as.factor(train[,ncol(train)]), ntree = Ntree, proximity = TRUE)\nmdl21 <- train(classe~., data = train, method = \"gbm\", verbose = FALSE)\nmdl31 <- train(classe~., data = train, method = \"lda\")\n\nThe following code constructs the combined model\n\npmdl11 <- predict(mdl11, valid)\npmdl21 <- predict(mdl21, valid)\npmdl31 <- predict(mdl31, valid)\njoin <- data.frame(pmdl11, pmdl21, pmdl31, classe = valid$classe)\njmdl <- randomForest(x = join[,1:3], y = as.factor(join$classe), ntree = Ntree)"
  },
  {
    "objectID": "posts/2022-01-29-fitness-tracker-modeling-ml/index.html#model-evaluation",
    "href": "posts/2022-01-29-fitness-tracker-modeling-ml/index.html#model-evaluation",
    "title": "Fitness Tracker Modeling: ML",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nThe new models will need to be examined against the validation data set. The out of bag error for the random forest models were not used, as the validation data provides a uniform comparison for all models. The following function was used to test the models:\n\nExacc <- function(mdl, test){\n        mean(predict(mdl,test) == test$classe)\n}\n\nThe model’s accuracy are summarized in the following dataframe when they are used to predict the results in the validation set:\n\n\n  Model  accuracy\n1 mdl11 0.8935303\n2 mdl21 0.7989472\n3 mdl31 0.3625403\n4 joint 0.8897945"
  },
  {
    "objectID": "posts/2022-01-30-job-posting-analysis/index.html",
    "href": "posts/2022-01-30-job-posting-analysis/index.html",
    "title": "Job posting analysis",
    "section": "",
    "text": "Recently, there was a post on medium about the use of Natural Language Processing (NLP) to study a job posting for keywords. I found that this article was very similar to R shiny App that I created a while ago. 1"
  },
  {
    "objectID": "posts/2022-01-30-job-posting-analysis/index.html#introduction",
    "href": "posts/2022-01-30-job-posting-analysis/index.html#introduction",
    "title": "Job posting analysis",
    "section": "Introduction",
    "text": "Introduction\nTechnology has changed the job application process, making it easier and quicker to apply to jobs. As a result, the average job posting will receive around 250 resumes. 2 So how can hiring managers handle spending their time looking through that many resumes for one posting? That’s easy, they cheat.\nHiring Managers no longer look at individual resumes, but use automatic software called applicant tracking system (ATS). These programs filter resumes by a set of keywords, reducing the amount of resumes to a more manageable amount. So how can a job applicant make sure their resume is looked at? Well, they should cheat.\nThe medium article I mentioned uses Python and Natural Language Processing (NLP) to skim through the job posting to look for the most common words used. This is useful information, but not necessarily the keywords used by the ATS software. I propose the use of an R Shiny App to filter a job posting by a list of common keywords.\nAn R Shiny App is an interactive web based application that runs R code. The syntax for a Shiny App is a little different from R and requires some additional understanding. The product will be a basic, interactive program that can be hosted online. One free Shiny App hosting site that I recommend is shinyapps.io."
  },
  {
    "objectID": "posts/2022-01-30-job-posting-analysis/index.html#inialization",
    "href": "posts/2022-01-30-job-posting-analysis/index.html#inialization",
    "title": "Job posting analysis",
    "section": "Inialization",
    "text": "Inialization\nThe shiny App will require the following libraries.\n\nlibrary(shiny)\nlibrary(wordcloud2)\nlibrary(tidyverse)\nlibrary(XML)\nlibrary(rvest)\nlibrary(tidytext)\n\nThe Shiny App will use a csv files which contains a set of keywords that ATS will look for. This list was found online, but I have modified by adding additional keywords as I see fit. The file can be downloaded here from my GitHub site. Here is a sample of some keywords:\n\nKeywords <- read_csv(\"Keywords.csv\") \nKeywords$Keys %>% head()\n\n[1] \".NET\"                \"account management\"  \"accounting\"         \n[4] \"accounts payable\"    \"accounts receivable\" \"acquisition\""
  },
  {
    "objectID": "posts/2022-01-30-job-posting-analysis/index.html#app-structure",
    "href": "posts/2022-01-30-job-posting-analysis/index.html#app-structure",
    "title": "Job posting analysis",
    "section": "App Structure",
    "text": "App Structure\nOne issue I found when developing this application was the use of keywords that are a combination of multiple words. This creates some complications, as a simple search of keywords would use only the first word and lose the context.\nThis challenge was met by breaking the website down into ngrams. An over simplification of a ngram is a group of n number of words. Wikipedia has a very good page that better explains ngrams.3 The website can then be split into ngrams of different lengths and the keywords searched for.\nAs a example, the phrase:\n\nThe quick brown fox\n\nfor a ngram of length 1 would return:\n\n(The) (quick) (brown) (fox)\n\nfor a ngram of length 2 would return:\n\n(The quick) (quick brown) (brown fox)\n\nand for a ngram of length 3 would return:\n\n(The quick brown) (quick brown fox)"
  },
  {
    "objectID": "posts/2022-01-30-job-posting-analysis/index.html#application-coding",
    "href": "posts/2022-01-30-job-posting-analysis/index.html#application-coding",
    "title": "Job posting analysis",
    "section": "Application Coding",
    "text": "Application Coding\n\nshinyApp(\n#This is the standard format for a shiny app\n        \n#The UI function controls all the frontend for the app\n        ui = fluidPage(\n                titlePanel(\"Job Posting Word Cloud\"),\n                sidebarLayout(\n                        sidebarPanel(\n#The user is asked for a url\n                                textInput(\"url\", \"input URL\", value = \"https://www.google.com/\")\n                                ),\n                        mainPanel(\n#The word cloud plot is displayed\n                                h4(\"Key-Word Cloud\"),\n                                wordcloud2Output(\"plot\")\n                                )\n                        )\n                ),\n        \n#The server function controls the backend for the app\n        server = function(input, output){\n                \n#The keywords are loaded and an index of how many words per keyword is created\n                Keywords <- read_csv(\"Keywords.csv\")\n                Keywords$Keys <- str_to_lower(Keywords$Keys) \n                index <- Keywords$Keys %>% str_count(\" \")\n                \n#The { brackets are used to create reactive functions which continuously run \n                data <- reactive({\n#The input variable is how the server side receives data from the ui side\n                url <- input$url\n#The text is read from the url provide by the user\n                data <- text %>%\n                        data.frame(text = .)\n#Since there are ngrams of length 1-3, there a three search's that are concatenated together\n                rbind(data %>%\n#the unnest_tolkens from the tidytext library is used to create the ngrams\n                              unnest_tokens(word, text, token = \"ngrams\", n= 1) %>%\n#A count is performed on each ngram in the website to find the most common ngrams \n                              count(word, name = 'freq', sort = TRUE) %>%\n#The ngram count is then filtered by the keywords of the same ngram length\n                              filter(word %in% Keywords$Keys[index == 0]),\n#The steps are repeated for bigrams (ngrams of length 2) and trigrams(ngrams of length 3)\n                      data %>%\n                              unnest_tokens(word, text, token = \"ngrams\", n= 2) %>%\n                              count(word, name = 'freq', sort = TRUE) %>%\n                              filter(word %in% Keywords$Keys[index == 1]),\n                      data %>%\n                              unnest_tokens(word, text, token = \"ngrams\", n= 3) %>%\n                              count(word, name = 'freq', sort = TRUE) %>%\n                              filter(word %in% Keywords$Keys[index == 2]))\n                        })\n                \n#The plot/wordcloud needs to be saved as an output value\n#The output variable is how the server sends data back to the UI\n                output$plot <- renderWordcloud2({\n#One part of the strange syntax of a shiny app is that the since the data is reactive\n#and changes with the user input, it is passed in a function so it needs to be called\n#as data ()\n                        wordcloud2(data())\n                        })\n        },\n\n  options = list(height = 500)\n)"
  },
  {
    "objectID": "posts/2022-01-30-job-posting-analysis/index.html#shiny-app",
    "href": "posts/2022-01-30-job-posting-analysis/index.html#shiny-app",
    "title": "Job posting analysis",
    "section": "Shiny App",
    "text": "Shiny App"
  },
  {
    "objectID": "posts/2022-02-11-fancy-tables-in-r/index.html#introduction",
    "href": "posts/2022-02-11-fancy-tables-in-r/index.html#introduction",
    "title": "Fancy Tables in R",
    "section": "Introduction",
    "text": "Introduction\nAs a continuation from my previous post exploring the use of the Stargazer library to create better looking tables, I thought I would look into the GT library. The GT library takes a different approach by creating an object class with the GT function. It is still able to create great looking tables in html or latex, but also adds support for RTF."
  },
  {
    "objectID": "posts/2022-02-11-fancy-tables-in-r/index.html#gt-library-intuition",
    "href": "posts/2022-02-11-fancy-tables-in-r/index.html#gt-library-intuition",
    "title": "Fancy Tables in R",
    "section": "GT Library Intuition",
    "text": "GT Library Intuition\nThe use of the GT library is pretty simple and starts with the creation of a GT object. For this example, I will use the ChickenWeight database that looks at the weights of different chickens.\n\nlibrary(gt)\nlibrary(tidyverse)\ndata(\"ChickWeight\")\n\nThe dataframe or tibble is passed into the GT function, which can be passed into additional modifier functions. This is better outlined and easier understood using the piping operator from the magrittr library, which is also included in the tidyverse library. The piping operator is incredibly useful and common.\n\nChickWeight %>% head() %>% gt()\n\n\n\n\n\n  \n  \n    \n      weight\n      Time\n      Chick\n      Diet\n    \n  \n  \n    42\n0\n1\n1\n    51\n2\n1\n1\n    59\n4\n1\n1\n    64\n6\n1\n1\n    76\n8\n1\n1\n    93\n10\n1\n1\n  \n  \n  \n\n\n\n\nThe result is a simple a clean table the displays the required data. There is a set structure for the GT objects, which is outlined in the following diagram:"
  },
  {
    "objectID": "posts/2022-02-11-fancy-tables-in-r/index.html#function-chaining",
    "href": "posts/2022-02-11-fancy-tables-in-r/index.html#function-chaining",
    "title": "Fancy Tables in R",
    "section": "Function Chaining",
    "text": "Function Chaining\nAs previously mentioned, by chaining GT functions together, we can add elements to the table. These elements can include titles, footnotes, source notes and conditional formatting similar to what you would use in an Excel table.\n\nChickWeight %>%\n        head() %>%\n        gt() %>%\n        tab_header(\n                title = \"Chicken Weight data\", \n                subtitle = \"remember to weight your chickens!\"\n        ) %>%\n        tab_footnote(footnote = \"measured in seconds\",\n                     locations = cells_column_labels(Time)) %>%\n        tab_source_note(source_note = \"From ChickenWeight Database\") %>%\n        tab_style(style = cell_fill(color = \"red\"),\n                  locations = cells_body(\n                          columns = weight,\n                          rows = weight > 50\n                  )) %>%\n        summary_rows(columns = c(weight,Time),\n                           fns = list(\n                                   avg = ~mean(., na.rm = TRUE),\n                                   total = ~sum(., na.rm = TRUE),\n                                   s.d. = ~sd(., na.rm = TRUE))\n                           )\n\n\n\n\n\n  \n    \n      Chicken Weight data\n    \n    \n      remember to weight your chickens!\n    \n  \n  \n    \n      \n      weight\n      Time1\n      Chick\n      Diet\n    \n  \n  \n    \n42\n0\n1\n1\n    \n51\n2\n1\n1\n    \n59\n4\n1\n1\n    \n64\n6\n1\n1\n    \n76\n8\n1\n1\n    \n93\n10\n1\n1\n    avg\n64.17\n5.00\n—\n—\n    total\n385.00\n30.00\n—\n—\n    s.d.\n18.24\n3.74\n—\n—\n  \n  \n    \n      From ChickenWeight Database\n    \n  \n  \n    \n      1 measured in seconds"
  },
  {
    "objectID": "posts/2022-02-11-fancy-tables-in-r/index.html#helper-functions",
    "href": "posts/2022-02-11-fancy-tables-in-r/index.html#helper-functions",
    "title": "Fancy Tables in R",
    "section": "Helper functions",
    "text": "Helper functions\nThe previous code includes smaller helper functions within the element functions. These functions (ex. cells_body) provide targeting information for locations or conditioning. There is a learning curve for these functions, I would recommend looking them up as you work on your table rather than trying to learn them all. The everything() function seems to be of particular usefulness, as it allows you to use all values, such as all columns."
  },
  {
    "objectID": "posts/2022-02-11-fancy-tables-in-r/index.html#additional-notes",
    "href": "posts/2022-02-11-fancy-tables-in-r/index.html#additional-notes",
    "title": "Fancy Tables in R",
    "section": "Additional notes",
    "text": "Additional notes\nThe summary_rows function can create summary rows for each grouping if the grouping is defined in the function or if the data is grouped itself. You can then use the grand_summary_rows function to create a grand summary.\nIt is always good practice to pass the na.rm = TRUE for your summary functions. Without it, you can create undesirable results.\nA useful resource for learning the GT library is an article on R studio found here. It goes through more in depth on the topics that I have skimmed over."
  },
  {
    "objectID": "posts/2022-02-16-/index.html#objective",
    "href": "posts/2022-02-16-/index.html#objective",
    "title": "Speed cameras in Toronto",
    "section": "Objective",
    "text": "Objective\nThis report plots the speed cameras in the Greater Toronto Area from the data provided by Open Toronto, which can be found here."
  },
  {
    "objectID": "posts/2022-02-16-/index.html#initialization",
    "href": "posts/2022-02-16-/index.html#initialization",
    "title": "Speed cameras in Toronto",
    "section": "Initialization",
    "text": "Initialization\nThe following code is used to initialize the required libraries.\n\ninstall.packages(\"opendatatoronto\", repos = \"https://cran.us.r-project.org\", dependencies = TRUE)\n\npackage 'opendatatoronto' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\Mark\\AppData\\Local\\Temp\\RtmpGUv5NT\\downloaded_packages\n\nlibrary(opendatatoronto)\nlibrary(dplyr)\nlibrary(leaflet)\n\nThe following code is provided by the Open Toronto site to download the dataset.\n\n# get package\npackage <- show_package(\"a154790c-4a8a-4d09-ab6b-535ddb646770\")\n\n# get all resources for this package\nresources <- list_package_resources(\"a154790c-4a8a-4d09-ab6b-535ddb646770\")\n\n# identify datastore resources; by default, Toronto Open Data sets datastore resource format to CSV for non-geospatial and GeoJSON for geospatial resources\ndatastore_resources <- filter(resources, tolower(format) %in% c('csv', 'geojson'))\n\n# load the first datastore resource as a sample\ndata <- filter(datastore_resources, row_number()==1) %>% get_resource()"
  },
  {
    "objectID": "posts/2022-02-16-/index.html#plotting-the-data",
    "href": "posts/2022-02-16-/index.html#plotting-the-data",
    "title": "Speed cameras in Toronto",
    "section": "Plotting the Data",
    "text": "Plotting the Data\nThe geometry in the dataset can be used directly with Leaflet, and the longitude and latitude do not need to be separated.\n\ndf <- data$geometry\n\nCustom icons for the speed cameras can be used with the following code:\n\ncameraicon <- makeIcon(\n        iconUrl = \"https://www.flaticon.com/svg/static/icons/svg/2164/2164608.svg\",\n        iconWidth = 35, iconHeight = 35\n)\n\nFinally, all the data and options can be passed to the leaflet function.\n\nplt <- df %>%\n        leaflet() %>%\n        addTiles() %>%\n        addMarkers(icon = cameraicon, clusterOptions = markerClusterOptions(), popup = data$location)"
  },
  {
    "objectID": "posts/2022-02-23-new-features-in-r/index.html",
    "href": "posts/2022-02-23-new-features-in-r/index.html",
    "title": "New features in R",
    "section": "",
    "text": "> Photo by Clint Patterson on Unsplash\nRecently I had updated my RStudio client and with it came a new update to R. This is an exploration of some of the most interesting changes from R 4.0 to R 4.1."
  },
  {
    "objectID": "posts/2022-02-23-new-features-in-r/index.html#native-pipe-function",
    "href": "posts/2022-02-23-new-features-in-r/index.html#native-pipe-function",
    "title": "New features in R",
    "section": "Native Pipe Function",
    "text": "Native Pipe Function\nDue to the extreme popularity of the magrittr pipe (‘%>%’), R has developed its own native pipe (‘|>’).\n\nlibrary(tidyverse)\ndata(\"morley\")\nmorley |>\n        group_by(Expt) |>\n        summarise(mean = mean(Speed, na.rm=TRUE))\n\n# A tibble: 5 × 2\n   Expt  mean\n  <int> <dbl>\n1     1  909 \n2     2  856 \n3     3  845 \n4     4  820.\n5     5  832.\n\n\nFrom this example, it is apparent that the behaviour of the native pipe is the same as the magrittr pipe.\nSome of the differences I have found is that the native pipe requires the brackets for functions, while the magrittr pipe will usually accept just the function name.\n\n2 %>% sqrt()\n\n[1] 1.414214\n\n2 |> sqrt()\n\n[1] 1.414214\n\n2 %>% sqrt\n\n[1] 1.414214\n\n\n\n2 |> sqrt\n\nError: The pipe operator requires a function call as RHS\n\n\nOne disadvantage of the native pipe is that it doesn’t support the placeholder operator (.) which helps refer to the data in the function. This is a useful function of the magrittr pipe when the data isn’t the first argument in the function, such as the lm function.\n\nmorley %>% lm(Speed~Run, data = .)\n\n\nCall:\nlm(formula = Speed ~ Run, data = .)\n\nCoefficients:\n(Intercept)          Run  \n   856.0947      -0.3519  \n\nmorley |> lm(Speed~Run, data = .)\n\nError in is.data.frame(data): object '.' not found\n\n\nOne advantage is there is no performance penalty as it acts the same as the function call. This is shown with the microbenchmark function, which shows not only the same level of performance as the regular call, but even the results themselves are shown as the function call.\n\nlibrary(microbenchmark)\nmicrobenchmark(sqrt(3),\n               4 |> sqrt(),\n               5 %>% sqrt())\n\nUnit: nanoseconds\n         expr  min   lq mean median   uq   max neval\n      sqrt(3)  100  100  239    200  200  5700   100\n      sqrt(4)    0  100  215    100  200  6800   100\n 5 %>% sqrt() 3200 3500 5110   3700 4000 43300   100\n\n\nSo when should we use the native vs the magrittr pipe? Well, it looks like not all the functionality of the magrittr pipe is carried over, so it should still be continued to be used. The native pipe, however, provides a good performance boost, which makes it a better option for code written in functions and libraries. I think that the major application should be to increase the readability of library and function code."
  },
  {
    "objectID": "posts/2022-02-23-new-features-in-r/index.html#lambda-functions",
    "href": "posts/2022-02-23-new-features-in-r/index.html#lambda-functions",
    "title": "New features in R",
    "section": "Lambda Functions",
    "text": "Lambda Functions\nThere has been a simplification in the creation of lambda functions. The notation is simplified, while the results are the same.\n\nlibrary(tidyverse)\nx <- 0:10/10\ny1 <- function(x) x + 0.5\ny2 <- \\(x) x^2 +1\ng <- ggplot(data.frame(x=x)) +\n        geom_function(fun = y1, aes(color = \"blue\")) +\n        geom_function(fun = y2, aes(color = \"red\"))\ng"
  },
  {
    "objectID": "posts/2022-02-23-new-features-in-r/index.html#other-minor-changes",
    "href": "posts/2022-02-23-new-features-in-r/index.html#other-minor-changes",
    "title": "New features in R",
    "section": "Other minor changes",
    "text": "Other minor changes\n\nThe default has been changed for ‘stringsAsFactors = FALSE’. Previously, when using the data.frame() or the read.table() the default option would turn strings into factors. This was an annoying feature that would always create headaches.\nIntroduction of an experimental implementation of hash tables. This development should be watched for people keen on program performance.\nc() can now combine factors to create a new factor. I am not familiar with the original behaviour, but this seems intuitive."
  },
  {
    "objectID": "posts/2022-02-27-bike-shares-in-toronto/index.html#bike-rental-shiny-app",
    "href": "posts/2022-02-27-bike-shares-in-toronto/index.html#bike-rental-shiny-app",
    "title": "Bike shares in Toronto",
    "section": "Bike Rental Shiny App",
    "text": "Bike Rental Shiny App\nThis application use the data collected from the Toronto Open Data to generate a histogram of the usage of rental bikes in Toronto during the month of June in 2020.\n\ninstall.packages(\"opendatatoronto\", \n                 repos = \"https://cran.us.r-project.org\",\n                 dependencies = TRUE)\nlibrary(opendatatoronto)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(shiny)"
  },
  {
    "objectID": "posts/2022-02-27-bike-shares-in-toronto/index.html#ui",
    "href": "posts/2022-02-27-bike-shares-in-toronto/index.html#ui",
    "title": "Bike shares in Toronto",
    "section": "UI",
    "text": "UI\nThere are two user inputs on the UI side:\n\nA slider that limits the maximum and minimum of the displayed values\nA checkbox that excludes users with a annual bike pass\n\n\n        sidebarPanel(\n            sliderInput(\"dur\",\n                        \"Trip Duration:\",\n                        min = 0,\n                        max = 500,\n                        value = c(0,500)),\n            checkboxInput(\"freq\",\n                        \"Exclude annual users:\",\n                        value = FALSE))"
  },
  {
    "objectID": "posts/2022-02-27-bike-shares-in-toronto/index.html#server",
    "href": "posts/2022-02-27-bike-shares-in-toronto/index.html#server",
    "title": "Bike shares in Toronto",
    "section": "Server",
    "text": "Server\nThe following code is used for the server side logic, this includes downloading the data from the ‘opendatatoronto’ library.\n\n # get package\n    package <- show_package(\"7e876c24-177c-4605-9cef-e50dd74c617f\")\n    \n    # get all resources for this package\n    resources <- list_package_resources(\"7e876c24-177c-4605-9cef-e50dd74c617f\")\n    # identify datastore resources; by default, Toronto Open Data sets datastore resource format to CSV for non-geospatial and GeoJSON for geospatial resources\n    datastore_resources <- filter(resources, tolower(format) %in% c('zip', 'geojson'))\n    # load the first datastore resource as a sample\n    data <- filter(datastore_resources, name == \"Bike share ridership 2020\") %>% get_resource()\n    data2 <-  data$`2020-06.csv`\n    data2[grepl(\"Time\",names(data2))] <- \n        lapply(data2[grepl(\"Time\",names(data2))], parse_date_time, orders = \"mdy HM\")\n    data2$Dur <- as.numeric(data2$End.Time - data2$Start.Time,units=\"mins\")"
  },
  {
    "objectID": "posts/2022-02-27-bike-shares-in-toronto/index.html#application",
    "href": "posts/2022-02-27-bike-shares-in-toronto/index.html#application",
    "title": "Bike shares in Toronto",
    "section": "Application",
    "text": "Application\nThe final application takes a while to load as the data needs to be downloaded and sorted through. In future iterations, I would save the data locally as an RDS file."
  },
  {
    "objectID": "posts/2022-03-03-python-in-r-markdown/index.html",
    "href": "posts/2022-03-03-python-in-r-markdown/index.html",
    "title": "Python in R Markdown",
    "section": "",
    "text": "The main advantage of using the R Markdown format is the utility of running R code within the text. This is clearly more advantageous than just writing code in a Markdown file. R Markdown is however limited to R code, unable to run Python scripts. The R library reticulate looks to add this capability."
  },
  {
    "objectID": "posts/2022-03-03-python-in-r-markdown/index.html#initial-setup",
    "href": "posts/2022-03-03-python-in-r-markdown/index.html#initial-setup",
    "title": "Python in R Markdown",
    "section": "Initial Setup",
    "text": "Initial Setup\nThe initial setup requires the installation of the reticulate library, after installation you shouldn’t need to call it, but I do in the preceding code. I have loaded the trees dataset as a test dataset and the tidyverse library just to explore the data a bit.\n\nlibrary(reticulate)\nlibrary(tidyverse)\ndata(trees)\nglimpse(trees)\n\nRows: 31\nColumns: 3\n$ Girth  <dbl> 8.3, 8.6, 8.8, 10.5, 10.7, 10.8, 11.0, 11.0, 11.1, 11.2, 11.3, …\n$ Height <dbl> 70, 65, 63, 72, 81, 83, 66, 75, 80, 75, 79, 76, 76, 69, 75, 74,…\n$ Volume <dbl> 10.3, 10.3, 10.2, 16.4, 18.8, 19.7, 15.6, 18.2, 22.6, 19.9, 24.…\n\n\nNow, R Studio will use your local version of Python when you write any code in a code chuck labelled with the “{Python}” header. If you don’t have any local version, R Studio will ask if you would like to install Miniconda. From here, you will need to start downloading the required Python modules.\nModules can be downloaded with the pip python package installer from the terminal or command line. The easiest method in R Studio is within the terminal window next to the console window. The command used is pip install \"module name\". Some modules can be tricky and won’t work if not installed after other modules.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "posts/2022-03-03-python-in-r-markdown/index.html#multiple-environments",
    "href": "posts/2022-03-03-python-in-r-markdown/index.html#multiple-environments",
    "title": "Python in R Markdown",
    "section": "Multiple Environments",
    "text": "Multiple Environments\nAfter the setup, you should see some additional options in the environment in R Studio. You should see that you have the option to switch between the R and Python environments.\nData is transitioned from the R environment to the Python environment with the r variable. This method should pretty similar to the Shiny Apt’s use of input\\output. It is not only data that can move between environments, but functions too.\nThe following code takes data from the R environment and creates a plot in Seaborn. The mean values of the columns are calculated in python to be imported into the R environment. A simple linear model is created with the SKlearn module.\n\ndata = r.trees\nmeans = np.mean(data, axis = 0)\ndata[\"big\"] = data.Height > means.Height \nsns.scatterplot(data = data, x= \"Girth\", y= \"Height\", hue = \"big\")\nsns.set_theme(color_codes=True)\nplt.show()\n\n\n\nfrom sklearn.linear_model import LinearRegression\nmdl = LinearRegression()\nmdl.fit(data[[\"Girth\"]], data[[\"Height\"]])\n\nLinearRegression()\n\nprint(mdl.coef_)\n\n[[1.05436881]]\n\n\nData is transitioned from Python to, R similarly with the variable py. Information on models can be passed but not the models themselves. This is important if you are more comfortable creating models in Python.\n\nprint(py$means)\n\n   Girth   Height   Volume \n13.24839 76.00000 30.17097 \n\nprint(py$mdl$intercept_)\n\n[1] 62.03131\n\npy$data %>%\n        ggplot(aes(x = Girth, y = Height, colour = big)) +\n        geom_point()"
  },
  {
    "objectID": "posts/2022-03-10-creating-dashboard-in-r/index.html",
    "href": "posts/2022-03-10-creating-dashboard-in-r/index.html",
    "title": "Creating Dashboards in R",
    "section": "",
    "text": "Dashboards are a great way to demonstrate knowledge and engage decision makers. Their utility has made PowerBI and Tableau household names. And while these solutions do support R and Python scripts and visualizations, the Flexdashboard package seeks to compete. The Flexdashboard packages does this all in R with the simplicity of writing a R Markdown file."
  },
  {
    "objectID": "posts/2022-03-10-creating-dashboard-in-r/index.html#initial-setup",
    "href": "posts/2022-03-10-creating-dashboard-in-r/index.html#initial-setup",
    "title": "Creating Dashboards in R",
    "section": "Initial Setup",
    "text": "Initial Setup\nThe setup is simple, you just need to download and load the Flexdashboard package. With the package installed, the easiest way to start is by creating a new R Markdown file using the Flexdashboard template. Loading the Shiny package is useful if you would like to use interactive plots, but it is not necessary.\nThe dashboard can be laid out by either columns or by rows, it doesn’t really make a difference. Just change the text of columns with rows in the following walk-through. A column is set up with “## Column” as the header. The size of the plot region can then be modified with “{data-width=”500”}” in the same header line. The next line should be the plot/area title, which is included with “### Plot Title” header. All that is left is to include a code chunk with your plot.\n\nlibrary(flexdashboard)\nlibrary(shiny)"
  },
  {
    "objectID": "posts/2022-03-10-creating-dashboard-in-r/index.html#sample-data",
    "href": "posts/2022-03-10-creating-dashboard-in-r/index.html#sample-data",
    "title": "Creating Dashboards in R",
    "section": "Sample Data",
    "text": "Sample Data\nI decided to demonstrate different dashboard features with a data set from Open Canada about charitable donations. More information can be found here\n\ndownload.file(\"https://www150.statcan.gc.ca/n1/tbl/csv/45100007-eng.zip\", \"donordata.zip\")"
  },
  {
    "objectID": "posts/2022-03-10-creating-dashboard-in-r/index.html#sample-dashboard-1",
    "href": "posts/2022-03-10-creating-dashboard-in-r/index.html#sample-dashboard-1",
    "title": "Creating Dashboards in R",
    "section": "Sample Dashboard 1",
    "text": "Sample Dashboard 1\nThe first dashboard was set up with the default columns layout. It includes an interactive bar chart, an interactive box plot and a pie chart. All the plot were created with GGplot2, the two plot were made interactive with the GGplotly function from the Plotly package. I created a pie chart to demonstrate the use of regular ggplots and because I recently read a complaint about GGplot2 for the creation of Pie charts on Reddit. In my opinion, Pie charts are not very good very conveying information."
  },
  {
    "objectID": "posts/2022-03-10-creating-dashboard-in-r/index.html#sample-dashboard-2",
    "href": "posts/2022-03-10-creating-dashboard-in-r/index.html#sample-dashboard-2",
    "title": "Creating Dashboards in R",
    "section": "Sample Dashboard 2",
    "text": "Sample Dashboard 2\nFor the second dashboard, I used the row layout. The process is that same with no additional complications. The dashboard features an interactive Leaflet plot, an interactive histogram and data table using the GT package. The table was transformed with Pivot_Wider function to better fill the space."
  },
  {
    "objectID": "posts/2022-03-10-creating-dashboard-in-r/index.html#conclusions",
    "href": "posts/2022-03-10-creating-dashboard-in-r/index.html#conclusions",
    "title": "Creating Dashboards in R",
    "section": "Conclusions",
    "text": "Conclusions\nThe Flexdashboard package can be used to create nice looking dashboards with a great level of control. The plots can also include interactive elements. When compared to PowerBi or Tableau, there remains one major deficiency. These other dashboards contain a smart interactive filter which ties all the plots together. If you select a specific element in one plot for filtering, all other plots have the same filter applied to them. This is a major boon for understanding data and not a simple feature to develop in Flexdashboard. It remains an interesting package, but I would still rely on PowerBI or Tableau to create dashboards.\n\nPhoto by Luke Chesser on Unsplash"
  },
  {
    "objectID": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html",
    "href": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html",
    "title": "Making the Connection with Crosstalk",
    "section": "",
    "text": "I recently wrote a post about creating dashboards in R which was based on the Flexdashboard library. My largest criticism was the lack of communication between visualizations on the same dashboard. This was before I learned about the Crosstalk package which adds this feature to html widgets, such as the Flexdashboard, to at least a limited degree."
  },
  {
    "objectID": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html#initialization",
    "href": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html#initialization",
    "title": "Making the Connection with Crosstalk",
    "section": "Initialization",
    "text": "Initialization\nThe Crosstalk package is available on CRAN and is loaded along with other important packages for this demonstration.\n\ninstall.packages(\"crosstalk\")\nlibrary(crosstalk)\nlibrary(tidyverse)\nlibrary(flexdashboard)\nlibrary(plotly)\n\nI have decided to use a Toronto Open dataset about city audits for apartment buildings. I limited the features to only the ones that I feel will be interesting to look at. More information about the data set can be found here.\n\ndownload.file(\"https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/4ef82789-e038-44ef-a478-a8f3590c3eb1/resource/979fb513-5186-41e9-bb23-7b5cc6b89915/download/Apartment%20Building%20Evaluation.csv\", \"data.csv\")\ndf <- read_csv(\"data.csv\") %>%\n        select(lng = LONGITUDE, \n               lat = LATITUDE, \n               SCORE, \n               YEAR_BUILT, \n               SITE_ADDRESS, \n               PROPERTY_TYPE) %>% \n        slice_sample(n = 200)\n\nThe key to the crosstalk library is the SharedData functions. An object is created when a Data Frame is passed to the SharedData$new function. This is what enables communication between plots.\n\nshared_df <- SharedData$new(df)"
  },
  {
    "objectID": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html#dashboard-creation",
    "href": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html#dashboard-creation",
    "title": "Making the Connection with Crosstalk",
    "section": "Dashboard Creation",
    "text": "Dashboard Creation\nThe dashboard is created pretty much as previous mentioned in my dashboard post, with the exception that the shared Data Frame object is passed rather than the Data Frame.\nThe dashboard can include filters that are very similar to the Shiny Apt filters, with the filter_* family of functions.\n\nfilter_slider(\"Score\", \"SCORE\", shared_df, ~SCORE, round = TRUE)\nfilter_checkbox(\"Property Type\", \"PROPERTY_TYPE\", shared_df, ~PROPERTY_TYPE, inline = TRUE)"
  },
  {
    "objectID": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html#conclusion",
    "href": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html#conclusion",
    "title": "Making the Connection with Crosstalk",
    "section": "Conclusion",
    "text": "Conclusion\nThe Crosstalk package does add some significant connectivity to Flex Dashboards. It is relatively simple to use with some basic functions. It does have the issue of not working with aggregating data. The utility of finding the mean value of a selection is something Tableu and PowerBI are still superior at. I think that it is still a welcome improvement."
  },
  {
    "objectID": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html#final-dashboard",
    "href": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html#final-dashboard",
    "title": "Making the Connection with Crosstalk",
    "section": "Final Dashboard",
    "text": "Final Dashboard\n\n\n\n\n\nPhoto by Jason Goodmanon Unsplash"
  },
  {
    "objectID": "posts/2022-03-20-simple-neural-networks-in-python/index.html",
    "href": "posts/2022-03-20-simple-neural-networks-in-python/index.html",
    "title": "Simple Neural Networks in Python",
    "section": "",
    "text": "Neural Networks (NN) have become incredibly popular due to their high level of accuracy. The creation of a NN can be complicated and have a high level of customization. I wanted to explore just the simplest NN that you could create. A framework as a workhorse for developing new NN.\nThe SciKitlearn provides the easiest solution with the Multi-Layer Perceptron series of functions. It doesn’t provide a bunch of the more advanced features of TensorFlow, like GPU support, but that is not what I’m looking for."
  },
  {
    "objectID": "posts/2022-03-20-simple-neural-networks-in-python/index.html#initialization",
    "href": "posts/2022-03-20-simple-neural-networks-in-python/index.html#initialization",
    "title": "Simple Neural Networks in Python",
    "section": "Initialization",
    "text": "Initialization\nFor the demonstration, I decided to use a data set on faults found in steel plates from the OpenML website. The data set includes 27 features with 7 binary predictors.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv('https://www.openml.org/data/get_csv/1592296/php9xWOpn')\n\npredictors = ['V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'Class']\ndf['Class'] -= 1 \n\nSince there are multiple binary predictors, I needed to create a single class variable to represent each class. The Class variable doesn’t currently represent this, it represents all faults that don’t fit in the categories of V28 to V33. The single variable class was created with the np.argmax function which returns the index of the highest value between all the predictors.\n\ny = np.argmax(df[predictors].values, axis =1)\nX = df.drop(predictors, axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
  },
  {
    "objectID": "posts/2022-03-20-simple-neural-networks-in-python/index.html#modelling",
    "href": "posts/2022-03-20-simple-neural-networks-in-python/index.html#modelling",
    "title": "Simple Neural Networks in Python",
    "section": "Modelling",
    "text": "Modelling\nThis is the most basic model that I would like to evaluate. I’ve used the GridSearch function, so all combinations of parameters are tested. The only parameter I wanted to examine was the size of the hidden layers. Each hidden layer provided is a tuple, where each number represents the number of nodes in a singled layer. Multiple numbers represent additional layers.\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100), \n(100,100,100,100), \n(100,100,100,100,100), \n(100,100,100,100,100,100), \n(100,100,100,100,100,100,100),\n(100,100,100,100,100,100,100,100),\n(100,100,100,100,100,100,100,100,100),\n(100,100,100,100,100,100,100,100,100,100)]}\nmodel = MLPClassifier(random_state = 1,max_iter = 10000, \nsolver = 'adam', learning_rate = 'adaptive')\n\ngrid = GridSearchCV(estimator = model, param_grid = parameters)\ngrid.fit(X_train, y_train)\nprint(grid.best_score_)\n\n0.4054982817869416\n\n\nThe performance of the best model in the grid is not impressive. It took me awhile to realize that I had forgotten to scale the features. I included this error to show the importance of scaling on model performance."
  },
  {
    "objectID": "posts/2022-03-20-simple-neural-networks-in-python/index.html#feature-scaling",
    "href": "posts/2022-03-20-simple-neural-networks-in-python/index.html#feature-scaling",
    "title": "Simple Neural Networks in Python",
    "section": "Feature Scaling",
    "text": "Feature Scaling\nThe features are simply scaled with the StandardScaler function. The same model is used on the scaled features.\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nscaler = sc.fit(X_train)\nX_train_sc = scaler.transform(X_train)\nX_test_sc = scaler.transform(X_test)\n\nparameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100), \n(100,100,100,100), \n(100,100,100,100,100), \n(100,100,100,100,100,100), \n(100,100,100,100,100,100,100),\n(100,100,100,100,100,100,100,100),\n(100,100,100,100,100,100,100,100,100),\n(100,100,100,100,100,100,100,100,100,100)]}\nmodel = MLPClassifier(random_state = 1,max_iter = 10000, \nsolver = 'adam', learning_rate = 'adaptive')\n\ngrid = GridSearchCV(estimator = model, param_grid = parameters, cv=3)\ngrid.fit(X_train_sc, y_train)\ngrid.best_score_\n\n0.7553264604810996\n\n\nThe performance of the scaled model is much more impressive. After the GridSearch function finds the parameters for the best model, it retrains the model on the entire dataset. This is because the function utilize cross validation, so some data was withheld for comparing the different models on test data."
  },
  {
    "objectID": "posts/2022-03-20-simple-neural-networks-in-python/index.html#conclusion",
    "href": "posts/2022-03-20-simple-neural-networks-in-python/index.html#conclusion",
    "title": "Simple Neural Networks in Python",
    "section": "Conclusion",
    "text": "Conclusion\nWith our model constructed, we can now test its performance on the original test set. It is important to remember to use the scaled test features, as that is what the model is expecting.\n\ngrid.score(X_test_sc, y_test)\n\n0.7304526748971193\n\n\nThe results are pretty satisfactory. A decent level of accuracy without a lot of complicated code. Default values were used, whenever they were appropriate. Additional steps could be taken, but this remains a good foundation for future exploratory analysis.\n\nPhoto by Alina Grubnyak on Unsplash"
  },
  {
    "objectID": "posts/2022-03-31-underrated-cran-packages/index.html",
    "href": "posts/2022-03-31-underrated-cran-packages/index.html",
    "title": "Underrated CRAN Packages",
    "section": "",
    "text": "I sit here looking for inspiration, nothing interesting to write about. Perhaps there are some popular R packages on CRAN that I don’t know about? You can explore the data on downloads from CRAN with the cranlogs package."
  },
  {
    "objectID": "posts/2022-03-31-underrated-cran-packages/index.html#top-cran-downloads",
    "href": "posts/2022-03-31-underrated-cran-packages/index.html#top-cran-downloads",
    "title": "Underrated CRAN Packages",
    "section": "Top CRAN downloads",
    "text": "Top CRAN downloads\nWith the following code, we can get the most popular packages from CRAN. The CRAN directory doesn’t represent all R packages, but a good amount of them.\n\nlibrary(tidyverse)\nlibrary(cranlogs)\ntop100 <- cran_top_downloads(when = 'last-month', count = 100)\ntop100 %>% head()\n\n  rank  package   count       from         to\n1    1  ggplot2 2502873 2022-07-17 2022-08-15\n2    2    rlang 2039913 2022-07-17 2022-08-15\n3    3 devtools 1844834 2022-07-17 2022-08-15\n4    4       sf 1756905 2022-07-17 2022-08-15\n5    5      cli 1672799 2022-07-17 2022-08-15\n6    6     glue 1624788 2022-07-17 2022-08-15\n\n\nFrom this list, we can see that the tidyverse represents a large amount of the top downloads with ggplot2, rlang and dplyr. The list includes the sf package for geospacial data, the glue package for string manipulation and the cli package which is used to create a command line interface for packages. Most of these packages I already have a good understanding of, so I need to narrow down the search."
  },
  {
    "objectID": "posts/2022-03-31-underrated-cran-packages/index.html#packages-installed",
    "href": "posts/2022-03-31-underrated-cran-packages/index.html#packages-installed",
    "title": "Underrated CRAN Packages",
    "section": "Packages installed",
    "text": "Packages installed\nYou can get a list of your installed packages with the installed_packages function. You can then filter the top 100 list and remove anything you already have installed to find new packages.\n\nmine <- installed.packages() %>%\n        data.frame() %>%\n        select(Package)\nnew <- top100 %>%\n        filter(!package %in% mine$Package)\nnew\n\n   rank     package   count       from         to\n1     3    devtools 1844834 2022-07-17 2022-08-15\n2     7        ragg 1589393 2022-07-17 2022-08-15\n3     8 textshaping 1583922 2022-07-17 2022-08-15\n4    10       rgeos 1524077 2022-07-17 2022-08-15\n5    11         rgl 1499311 2022-07-17 2022-08-15\n6    18     pkgdown 1162782 2022-07-17 2022-08-15\n7    19  enrichwith 1158675 2022-07-17 2022-08-15\n8    20      brglm2 1154128 2022-07-17 2022-08-15\n9    31         zoo  873095 2022-07-17 2022-08-15\n10   50       Hmisc  706994 2022-07-17 2022-08-15\n11   60     rstatix  648757 2022-07-17 2022-08-15\n12   62      nloptr  637641 2022-07-17 2022-08-15\n13   63        lme4  623554 2022-07-17 2022-08-15\n14   68    corrplot  601404 2022-07-17 2022-08-15\n15   72       rJava  577002 2022-07-17 2022-08-15\n16   75      ggpubr  544839 2022-07-17 2022-08-15\n17   88     cowplot  493972 2022-07-17 2022-08-15\n18   94         car  461659 2022-07-17 2022-08-15\n\n\nFrom some quick research, I have found the following about the new packages:\n\nragg - a 2D library as an alternative to the RStudio default\nrgl - functions for 3D interactive graphics\nrgeos - a geometry package, but is currently planned to be retired at the end of 2023 for the sf package\nzoo - a library to deal with time series\npkgdown - a library fOR building a blog website, I use blogdown\nnloptr - a library for solving non-linear optimization problems\nHmisc - an assortment of different data analysis tools\nlme4 - for fitting linear and generalized linear mixed-effects models\nRcppEigen - integration of the eigen library in R for linear algebra"
  },
  {
    "objectID": "posts/2022-03-31-underrated-cran-packages/index.html#take-away",
    "href": "posts/2022-03-31-underrated-cran-packages/index.html#take-away",
    "title": "Underrated CRAN Packages",
    "section": "Take-away",
    "text": "Take-away\nHopefully your take-way is a simple method to explore R library that you have never heard about. I know that a few of the libraries seem interesting and worth further exploring.\nWhile we are at it, might as well find the daily values for the new packages and plot them for the last month.\n\nnew$package %>%\n        cran_downloads(when = \"last-month\") %>%\n        ggplot(aes(x = date, y = count, color = package)) +\n        geom_line()"
  },
  {
    "objectID": "posts/2022-04-08-benchmarking-data-tables/index.html",
    "href": "posts/2022-04-08-benchmarking-data-tables/index.html",
    "title": "Benchmarking Data Tables",
    "section": "",
    "text": "When I started learning R, I heard vague tales of the use of Data Tables. Really just whisperers, of something to consider in the future after I’ve become more proficient. Well now is the time to learn what if anything I’ve been missing out on."
  },
  {
    "objectID": "posts/2022-04-08-benchmarking-data-tables/index.html#introduction",
    "href": "posts/2022-04-08-benchmarking-data-tables/index.html#introduction",
    "title": "Benchmarking Data Tables",
    "section": "Introduction",
    "text": "Introduction\nData Tables are a potential replacement for the common dataframe. It seeks to perform that same role but with improved performance. I would like to see the speed comparison between Data Frames, Data Tables and Tibbles. I will use the microbenchmark package to perform the actual benchmarking.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(microbenchmark)\nlibrary(farff)\n\nFor the benchmark, I will use the ‘credit-g’ dataset, which can be found on the open ml website. I’m pretty sure the last open ml dataset I used was a csv file, but they seem to have moved to a ARFF format. I will need to use the farff package to load the data.\n\ndf <- farff::readARFF('dataset_31_credit-g.arff')\ndt <- setDT(df)\nti <- tibble(df)"
  },
  {
    "objectID": "posts/2022-04-08-benchmarking-data-tables/index.html#syntax",
    "href": "posts/2022-04-08-benchmarking-data-tables/index.html#syntax",
    "title": "Benchmarking Data Tables",
    "section": "Syntax",
    "text": "Syntax\nThe syntax for Data Tables is a little different:\n\nDT[i,j,by]\n\nIn this manner, a data table can be subset by i, to calculate j when grouped with a by. Along with the special syntax, there are some common functions that add some additional simplification.\n\n.()\n\nThe ‘.()’ function can be used as a placeholder for ‘list()’. The list function is useful for subsetting."
  },
  {
    "objectID": "posts/2022-04-08-benchmarking-data-tables/index.html#grouped-aggregate",
    "href": "posts/2022-04-08-benchmarking-data-tables/index.html#grouped-aggregate",
    "title": "Benchmarking Data Tables",
    "section": "Grouped Aggregate",
    "text": "Grouped Aggregate\nAggregating data in Data Tables is simple by using the j and by parameters in the syntax. Again, multiple functions or even multiple groupings can be passed with the ‘.()’ function. For this comparison, we will look at the performance of finding the average age of the credit holders grouped by the class or credit rating.\n\ngroup <- microbenchmark(Data_Frame = df %>% \n                                 group_by(class) %>%\n                       summarise(avg = mean(age)),\n               Data_Table = dt[,.(avg = mean(age)), by = class],\n               Tibble = ti %>% \n                       group_by(class) %>%\n                       summarise(avg = mean(age)))\nprint(group)\n\nUnit: microseconds\n       expr    min     lq     mean  median      uq     max neval\n Data_Frame 5457.8 5916.5 8281.596 6602.00 8040.75 35931.3   100\n Data_Table  654.2  894.3 2715.814  983.90 1222.40 43101.0   100\n     Tibble 5587.6 6108.6 7880.329 6605.75 7981.15 48245.3   100"
  },
  {
    "objectID": "posts/2022-04-08-benchmarking-data-tables/index.html#taking-counts",
    "href": "posts/2022-04-08-benchmarking-data-tables/index.html#taking-counts",
    "title": "Benchmarking Data Tables",
    "section": "Taking counts",
    "text": "Taking counts\nAnother function of interest is the ‘.N’ function. This function will return the count of rows. The test looks are the number of people with over 5000 in credit and younger than 35.\n\ncounts <- microbenchmark(Data_Frame = df %>% \n                                 filter(credit_amount > 5000, age <35) %>%\n                       nrow(),\n               Data_Table = dt[credit_amount > 5000 & age < 35, .N ,],\n               Tibble = ti %>% \n                       filter(credit_amount > 5000, age <35) %>%\n                       nrow())\nprint(counts)\n\nUnit: microseconds\n       expr    min     lq      mean   median       uq     max neval\n Data_Frame 8465.8 8789.4 11712.951  9627.60 11767.20 48873.5   100\n Data_Table  285.9  336.7   500.222   463.95   510.25  2196.7   100\n     Tibble 9031.1 9421.9 13459.711 10219.80 12669.95 64585.3   100"
  },
  {
    "objectID": "posts/2022-04-08-benchmarking-data-tables/index.html#creating-new-columns",
    "href": "posts/2022-04-08-benchmarking-data-tables/index.html#creating-new-columns",
    "title": "Benchmarking Data Tables",
    "section": "Creating new columns",
    "text": "Creating new columns\nData Tables also contain a very simple syntax for creating a new column with ‘:=’. I compare this to the tidyverse mutate function. Using the base R to create a column is still the fastest method, taking about half the time of the Data Table method.\n\nnew <- microbenchmark(Data_Frame = df %>% mutate(property = paste(property_magnitude, housing)),\n               Data_Table = dt[,property := paste(property_magnitude, housing)],\n               Tibble = ti %>% mutate(property = paste(property_magnitude, housing)))\nprint(new)\n\nUnit: microseconds\n       expr    min      lq     mean  median      uq     max neval\n Data_Frame 2074.8 2186.05 3169.626 2318.60 2627.85 29837.9   100\n Data_Table  502.3  563.00  843.560  668.55  776.45 11603.2   100\n     Tibble 2576.8 2838.50 4705.342 3051.55 3986.85 41499.7   100"
  },
  {
    "objectID": "posts/2022-04-08-benchmarking-data-tables/index.html#chaining-data-tables",
    "href": "posts/2022-04-08-benchmarking-data-tables/index.html#chaining-data-tables",
    "title": "Benchmarking Data Tables",
    "section": "Chaining Data Tables",
    "text": "Chaining Data Tables\nAnother point of exploration is that Data Tables can be chained together to create more complicated structures\n\ndt[credit_amount > 1000, .(age = mean(age)),by = .(purpose, class)][class == \"good\" & age < mean(age)]\n\n               purpose class      age\n1:            radio/tv  good 35.44865\n2: furniture/equipment  good 33.21930\n3:            used car  good 36.91860\n4:            business  good 34.50000\n5:  domestic appliance  good 35.50000\n6:          retraining  good 34.00000\n\n\nI don’t think this is the most useful feature, as you can already create some very complicated transformation with a single call. Chaining also makes it more difficult to understand."
  },
  {
    "objectID": "posts/2022-04-08-benchmarking-data-tables/index.html#conclusions",
    "href": "posts/2022-04-08-benchmarking-data-tables/index.html#conclusions",
    "title": "Benchmarking Data Tables",
    "section": "Conclusions",
    "text": "Conclusions\nIt is clear that there are significant performance improvements when using Data Tables versus Data Frames (an average decrease of time by -67%). There are also insignificant differences between Data Frames and Tibbles. Also, the syntax for Data Tables is fairly simple and straight forward and yet extremely powerful.\nSo, to answer the most important question, should you change to Data Tables from Data Frames? Probably, they present a significant performance gain and their structure is very flexible.\n\nPhoto by Tyler Clemmensen on Unsplash"
  },
  {
    "objectID": "posts/2022-04-11-r-bloggers-site/index.html",
    "href": "posts/2022-04-11-r-bloggers-site/index.html",
    "title": "R-Bloggers site",
    "section": "",
    "text": "I would like to take the time to mention the r-bloggers site. It is a vast collection of Blogs on everything that has to do will the R language. I would very much like to contribute to their work with this blog.\nJust to keep it interesting, I would like to recommend this specific post by The Jumping Rivers Blog about upcoming R conferences. I’ve attended a few digital R Conferences, hosted by R-Studio, and they are very rewarding experiences. They have the ability to both inspire and educate. Unfortunately, there are no conferences in my native Canada.\n\nPhoto by Andrew Neel on Unsplash"
  },
  {
    "objectID": "posts/2022-04-14-merging-pdfs-with-python/index.html",
    "href": "posts/2022-04-14-merging-pdfs-with-python/index.html",
    "title": "Merging PDFs with Python",
    "section": "",
    "text": "I am currently looking for a new job, which means I need to create many resumes and cover letters. When creating a resume, it is good practice to create a PDF file. PDFs cannot be edited, which can make them difficult to work with, but they retain their formatting. It is impossible to tell which version of Microsoft Word a hiring manager is using. So you have to risk a possible formatting error or create a compatible resumes without the latest features.\nOne issue with using PDFs is that employers will sometimes ask for a cover letter and resume to be submitted as a single PDF. This wouldn’t be an issue if they were both stored in the same document, but if you are like me, you have separate documents creating separate PDFs. You can always use free online PDF mergers, but they can have limitations, and it may not be desirable to give away your personal data. So I decided to create a small Python script that will merge my PDFs together.\nThe script will require the PyPDF2 package for dealing with PDFs and the os package. The os package is just used to automatically merge all PDFs in the root folder.\n\nimport PyPDF2, os\n\nThe first step is to create a list of the PDFs in the current folder. It also ensures that the merged PDF is not in the list.\n\npdfiles = []\n\nfor filename in os.listdir('.'):\n        if filename.endswith('.pdf'):\n                if filename != 'merged.pdf':\n                        pdfiles.append(filename)\n                        \npdfiles.sort(key = str.lower)\n\nThe file list is also sorted alphabetically to ensure the results are predictable and easy to control. The merged PDF will contain the PDFs in the same order.\nThe next step is to create a PdfFileMerger object, which will be the destination for all the data in the PDFs. The first step is to open the first PDF file in the PDF file list. The PdfFileMerger object will only accept a file object, so we need to create a PdfFileReader object from the opened PDF. This PdfFileReader object will then be appended to the PdfFileMerger object. We proceed then to the next PDF. After all files are added, the write method is used on the merged object to create a merged PDF.\n\npdfMerge = PyPDF2.PdfFileMerger()\n\nfor filename in pdfiles:\n        pdfFile = open(filename, 'rb')\n        pdfReader = PyPDF2.PdfFileReader(pdfFile)\n        pdfMerge.append(pdfReader)\n\npdfFile.close()\npdfMerge.write('merged.pdf')\n\nAnd that’s everything, a simple Python script that creates a merged PDF from all PDFs in the root folder. It is important to remember that a PDF file needs to be opened, and then a file object can be created. Using the regular PDF file will not work.\n\nPhoto by krakenimages on Unsplash"
  },
  {
    "objectID": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html",
    "href": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html",
    "title": "Dashboards in R with Shiny Dashboard",
    "section": "",
    "text": "In a previous post, I explore the Flex dashboard library for the creation of a clean and interactive dashboard. That post can be found here. Unknown to me at the time, but I sort of skipped over the more natural progression of creating a dashboard with R Shiny. This is my attempt to recreate that dashboard and compare the ease of creation and functionality of the Shinydashboard library."
  },
  {
    "objectID": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#application-structure",
    "href": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#application-structure",
    "title": "Dashboards in R with Shiny Dashboard",
    "section": "Application Structure",
    "text": "Application Structure\nMy shiny dashboard will at heart be a shiny application. I will use the single App file structure rather than the separate UI/Server file structure. The R Studio site has a very good outline on the basic structure for a Shiny Dashboard.\nMY application will then be broken down into the following structure:\n\nData Collection\nPlot Creation\nUI\nServer\n\nBefore starting to write the Shiny App, the raw data needs to be cleaned and setup to increase application performance."
  },
  {
    "objectID": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#data-preparation",
    "href": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#data-preparation",
    "title": "Dashboards in R with Shiny Dashboard",
    "section": "Data Preparation",
    "text": "Data Preparation\nThe data is the same from the previous dashboard, which was the level of donations for Canadians to charities. The raw data can be found here.\nThe following code is used to clean up the raw data and is not included in the Shiny App. This code is just used to create an RDS file (compressed data file) that the Shiny App will more easily load.\n\nlibrary(tidyverse)\n# Download the data and unzip\ndownload.file(\"https://www150.statcan.gc.ca/n1/tbl/csv/45100007-eng.zip\", \"donordata.zip\")\nunzip(\"donordata.zip\")\n\n# Read the data into R\ndata = read_csv(\"45100007.csv\")\n\n# Clean up the data\ndata = data %>%\n        filter(`Donation statistics (UOM)`=='Average annual donations') %>%\n        filter(!GEO==\"Canada\") %>%\n        filter(!Education == \"All education levels\") %>% rename(Province = GEO)\ndata$Province[data$Province == \"Newfoundland and Labrador\"] <- \"Newfoundland\"\ndata$Province[data$Province == \"Prince Edward Island\"] <- \"P.E.I.\"\n\n# Saved the clean data as an RDS file\nsaveRDS(data, 'data.rds')"
  },
  {
    "objectID": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#data-collection",
    "href": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#data-collection",
    "title": "Dashboards in R with Shiny Dashboard",
    "section": "Data Collection",
    "text": "Data Collection\nWith the data preparation completed, we can now start with writing the application. Shiny Application can be broken down to two parts, reactive and nonreactive. It is important to keep our calculations in the nonreactive part if their values do not change because it will be very demanding on system resources otherwise.\n\n# Loading the libraries\nlibrary(shiny)\nlibrary(shinydashboard)\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(rgdal)\nlibrary(gt)\n\n# Data is loaded into the shiny app from the previously generated RDS file\ndata = readRDS('data.rds')\n\n# Summary data is created from the loaded data and saved as data2\ndata2 <- data %>%\n        group_by(Province) %>%\n        summarise(Donations = sum(VALUE))\n\n# For regional information for mapping, the rgdal library is used.\nlibrary(rgdal)\n\n# The following code will download a regional outlines for maps if the file doesn't exist on the system \nif (!file.exists(\"./src/ref/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.dbf\")){\n        download.file(file.path('https://www.naturalearthdata.com/http/',\n                                'www.naturalearthdata.com/download/50m/cultural',\n                                'ne_50m_admin_1_states_provinces_lakes.zip'), \n                      f <- tempfile())\n        unzip(f, exdir = \"./src/ref/ne_50m_admin_1_states_provinces_lakes\")\n        rm(f)\n}\n\n# The regional data is then loaded into R and some data is edited to make it more inline with the regional data\nregion <- readOGR(\"./src/ref/ne_50m_admin_1_states_provinces_lakes\", 'ne_50m_admin_1_states_provinces_lakes', encoding='UTF-8', verbose = FALSE\n                  )\ndata2$Province <- c(\"Alberta\", \"British Columbia\", \"Manitoba\", \"New Brunswick\", \"Newfoundland and Labrador\", \"Nova Scotia\", \"Ontario\", \"Prince Edward Island\", \"QuÃ©bec\", \"Saskatchewan\")"
  },
  {
    "objectID": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#plot-creation",
    "href": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#plot-creation",
    "title": "Dashboards in R with Shiny Dashboard",
    "section": "Plot Creation",
    "text": "Plot Creation\nJust as the data was collected in the nonreactive section, so should the plot creations. This doesn’t mean that the plots won’t be interactive, just that their designs will remain static.\n\nbar_plot <- data %>%\n        group_by(Province) %>%\n        summarise(Donations = sum(VALUE)) %>%\n        ggplot(aes(x = Province, y = Donations, fill = Province)) +\n        geom_bar(stat = \"identity\", show.legend = FALSE) +\n        theme(axis.text.x = element_text(angle = 90), legend.position='none')\n\n# This call was added for illustration\nbar_plot\n\n\n\nedu_plot <- data %>%\n        group_by(Education) %>%\n        rename(Donations = VALUE) %>%\n        ggplot(aes(y= Donations, x = Education, fill = Education)) +\n        geom_boxplot() +\n        theme(axis.title.x=element_blank(),\n              axis.text.x=element_blank(),\n              axis.ticks.x=element_blank())\n\n# This call was added for illustration\nedu_plot\n\n\n\npie_plot <- data %>%\n        group_by(Province) %>%\n        summarise(Donations = sum(VALUE)) %>%\n        ggplot(aes(x = '', y = Donations, fill = Province)) +\n        geom_bar(stat = \"identity\", width = 1) +\n        coord_polar(\"y\", start = 0) +\n        theme_void()\n\n# This call was added for illustration\npie_plot\n\n\n\nmap_leaf <- leaflet() %>% \n        addTiles() %>% \n        setView(-74.09, 45.7,  zoom = 2) %>% \n        addPolygons(data = subset(region, name %in% data2$Province), color = \"#444444\", opacity = 1.0, fillOpacity = 0.75,\n                    fillColor = ~colorQuantile(\"Greens\", data2$Donations)(data2$Donations),\n                    weight = 1)\n\n# This call was added for illustration\nmap_leaf\n\n\n\n\ngt_table <- data2 %>% \n        pivot_wider(names_from = Province, values_from = Donations) %>%\n        gt()\n\n# This call was added for illustration\ngt_table\n\n\n\n\n\n  \n  \n    \n      Alberta\n      British Columbia\n      Manitoba\n      New Brunswick\n      Newfoundland and Labrador\n      Nova Scotia\n      Ontario\n      Prince Edward Island\n      QuÃ©bec\n      Saskatchewan\n    \n  \n  \n    3350\n2411\n2928\n1359\n1486\n1580\n2084\n1982\n1039\n2637"
  },
  {
    "objectID": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#ui",
    "href": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#ui",
    "title": "Dashboards in R with Shiny Dashboard",
    "section": "UI",
    "text": "UI\nThe following code creates the UI interface that the user interacts with. For those familiar to Shiny Apps, the structure will be similar. The actual functions used will be different, but usually their names will just have the ‘dashboard’ prefix. For those not familiar with Shiny Apps, the UI is created by a nesting of a series of functions. The notation can get a little difficult as you need to remember to use ‘,’ when passing multiple functions. Though not used in this dashboard, UI interfaces can be used to control elements of the display. These controls create variables that need to be saved as input$\"variable_name\". This allows the transfer of information from the UI to the server side. Likewise, plots and figures from the server side need to be saved as output$\"variable_name\". When an output variable is referenced in the UI, such as the plotOutput function for displaying plots, they are referenced as “variable_name” in quotes.\nAgain, the syntax can be difficult to learn and use, but the R Studio post on Shiny Dashboards provide the basic skeleton to learn from. My biggest issue is keeping track of all the ‘)’ and ‘,’ used.\n\n# The main page structure is the dashboardPage\nui <- dashboardPage(\n        \n# The header is simple the header for the dashboard\n        dashboardHeader(title = \"Donations in Canada Dashboard\"),\n\n# The sidebar functions adds the controls for flipping through dashboard pages        \n        dashboardSidebar(\n                \n# Each menu item is a link that will apear on the sidebar\n# There are a bunch of quality icons that the icon function can return \n                menuItem(\"Dashboard\", tabName = \"gauge\", icon = icon(\"dashboard\")),\n                menuItem(\"Education\", tabName = \"education\", icon = icon(\"school\")),\n                menuItem(\"Map\", tabName = \"map\", icon = icon(\"map\"))),\n        \n# The next section includes the body or the main section of the dashboard\n        dashboardBody(\n# In the main body, each of the tabs in the side bar need to be referenced. The tabItems function is the container for all the tabs\n                tabItems(\n                        \n# Each previous mentioned tab, will have a separate tabItem.\n                        tabItem(tabName = \"dashboard\",\n\n# The fluidRow function is used to align the elements on the page. Through trial and error, I have found this to be the element for changing the size of plots.\n                                fluidRow(\n                                        \n# A box is drawn around each plot. Each plot/figure/element is referenced here with the plotOutput function or a likewise function. In the server side, I will transform the ggplots into plotly plots to create interactive plots so the plotlyOutput function is used. \n                box(plotlyOutput(\"plot1\"), width = 6),\n                \n# The pie plot is a regular ggplot object, so it can be passed to the plotOutput function\n                box(plotOutput(\"pie\"), width = 6))),\n                \n                tabItem(tabName = \"education\",\n                        fluidRow(\n                                box(plotlyOutput(\"plot3\"), width = 12))),\n                \n# For the map tab, the leafletOutput function is used to display the leaflet map created in the server side. \n                tabItem(tabName = \"map\", \n                        fluidRow(\n                                box(leafletOutput(\"plot2\"), width = 8)\n                                ),\n                        \n# On a second row, under the map, a GT table requires the gt_output function\n                        fluidRow(\n                                box(gt_output(\"table\"), width = 12)\n                        ),\n                        )\n                )\n        )\n)"
  },
  {
    "objectID": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#server",
    "href": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#server",
    "title": "Dashboards in R with Shiny Dashboard",
    "section": "Server",
    "text": "Server\nThe server is the second major function in the Shiny App. Rather than using the output series of functions, it requires the paired version of the render function. Any controls from the UI would need to be passed as part of the input variable.\n\n# The same structure for any other Shiny App\nserver <- function(input, output) {\n\n# Any data to pass back to the UI, such as figures or plots, will need to be saved as part of the output. Anything within the {} brackets becomes an interactive element. If they are not present, the plot will not change.\n        output$plot1 <- renderPlotly({\n                \n# The ggplot for the barplot is converted to an interactive plotly plot.\n                        ggplotly(bar_plot)\n                })\n\n        output$plot2 <- renderLeaflet({map_leaf})\n        \n        output$plot3 <- renderPlotly({ggplotly(edu_plot)})\n        \n# The GT table requires the render_gt function which pairs with the gt_output function in the UI.\n        output$table <- render_gt({gt_table})\n        \n        output$pie <- renderPlot({pie_plot})\n}"
  },
  {
    "objectID": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#finalized-dashboard",
    "href": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#finalized-dashboard",
    "title": "Dashboards in R with Shiny Dashboard",
    "section": "Finalized Dashboard",
    "text": "Finalized Dashboard\nThe finalized dashboard looks pretty good. The sidebar adds a good level of control to the user and makes the data better organized. I also enjoy the default styling, but I’m sure it can be changed.\nWhen compared to the dashboard created with flexdashboard, it is much nicer looking. The flex dashboard, however, has the advantage of being much easier to create with r markdown. It is also much more flexible as it produces an HTML file which is easy to manage. The shiny dashboard needs to be hosted, which makes it slightly clunky.\nSpeaking of the clunky nature of Shiny Apps, you also need to be conscious of what needs to be reactive and what doesn’t. It is very easy to slow down a Shiny Dashboard with a bunch of unnecessary calculations. The writing of the application itself is also quite clunky. It doesn’t compare to the ease of writing in R markdown.\nSo which dashboard do I prefer? Well I think for a quick solution, flex dashboard is great but the amount of control and styling of a shiny dashboard is just better. I do think that there is a place for both, but if I had to choose, I would go for a Shiny Dashboard."
  },
  {
    "objectID": "posts/2022-05-01-level-up-your-progamming-skills/index.en.html",
    "href": "posts/2022-05-01-level-up-your-progamming-skills/index.en.html",
    "title": "Level up your programming skills",
    "section": "",
    "text": "How do you become a better programmer? Well, there is strong scientific evidence for the support of the principle of deliberate practice. Deliberate practice is a method of skill development first written by Anders Ericsson in the book “Peak: Secrets from the New Science of Expertise”. I would also recommend reading “Talent Is Overrated: What Really Separates World-Class Performers from Everybody Else” by Geoff Colvin."
  },
  {
    "objectID": "posts/2022-05-01-level-up-your-progamming-skills/index.en.html#deliberate-practice",
    "href": "posts/2022-05-01-level-up-your-progamming-skills/index.en.html#deliberate-practice",
    "title": "Level up your programming skills",
    "section": "Deliberate Practice",
    "text": "Deliberate Practice\nDeliberate Practice can be summarized to the following points:\n\nTalent is not enough, and to become great at a task requires a lot of practice and repetition.\nDeliberate practice is hard-work, in order strengthen your skills through practice you need to be challenged. This means that repetition by itself will not develop skill. This also means you need to constantly increase the challenge of your practice as you become better at it.\nFocus plays a large role in deliberate practice. This connects to the previous point for the required challenge of practice. This can also tie into the principles of flow, as best described by Mihaly Csikszentmihalyi in his book “Flow: The Psychology of Optimal Experience”.\nSetting goals becomes a powerful motivator. With the completion of a goal, there is a release of endorphins, which cause a sense of satisfaction. Goals are can also be utilized to increase the difficulty of practice, making an otherwise easy task a challenge.\nFeedback is important. Feedback provides motivation by comparing current to previous performance."
  },
  {
    "objectID": "posts/2022-05-01-level-up-your-progamming-skills/index.en.html#application-for-programmers",
    "href": "posts/2022-05-01-level-up-your-progamming-skills/index.en.html#application-for-programmers",
    "title": "Level up your programming skills",
    "section": "Application for programmers",
    "text": "Application for programmers\nSo, how can programmers incorporate the principles of deliberate practice? I’ve recently been recommended the site Hacker Rank and I can say it is fantastic. The Hacker Rank site provides a wide array of challenges for programmers of varying skills levels. There is a selection of different topics from Algorithms to Regular Expressions.\nSo how does Hacker Rank fit in with deliberate practice? Well, there certainly are a good level of challenges to work through. There a three different levels per topic, with challenges in the same difficulty having nothing in common.\nThe design of the site is quite simple, with very little to distract you from your challenge. There is the option for a dark theme if you are, like me, a person of sophistication. It also pretty easy to have the problem on one side of your screen with your programming on the other.\nThere are multiple built-in goals to work on. Certifications for each topic to test your current skill level and to advertise to potential employers. There a preparation kits for interviews with time frames between 1 and 12 weeks, which ever is most convenient for you.\nThere is immediate feedback from assignments, with automated program testing. A leader board is provided for the most competitive, who are interested in their global ranking. You also get feedback from the built-in IDE on your programming errors."
  },
  {
    "objectID": "posts/2022-05-01-level-up-your-progamming-skills/index.en.html#site-criticisms",
    "href": "posts/2022-05-01-level-up-your-progamming-skills/index.en.html#site-criticisms",
    "title": "Level up your programming skills",
    "section": "Site Criticisms",
    "text": "Site Criticisms\nI do enjoy the site, but I still have some minor issues. To its credit, the site does support multiple programming languages, even different versions of the same programming language. This does, however, make it difficult to following along with the tutorials if they are done in a language that you are not familiar with.\nAlso, at this time, there doesn’t seem to be support to retake certification exams for some topics. I myself, had failed the R basic certification as I am more used to using the tidyverse package rather than base R.\nI can’t really speak on behalf of the incorporation of potential employers in it. But that does seem like a very promising idea. I still think it is a great to tool for the programming community, and I will continue to utilize it for my personal skill development, as it can easily provide a source of deliberate practice."
  },
  {
    "objectID": "posts/2022-05-09-formatting-our-outout-with-python-s-f-strings/index.html",
    "href": "posts/2022-05-09-formatting-our-outout-with-python-s-f-strings/index.html",
    "title": "Formatting our output with Python’s F strings",
    "section": "",
    "text": "I have recently been on a tear of different challenges on the site HackerRank. I am about halfway through their 30 days of code and 10 days of statistics. These challenges often require to output number to a certain a number of significant digits. I’ve always thought that the round function can be used for this, but I am wrong. The F string seems to be a powerful tool to accomplish this, and worth your time learning if you are unfamiliar."
  },
  {
    "objectID": "posts/2022-05-09-formatting-our-outout-with-python-s-f-strings/index.html#structure-of-an-f-string",
    "href": "posts/2022-05-09-formatting-our-outout-with-python-s-f-strings/index.html#structure-of-an-f-string",
    "title": "Formatting our output with Python’s F strings",
    "section": "Structure of an F string",
    "text": "Structure of an F string\nThe formatting of an F string starts with a f prior to quotations, whether they be single or double quotes. Any variable can then be included within a series of {}. This formatting can make it easier than turning values into strings and concatenating all strings into a single line of text. This is easily demonstrated with a large mix of values and strings.\n\nx = 1/3\ny = 1/6\n\nprint(\"The value is \" + str(x) + \" is greater than \" + str(y))\nprint(f\"The value is {x} is greater than {y}\")\n\nThe value is 0.3333333333333333 is greater than 0.16666666666666666\nThe value is 0.3333333333333333 is greater than 0.16666666666666666\n\n\nThe values can then be formatted with : after the variable name. The number of digits prior and post the decimal can then be specified. The f is added after the decimal formatting to ensure the value is treated as a float.\n\nprint(f\"The value is {x:.3f} is greater than {y:.2f}\")\n\nThe value is 0.333 is greater than 0.17\n\n\nThe values passed are not specific to the number of digits, but the minimum number of spaces. This means you can ensure specific space aligned, such as for a table, by including these values.\n\nz = [10000, 500, 10, 0.001, .1]\nfor i in z:\n        print(f\"the value is: {i:5}\")\n\nthe value is: 10000\nthe value is:   500\nthe value is:    10\nthe value is: 0.001\nthe value is:   0.1\n\n\nAdditionally, we can add leading zeros by adding zero prior to the number of digits.\n\nfor i in z:\n        print(f\"the value is: {i:05}\")\n\nthe value is: 10000\nthe value is: 00500\nthe value is: 00010\nthe value is: 0.001\nthe value is: 000.1"
  },
  {
    "objectID": "posts/2022-05-09-formatting-our-outout-with-python-s-f-strings/index.html#alternative-formatting",
    "href": "posts/2022-05-09-formatting-our-outout-with-python-s-f-strings/index.html#alternative-formatting",
    "title": "Formatting our output with Python’s F strings",
    "section": "Alternative formatting",
    "text": "Alternative formatting\nThere are a few alternative methods for f strings. From my understanding, they are not as fast when it comes to performance. I don’t think that is of particular importance. If your script needs a high level of performance, than you probably don’t want many print statements.\n\nFormat Method()\nThe format method is very similar to f strings with the use of the {}. The string is not preceded by f and the {} can remain empty or contain position indexing. The values are then added in the .format function after the string. The order of the variable in the string will correspond with the number used in the {}, if used at all.\n\nprint(\"The value is {} is greater than {}\".format(1/3, 1/6))\n\nThe value is 0.3333333333333333 is greater than 0.16666666666666666\n\n\n\n\nOld % Method\nThe Old % operator (modulo) replaces the value in the string. Formatting details, such as those previously discussed, are entered after the %. The variables or values are then entered after the string when preceded by another %. Multiple values can be passed.\n\nprint(\"The value is %5.3f is greater than %5.3f\" %(x,y))\n\nThe value is 0.333 is greater than 0.167"
  },
  {
    "objectID": "posts/2022-05-09-formatting-our-outout-with-python-s-f-strings/index.html#conclusions",
    "href": "posts/2022-05-09-formatting-our-outout-with-python-s-f-strings/index.html#conclusions",
    "title": "Formatting our output with Python’s F strings",
    "section": "Conclusions",
    "text": "Conclusions\nWhichever method you decide, it probably won’t make a huge difference. The important part is to understand is the actual formatting. F strings also seem to make it easier to understand the code, as the actual values are inline with the string and the formatting.\n\nPhoto by Sigmund on Unsplash"
  },
  {
    "objectID": "posts/2022-05-16-the-beauty-of-list-comprehensions-in-python/index.html",
    "href": "posts/2022-05-16-the-beauty-of-list-comprehensions-in-python/index.html",
    "title": "The beauty of List comprehensions in Python",
    "section": "",
    "text": "I have spent awhile learning Python, and I was a little perplexed when it came to list comprehensions. Why would you use them? Isn’t there just an easier why?\nAs my proficiency increase, I have found them to be an incredibly useful tool. They save you lines of code, are easy to understand, and are usually better for performance. A good list comprehension, is truly a work of beauty."
  },
  {
    "objectID": "posts/2022-05-16-the-beauty-of-list-comprehensions-in-python/index.html#structure",
    "href": "posts/2022-05-16-the-beauty-of-list-comprehensions-in-python/index.html#structure",
    "title": "The beauty of List comprehensions in Python",
    "section": "Structure",
    "text": "Structure\nThe basic structure of a list comprehension is pretty simple, you contain an expression and an iterable within a set of []. Depending on the type of brackets used, you can create a list, a generator, set or a dictionary.\n\n[i for i in range(5)]\n(i for i in range(5))\n{i for i in range(5)}\n\n{0, 1, 2, 3, 4}\n\n\nIt may appear from first impressions that a list comprehension is a simple one line for loop, but it is much more powerful than that.\n\nConditions\nMuch more complicated lists can be created with an included if statement. The if statement fits right at the end of the statement.\n\n[a for a in range(10) if a % 2 == 0]\n\n[0, 2, 4, 6, 8]\n\n\nBut what if you need to create an even more complicated list, one that requires an else statement along with the if statement. Then the structure of the list comprehension changes a little, the iterable statement is moved to the end.\n\n[a if a % 2 == 0 else 0 for a in range(10)]\n\n[0, 0, 2, 0, 4, 0, 6, 0, 8, 0]\n\n\n\n\nExpressions\nOf course, expressions can be more complicated than returning single values. One common issue I find is when I have a list of a value type and I need them to be of a different type. This conversion is easy with list comprehensions.\n\na = ['0', '1', '2', '3', '4']\n[int(x) for x in a]\n\n[0, 1, 2, 3, 4]\n\n\nThere is nearly an unlimited potential of different expressions you can use.\n\n\nMore Iterables\nList comprehensions are not limited to a single iterable. Far warning, however, increasing the number of iterables will reduce readability. At some level of complication, it will be a better idea to separate steps.\n\na = range(5)\nb = [5,10,15]\n[x*y for x in a for y in b]\n\n[0, 0, 0, 5, 10, 15, 10, 20, 30, 15, 30, 45, 20, 40, 60]\n\n\nThe results are an element-wise evaluation across multiple iterables. These iterables don’t need to be the same size.\n\n\nDictionary Comprehensions\nAs previously mentioned, by changing the structure, we can generate dictionaries.\n\n{char : num for num, char in enumerate(['a','b','c','d','e'])}\n\n{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4}\n\n\nLikewise, you can create a set rather than a list. Sets can be useful if you don’t need the data to be in order, and you don’t want any duplicate values.\n\n\nOther Applications\nThere is great potential in list comprehensions. Often I find that I need to create a list of zeroes or of boolean logic of the same size as a current list. This is easy to create, just don’t refer to the iterable within the expression.\n\na = range(5)\n[True for x in a]\n\n[True, True, True, True, True]\n\n\nWhile it may not be best practice, you can nest a list comprehension within another list comprehension.\n\n[x for x in [b for b in range(20) if b %2 == 0] if x %3 == 0]\n\n[0, 6, 12, 18]\n\n\n\n\nConclusions\nHopefully I have won you over with the beauty of list comprehensions. They are simple and clean to create yet extremely flexible in their design. So take a minute, to really appreciate the beauty of list comprehensions.\n\nPhoto by Kelly Sikkema on Unsplash"
  },
  {
    "objectID": "posts/2022-06-08-text-prediction-shiny-app-pt-2/index.html#description",
    "href": "posts/2022-06-08-text-prediction-shiny-app-pt-2/index.html#description",
    "title": "Text Prediction Shiny App pt 2",
    "section": "Description",
    "text": "Description\nThis is the second part for the creation of a text prediction Shiny Application. From the previous post, we have developed and Corpus of text to start creating text prediction applications.\nWe have also explored the corpus, looking at the frequency of words in the vocabulary. It is now time to start to develop ngram models."
  },
  {
    "objectID": "posts/2022-06-08-text-prediction-shiny-app-pt-2/index.html#n-gram-models",
    "href": "posts/2022-06-08-text-prediction-shiny-app-pt-2/index.html#n-gram-models",
    "title": "Text Prediction Shiny App pt 2",
    "section": "N-gram models",
    "text": "N-gram models\nA ngram is a continuous sequence of tokens, where the order is determined by how many tokens are in the sequence. For our purpose, a token is created for each word in a sentence. Other tokens can be created, such as sentence in a paragraph or letters in a word. It really depends on your application needs.\nA line of text can be broken down into ngrams in many ways. For example, the following text:\n“The quick brown fox”\ncan be broken down to the following unigrams:\n\n(“the”)(“quick”)(“brown”)(“fox”)\n\nor to the following bigrams:\n\n(“the quick”)(“quick brown”)(“brown fox”)\n\nor to the following trigrams:\n\n(“the quick brown”)(“quick brown fox”)\n\nor to the single tetragram:\n\n(“the quick brown fox”)\n\nThe process for creating tokens from text, tokenization, drops the text to lower case and removes all punctuation. For this application, I would recommend the unnest_tokens function from the tidytext package.\nNgrams can be used for predictive text by reserving the last word in the ngram as the predicted word."
  },
  {
    "objectID": "posts/2022-06-08-text-prediction-shiny-app-pt-2/index.html#models",
    "href": "posts/2022-06-08-text-prediction-shiny-app-pt-2/index.html#models",
    "title": "Text Prediction Shiny App pt 2",
    "section": "Models",
    "text": "Models\n\nStupid Back-off\nA higher level of n-gram should provide a better predictive quality for our models. However, these higher n-grams have lower levels of occurrences. Each additional word included in the created n-grams, reduce the different possible solutions but should have a higher level of accuracy as there is more context provided to the model.\nWe need to create some shiny functions to help use determine the highest possible ngram model that we can use. The first function, turns the user input in unigram tokens, which does a lot of pre-processing for us. For words not in the vocabulary, we change the values to the ‘’ token, which the models already have included in the ngrams.\nThe final function simply finds the minimum between the length of the user input and the highest level of ngram models. The result will be the highest degree of ngram that we can use. This is often refereed to as the “Stupid Back-off” method, as a higher order ngram is “backed-offed” to a lower level ngram.\n\ntruetext <- reactive({\n        truetext <- input$text %>%\n                tibble(text=.) %>%\n                unnest_tokens(word, text, token=\"ngrams\", n=1)\n        \n        truetext[!truetext$word %in% voc$word,] <- \"unk\"\n        truetext})\n        \n        maxuse <- reactive({\n                min(nrow(truetext()) + 1,maxn)\n                })\n\n\n\nMaximum Likelihood Estimation\nThe maximum likelihood estimation (MLE) is the simplest model to examine. We simply count all the occurrence where the all values from the user input match with the ngrams to the final word in the n-gram. The final for in the ngram is reserved for the predicted estimation.\n\\[p_x = \\frac{C_x}{C}\n\\]\nWhere \\(p_x\\) is the probability that the word x will be predicted, \\(C_x\\) is the count of the word x occurring, and \\(C\\) is the count of all words.\nThe MLE model produces an unbalanced model, where there a many values from the vocabulary that have zero probability of being predicted. We would like to address this issue by developing more complicated models.\nThe following plot is a sample distribution created with the MLE model. The predicted values are sorted into bins based on the first letter of the predicted value. Some bins/letters have no value and therefore will have no probability assigned to them.\n\ndf <- ngrams$three %>%\n        filter(word1 ==\"what\", word2 == \"is\") %>%\n        select(word3, n) %>% \n        right_join(voc, by = c(\"word3\" = \"word\")) %>%\n        mutate(bin = substr(word3,1,1)) %>% \n        group_by(bin)\n\ndf$n[is.na(df$n)] <- 0\n\ndf %>%\n        ggplot(aes(x = bin, y = n)) +\n        geom_bar(stat = \"identity\")\n\n\n\n\n\n\nAdd One Smoothing\nThe simplest way to deal with the issue of zero probability values is to add one to all unseen counts. This is also referred to as Laplace Smoothing.\n\\[p_x = \\begin{cases}\n\\frac{C_x}{C} & C_x > 0 \\\\\n\\frac{1}{C} & C_x = 0\n\\end{cases}\n\\]\nThe plot for the add one model is pretty easy to create from the previous sample. It is clear that there are some values now in each bin, so there is some probability to every word in the vocabulary. The heights of the bins are also increased, as there previously were words in each bin that had 0 occurrences now occurring once.\n\ndf <- ngrams$three %>%\n        filter(word1 ==\"what\", word2 == \"is\") %>%\n        select(word3, n) %>% \n        right_join(voc, by = c(\"word3\" = \"word\")) %>%\n        mutate(bin = substr(word3,1,1)) %>% \n        group_by(bin)\n\ndf$n[is.na(df$n)] <- 1\n\ndf %>%\n        ggplot(aes(x = bin, y = n)) +\n        geom_bar(stat = \"identity\")\n\n\n\n\n\n\nGood Turing\nIn order to understand the Good Turing Smoothing, we need to introduce some new notation, \\(N_C\\), to represent the frequency of frequencies. The frequency of frequencies represents how often a number of occurrences will happen in or distribution. For example, \\(N_0\\) represents the word count in our vocabulary where there are no occurrences of that word in the distribution. \\(N_1\\) then represents the count of the words that have one occurrence. The frequency of frequencies is a one layer of abstraction from our counts. It is helpful to consider our previous plots where we created bins based on the first letter of the predicted word, but instead we are creating bins one how often our predicted words occur.\nTo create these \\(N_C\\) values, we can use the count function. The original values for ‘n’ were created with the count function, we can repeat it over the values of ‘n’ to create a count of counts which I have called ‘nn’. The plot is as expected, there are many words with a low number of counts and a few high count values.\n\ndf <- ngrams$three %>%\n        filter(word1 ==\"what\", word2 == \"is\") %>%\n        select(word3, n) %>% \n        right_join(voc, by = c(\"word3\" = \"word\"))\n\ndf$n[is.na(df$n)] <- 0\n\nNr <- count(df, n, name = \"nn\")\n\n\nNr %>%\n        head()\n\n# A tibble: 6 × 2\n      n    nn\n  <dbl> <int>\n1     0 64330\n2     2    51\n3     3    28\n4     4    13\n5     5     2\n6     6     7\n\n\nThe first intuition of the Good Turing is that the probability of something new, a word with a count of zero, should be assigned the probability for an event that occurred once. For this example, we have the very unlikely event that there are no counts of words that appear once, so we use the next available count(X). This will give the probability of all words with zero count, we will later divide it by the number of words with the count 0.\n\\[P_0 = \\frac{C_1}{C} = \\frac{C_x\\cdot N_x}{\\Sigma C_N\\cdot N_N}\n\\]\nSince we have grouped the words by frequencies, we can use the product of all frequency of the frequencies by their count.\n\ntotal <- sum(Nr$nn*Nr$n)\ntotal\n\n[1] 1449\n\n\nGood Turing requires some additional calculations, so it is beneficial to add some columns to the dataframe at this point.\n\nNr <- Nr %>%\n        arrange(n) %>% \n        mutate(c= 0) %>%\n        mutate(sc = 0) %>%\n        mutate(GT = 0)\n\nThis snippet of code is used to determine the probability for a word with zero count.\n\n#the probability for unseen matches is set to the next value probability\nNr$GT[Nr$n==0] <- Nr$nn[2]*Nr$n[2]/total\n\nAll other counts are to be adjusted. The Good Turing Smoothing is defined by the following equation:\n\\[C^*=\\frac{(C+1)N_{C+1}}{N_C}\\]\nWhere \\(C^*\\) is the adjusted count number. Since the general trend is that the frequencies decrease as the count increases, the term \\(\\frac{N_{C+1}}{N_C}\\) will decrease the value for the count. This is the desired behaviour, as we want that probability to be distributed to zero counts.\nOne major issue that need to be addressed is that the frequency table is not continuous. There are holes as not all counts exist. To overcome this obstacle, we can create a regression model to fill in the missing values. A logistics regression model fits the values much better than a linear model.\n\nZn <- Nr[-1,] %>% add_row(n=Nr$n[nrow(Nr)]+1)\nZr <- Nr[-1,] %>% lm(log(nn)~log(n), data=.) %>% predict(newdata=Zn)\nZr <- exp(Zr)\n\nThe next code chunk can look quite complicated. In this chunk, the corrected count, \\(C^*\\), are calculated. The variable j is used to control whether the regression model is used to substitute the value for \\(N_{C+1}\\).\n\n#creates the new adjusted counts\nj <- 0 \nfor (i in 2:nrow(Nr)) {\n        Nr$c[i] <-  (Nr$n[i]+1)*Nr$nn[i+1]/Nr$nn[i]\n        Nr$c[i][is.na(Nr$c[i])] <- 0\n        Nr$sc[i] <-  (Nr$n[i]+1)*Zr[i]/Zr[i-1]\n        if(Nr$n[i+1]-Nr$n[i] > 1 | i == nrow(Nr)){\n                j <- 1}\n        Nr$GT[i] <-  Nr$c[i]*(1-j) + Nr$sc[i]*j\n        }\n\nThe probabilities at this time need two additional modifications, they need to be normalized as the regression model skews the overall probability and the probabilities need to be divided by the frequency counts to get a word specific probability.\n\n#the specific prop from words with the same count\nNr$GT[Nr$GT < 0] <- Nr$nn[2]/total\nNr$GT <- Nr$GT/sum(Nr$GT)\nNr$GT2 <- Nr$GT/Nr$nn\n\nWe can now plot the completed ngram prediction for the Good Turing Smoothing. The plot looks similar to previous plots, but we plot the probabilities rather than the count values ‘n’.\n\ndf <- ngrams$three %>%\n        filter(word1 ==\"what\", word2 == \"is\") %>%\n        select(word3, n) %>% \n        right_join(voc, by = c(\"word3\" = \"word\")) %>%\n        mutate(bin = substr(word3,1,1)) %>% \n        group_by(bin)\n\ndf$n[is.na(df$n)] <- 0\n\ndf %>%\n        left_join(select(Nr,n,GT2), by = \"n\") %>%\n        ggplot(aes(x = bin, y = GT2)) +\n        geom_bar(stat = \"identity\")\n\n\n\n\n\n\nAbsolute Discounting\nGood Turing Smoothing is an effective model, but man can it be complicated. One observation that you can make when looking at the values for \\(C\\) and \\(C^*\\) is that there is nearly constant discounting. The distribution in our example is skewed, but we can see that the most common value is between 0 and 1.\n\nNr %>%\n        select(c,sc) %>%\n        mutate(diff = c-sc) %>%\n        ggplot(aes(x=diff)) +\n        geom_histogram()\n\n\n\n\nThis would suggest that we could significantly simplify the adjusted counts calculations by subtracting a constant value. The algorithm is described by the following equation:\n\\[p_x = \\frac{C_x - d}{C} + \\lambda \\cdot p_{unigram}\n\\]\nwhere ‘d’ is the discounting amount, \\(\\lambda\\) is the Interpolation rate and \\(p_{unigram}\\) is the unigram probability based on the MLE.\n\ndiscount <- 0.75\nADI <- df %>%\n        ungroup() %>%\n        select(word3, n) %>%\n        mutate(ADI = (n - discount)/sum(n))\n                \nADI$ADI[ADI$ADI < 0 ] <- 0\n\nAs previously mentioned, the unigram probability is calculated by applying the MLE to the unigram counts.\n\nunigram.prop <- ngrams$one %>%\n        mutate(prop = n / sum(n))\n\nThe interpolated weight (\\(\\lambda\\)) can be found by finding the probability that was discounted.\n\nuni.wt <- 1 - sum(ADI$ADI)\n\nADI <- ADI %>% \n        add_column(uni = unigram.prop$prop*uni.wt) %>% \n        mutate(ADI = ADI + uni, .keep = \"unused\")\n\nWe can see that the plot of the probabilities for the absolute discounting is very similar to the Good Turing plot, but it was much easier to understand and calculate.\n\nADI %>%\n        mutate(bin = substr(word3,1,1)) %>% \n        group_by(bin) %>%\n        ggplot(aes(x = bin, y = ADI)) +\n        geom_bar(stat = \"identity\")\n\n\n\n\n\n\nKneser-Ney\nThe issue with Absolute Discounting is the reliance on the unigram probabilities. The unigram probability doesn’t provide any contextual information. We would rather rely on the continuation probability. Rather than looking at how often the word occurs, the continuation probability looks at how many bigrams the word completes. The Kneser-Ney model follows this equation:\n\\[p_x = \\frac{max(C_x - d, 0)}{C} + \\lambda \\cdot p_{continuation}\n\\]\nThe next chunk of code is very similar to the code used in the absolute discounting model.\n\nKNS <- df %>%\n        ungroup %>%\n        select(word3, n) %>%\n        mutate(KNS = (n - discount)/sum(n))\n\nKNS$KNS[KNS$KNS < 0 ] <- 0\ncont.wt <- 1 - sum(KNS$KNS)\n\n\nContinuation Probabilities\nThe following code is used to determine the continuation probabilities. Since the highest order ngram is six, the continuation probability needs to be calculated for six different ngram series.\n\ncont.prop.func <- function(word, ngrams){\n        out <- ngrams %>% \n                filter(.[,ncol(ngrams)-1] == word) %>%\n                nrow() \n        out / nrow(ngrams)\n}\ncont.prop <- list()\ncont.prop$one <- tibble(word=voc$word, prop = ngrams$one$n/sum(ngrams$one$n))\ncont.prop$two <- tibble(word=voc$word, prop = map_dbl(word, cont.prop.func, ngrams=ngrams$two))\ncont.prop$three <- tibble(word=voc$word, prop = map_dbl(word, cont.prop.func, ngrams=ngrams$three))\ncont.prop$four <- tibble(word=voc$word, prop = map_dbl(word, cont.prop.func, ngrams=ngrams$four))\ncont.prop$five <- tibble(word=voc$word, prop = map_dbl(word, cont.prop.func, ngrams=ngrams$five))\ncont.prop$six <- tibble(word=voc$word, prop = map_dbl(word, cont.prop.func, ngrams=ngrams$six))\nsaveRDS(cont.prop, \"cont.prop.rds\")\n\nThe difficulty is with finding the continuation probability. After they are found, it is pretty easy to add them to the model.\n\nKNS$KNS <- KNS$KNS + cont.prop$three$prop*cont.wt\n\nKNS %>%\n        mutate(bin = substr(word3,1,1)) %>% \n        group_by(bin) %>%\n        ggplot(aes(x = bin, y = KNS)) +\n        geom_bar(stat = \"identity\")"
  },
  {
    "objectID": "posts/2022-06-08-text-prediction-shiny-app-pt-2/index.html#shiny-app",
    "href": "posts/2022-06-08-text-prediction-shiny-app-pt-2/index.html#shiny-app",
    "title": "Text Prediction Shiny App pt 2",
    "section": "Shiny App",
    "text": "Shiny App\nWith all the models created, we can bundle it together in a single Shiny Application. This Shiny Application retrieves the user’s input and attempts to predict the next word. A table is generated to summarize the most highly predicted word. Since there are five different models, there are five different rows. A plot is generated for each model where the predicted words are in bins with other words with the same first letter.\n\n\n\n\n\n\nPhoto by Jaredd Craig on Unsplash"
  },
  {
    "objectID": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html",
    "href": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html",
    "title": "Webscraping in R with Rvest",
    "section": "",
    "text": "Web scraping has become an incredibly important tool in data science, as an easy way to generate new data. The main advantage is the automation of some pretty repetitive tasks. Web scrapping can also be a good way of keeping up with new data on a website, assuming it doesn’t have a big change in its HTML structure."
  },
  {
    "objectID": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#introduction",
    "href": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#introduction",
    "title": "Webscraping in R with Rvest",
    "section": "Introduction",
    "text": "Introduction\nThis project is inspired from a YouTube video created by Thu Vu about at data scraping project about the Witcher books series. Her project utilizes python and Selenium. I love the book series and I loved the project idea. I’ve also had it on my backlog to learn the Rvest library for a while, so it seems like a great opportunity to combine these two interests.\nRather than completing the project on the Witcher series, I thought it would be interesting to explore another book series that I love in the Stormlight Archive by Brandon Sanderson. If you are not familiar with the series, it is an epic fantasy story sprawling over four main books at the time of the publishing of this post. Sanderson is a fantastic author and I feel that the Stormlight Archive is his best work.\nFor this project, I will scrap the Coppermind website for all the character names in the series. The Coppermind is a fan made Wiki site that covers the work of Brandon Sanderson. After retrieving all the character names, I will create a graph outlining the relationships between each character. This work will be done in a future post, so please look forward to it."
  },
  {
    "objectID": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#inializaton",
    "href": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#inializaton",
    "title": "Webscraping in R with Rvest",
    "section": "Inializaton",
    "text": "Inializaton\nThe first step is to download the Rvest library. This is done with the following code.\n\ninstall.packages(\"rvest\")\n\nThe rvest package is also installed if you have the tidyverse package installed. Loading the tidyverse package however will not load the rvest package, so they both need to be loaded separately.\n\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")"
  },
  {
    "objectID": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#datascraping",
    "href": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#datascraping",
    "title": "Webscraping in R with Rvest",
    "section": "Datascraping",
    "text": "Datascraping\nTo start the data scraping exercise, we need to save the URL of the website we would like to scrape. This is the URL for the character page for the Stormlight Archives series.\n\nsite <- read_html(\"https://coppermind.net/wiki/Category:Rosharans\")\n\nWhile on the website in your own browser, you right-click on the specific element you’re interested in scrapping and select inspect. This is at least the method used for Firefox, but it should be similar to other browsers.\nFrom here, you have to do a little digging and a little experimentation to determine which HTML elements are important for the character list. It is pretty useful to have a strong understanding of HTML at this point. From my experimentation, I found that the list was contained within a div with the class “mw-category-group”. A div is a generic divider tag in HTML and can represent many things. I selected the elements with the following code:\n\nnames <- site %>% \n        html_elements(\"div.mw-category-group\")\n\nYou use the html_elements command to select the all the elements for a specific HTML tag you pass. The addition of the “.mw-category-group”, specifies the selection to only divs with the specific class. The class is an attribute of the HTML tags, used to identify and group HTML elements together. I have found that this notation is the best way to filter elements.\nWithin the div elements, there is a further sub-structure for an element in the character list. The characters are contained within an <li> tag as a list item and as an <a> tag as a hyperlink within that list item. We can explore further into the HTML structure by selecting these elements. After the final structure is selected, we can use the html_attr function to return an attribute of the selected elements. The ‘title’ attribute stores the character name in the HTML. We could also the html_text2 function to return the text of the hyperlink, but I’ve found that the ‘title’ attribute is better structured.\n\nnames <- names %>%\n        html_elements(\"li\") %>%\n        html_elements(\"a\") %>%\n        html_attr(\"title\")"
  },
  {
    "objectID": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#data-cleaning",
    "href": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#data-cleaning",
    "title": "Webscraping in R with Rvest",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nWe can start exploring the results of the scrapping\n\nprint(head(names))\n\n[1] \"Category:Aimians\"    \"Category:Alethi\"     \"Category:Azish\"     \n[4] \"Category:Emuli\"      \"Category:Herdazians\" \"Category:Iriali\"    \n\n\nOops, the program has captured an additional list that precedes the character list. Through my testing, I have not found a way to distinguish between the two lists from the HTML structure. Thankfully, we can rely on Regular Expressions to complete the job. The unwanted list items all start with “Category:”, so with a single expression of the str_starts from the stringr package we can remove these elements.\n\nnames <- names[!str_starts(names, \"Category:\")]\n\nThe list still requires some additional work, as there are “()” used throughout the list to give additional context. These “()” will not appear in the text, so we need to remove them with a second Regular Expression. Although it is not clear to me, the “(” needs to be double escaped with two “\\” rather than just one.\n\nnames <- str_remove_all(names,\" \\\\(.*\\\\)\")\n\nprint(head(names, 10))\n\n [1] \"Abaray\"        \"Abiajan\"       \"Abrial\"        \"Abrobadar\"    \n [5] \"Abronai\"       \"Abry\"          \"Acis\"          \"Adin\"         \n [9] \"Adis\"          \"Adolin Kholin\"\n\n\nWe can see that the scrapped data is in much better condition. There is still additional work we can do, as the names will sometimes include first and last names. The last names are not particularly important, so we can drop them altogether.\n\nnames <- str_remove_all(names,\" .*\") \n\nprint(head(names, 10))\n\n [1] \"Abaray\"    \"Abiajan\"   \"Abrial\"    \"Abrobadar\" \"Abronai\"   \"Abry\"     \n [7] \"Acis\"      \"Adin\"      \"Adis\"      \"Adolin\"   \n\nnames <- names[!names %in% c(\"Word\",\"She\", \"User:Thurin\")]\n\nNow we can see that the final list is in condition that we can use to better explore the relationships."
  },
  {
    "objectID": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#conclusion",
    "href": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#conclusion",
    "title": "Webscraping in R with Rvest",
    "section": "Conclusion",
    "text": "Conclusion\nWe have scraped the Stormlight Archive character wiki website with the rvest package. We loaded the website with read_html function. Furthermore, we were then able to sort through the different HTML elements with the html_elements to find where the character list is stored. We then obtained the actual names with the html_attr function. The data collected still contained some unwanted data. We were able to remove an additional list, data in parentheses and the last names of all characters. We can now move forward with scrapping the books to identify the strength of relationships between each character.\nPhoto by Paz Arando on Unsplash"
  },
  {
    "objectID": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html",
    "href": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html",
    "title": "Relationship Extraction with Spacyr",
    "section": "",
    "text": "This is the continuation of the previous project, where we scrapped the Cooper Mind website with the rvest package. Please refer to that posting for the necessary steps to obtain the verified character names.\nAs a reminder, this project was inspired by the work of Thu Vu where she created a network mapping of the characters in the Witcher series. I thought it would be interesting to do some recreation of this project, but in R and with the Stormlight Archive book series.\nFor those unfamiliar with the series, it is an epic fantasy story sprawling over four main books at the time of the publishing of this post. Sanderson is a fantastic author and I feel that the Stormlight Archive is his best work."
  },
  {
    "objectID": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#introduction",
    "href": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#introduction",
    "title": "Relationship Extraction with Spacyr",
    "section": "Introduction",
    "text": "Introduction\nSo in a previous post, we created a list of characters which will represent the nodes in our network graph. The next step in the project is to create the edges. The edges represent the relationships between characters. In our graph, we are going to have the edges represent that strength of the relationships between characters. In order to determine these edge values, we will need to perform relationship extraction from the text with the spacyr package.\nThe spacyr package is simply a wrapper for the python spaCy library, with the following functionality:\n\ntokenization\nlemmatizing tokens\nparsing dependencies (to determine grammatical structure)\nextracting form named entities\n\nIt uses the reticulate to create the python environment. I have previously written a post about using the reticulate package for using python code in RMarkdown."
  },
  {
    "objectID": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#initialization",
    "href": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#initialization",
    "title": "Relationship Extraction with Spacyr",
    "section": "Initialization",
    "text": "Initialization\nWe start with the loading of the necessary libraries to complete the project.\n\nlibrary(spacyr)\nlibrary(tidyverse)\nlibrary(data.table)\n#necessary to create a corpus object\nlibrary(readtext)\nlibrary(quanteda)\nlibrary(rainette)\n\nIf you have an environment of python with a version a spaCy, you can pass the destination into the spacy_intialize function. If not, you need to use the spacy_install function to create a Conda environment that will also include the spaCy package. For this project, I let spacyr create the Conda environment for me. This process did take a while for me, so don’t be surprised if it is the same for you.\n\nspacy_install()\n\nI have the name list from the web scraping post saved as a RDS files. RDS files are compressed text files which load quicker and take up much less space than a CSV file.\n\nnames <- read_rds(\"data/names.RDS\")"
  },
  {
    "objectID": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#text-reading",
    "href": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#text-reading",
    "title": "Relationship Extraction with Spacyr",
    "section": "Text Reading",
    "text": "Text Reading\nThe first step is to read all the text files into the system. I found this interesting little snippet of code that allows you to create a list of all the text files in a specific folder. For this project, all the books were stored in a single data folder.\n\nlist_of_files <- list.files(path = \".\", recursive = TRUE,\n                            pattern = \"\\\\.txt$\", \n                            full.names = TRUE)\n\nWith the list of files, we can use the map_df function from the purr package. The purr package is part of the tidyverse package, so we don’t need to load it separately. The map series of functions allows use to pass a vector of values and a function. Each value will then be passed to that function. The _df part of the function is just the requirement that the output be in the format of a dataframe.\nThe same task can be completed with a for loop, but it is much faster in the map function as it utilize vectorization. Vectorization is the strategy of performing multiple operations rather than a single operation at the same time. I am not very familiar with the purr package, so I plan to write a new article on the topic in the near future.\nAfter all the books are read into memory, we need to create a corpus. A corpus is a large body of text, much like a library, for the sorting and organization of books. This is completed with the corpus function from the quanteda package. This corpus structure is necessary to utilize functions from the spacyr package.\nThe organizational structure in the Corpus is why I needed to load the books in with the readtext function from the readtext package. I’ve tried many methods to read the text(readlines, read_Lines, readfile) but none of them performed the proper way for the corpus function. There were plenty of issues, hours of difficulty, which resulted in me referring to the quanteda package website. There I learnt about the readtext function and it worked flawlessly on the first time. Well, I did find an issue with the default encoding not interpreting characters correctly, but this was corrected easily.\nWhen the time came to modelling, issues arose with the size of the Corpus. There is a limitation in spaCy, it will only work with text files less than 100,000,000 characters long. I think that each book was a little over twice that size. So I needed to batch the process by breaking the corpus up into smaller sections. This was done with the split_segments function from the rainette package. The function only accepts a split based on number of sentences, so I arrived at a value of 100,000 sentences per document.\n\ncorpus <- list_of_files %>% \n        map_df(readtext, encoding='utf-8') %>%\n        corpus() %>%\n        rainette::split_segments(segment_size = 100000)\n\n  Splitting...\n\n\n  Done.\n\n\nWith the books read into file, the corpus created and the corpus split into sections, we now have 18 documents. We can proceed to entity modelling with the spaCy functions.\nUnfortunately, we still have size issues, as passing the entire Corpus to be parsed is unaffected by the number of documents. So I needed to create a simple for loop to analyze each document one at a time and bind the results to a data table. Data tables are like data frames, but they have some unique notation and increased performance.\nThe Corpus is parsed with the spacy_parse function. Setting pos and lemma to false should reduce performance time as the function doesn’t need to return dependency POS tag set and lemmatized tokens. The POS tag set refers to the type of word such as Noun, while the lemmatized token is the base of a word, such as for the word “am” would be lemmatized to “be”. The parsing of the corpus takes a very long time.\n\ndf <- corpus[[1]] %>%\n        spacy_parse(pos = FALSE, lemma = FALSE)\n\nFound 'spacy_condaenv'. spacyr will use this environment\n\n\nsuccessfully initialized (spaCy Version: 3.1.3, language model: en_core_web_sm)\n\n\n(python options: type = \"condaenv\", value = \"spacy_condaenv\")\n\nfor (i in 2:length(corpus)){\n        temp <- corpus[[i]] %>%\n        spacy_parse(pos = FALSE, lemma = FALSE) \n        \n        df <- rbind(df,temp)}\nrm(temp)\n\nThe parsing creates an object that acts very similarly as a data table. There is an entry for each word, which is more than what is required for this project. The original data table is preserved, in case we would like to reference a sentence in the corpus, and we create a filtered data table. The data table is filtering the tokens in the names list and by the identified entity, making sure it starts with person.\n\ndfclean <- df %>% \n        filter(token %in% names,\n               str_starts(entity, \"PERSON\"))"
  },
  {
    "objectID": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#relationship-modelling",
    "href": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#relationship-modelling",
    "title": "Relationship Extraction with Spacyr",
    "section": "Relationship modelling",
    "text": "Relationship modelling\nThe final step is to create a model that will connect people in the data table. I have decided to use a sentence windows that creates a connection when two names are mentioned within that window.\nThis is another very time-consuming tasks the requires two for loops. The first loop goes through all 30025 rows and its sentence id. A second for loop that excludes all rows already used in the first loop is used to compare a second sentence id. If the difference between the sentence ids is less than the windows, the tokens for these rows are added to an empty data table. If the difference is greater than the window size, we break the second for loop as all the sentence ids are incremental. It is not a very clean or smooth method, but it works.\n\nwindow_size <- 5\nrelated <- data.table(\"Person1\" = character(), \"Person2\" = character())\n\nfor(i in 1: (nrow(dfclean)-1)){\n        for(j in (i + 1):nrow(dfclean)){\n                if((dfclean$sentence_id[j] - dfclean$sentence_id[i]) < window_size){\n                        \n                        related <- rbindlist(list(related, list(dfclean$token[i], dfclean$token[j])))\n                }\n                \n                else{\n                        break\n                }\n        }\n}\n\nThe following is a sample of the data table we have created to build the relationships.\n\nrelated %>% head()\n\n   Person1 Person2\n1: Shallan  Kabsal\n2: Jezrien Jezrien\n3: Jezrien Jezrien\n4: Jezrien Jezrien\n5: Jezrien Jezrien\n6: Jezrien   Kalak\n\n\nWe can identify two issues with this sample. The first issue is when two of the same names are within the same window. We will have to filter out when ‘Person1’ is equal to ‘Person2’. The second issue is that we would actually like to aggregate the data. We would like a count of when two different names are in the same window. Both of these tasks are easy enough to solve using the built-in data table notation. For more information on data tables, please refer to my previous post on the topic.\n\nrelatedagg <- related[Person1 != Person2,.N,by = c(\"Person1\", \"Person2\")]\n\nrelatedagg %>% head()\n\n   Person1 Person2  N\n1: Shallan  Kabsal  8\n2: Jezrien   Kalak 10\n3:   Kalak Jezrien  9\n4:   Szeth Dalinar 98\n5:   Szeth  Jasnah 11\n6:   Szeth Elhokar  3\n\n\nThe final issue is for the relationships for ‘Person1’ and ‘Person2’ when their places are switched, but that will be dealt with in the next post."
  },
  {
    "objectID": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#conclusion",
    "href": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#conclusion",
    "title": "Relationship Extraction with Spacyr",
    "section": "Conclusion",
    "text": "Conclusion\nWith some hard-work, we were able to create an organized Corpus of all the current 4 Stormlight Archive books. We were able to split this Corpus into smaller sized documents, making them easier to manage. The spacyr library was then used to model entities within the Corpus, identifying the tokens that represent people. The next step was to clean up the results, keeping only the verified characters names as tokens. We then used a model to developed relationships using a window. A relationship was created whenever two character names were mentioned in the same window. We then filtered out characters relationships to themselves and aggregated the data. The clear next step is to actually build the graphs with the characters as nodes and their relationships as edges. But that is a post for another day.\nPhoto by Alec Favale on Unsplash"
  },
  {
    "objectID": "posts/2022-07-12-network-graphs-in-r/index.en.html#introduction",
    "href": "posts/2022-07-12-network-graphs-in-r/index.en.html#introduction",
    "title": "Network Graphs in R",
    "section": "Introduction",
    "text": "Introduction\nNetwork graphs are an important tool for network analysis. They illustrate points, referred to as nodes, with connecting lines, referred to as edges. Since network graphs are such useful tools, there are many options for graph generation. In this posting, I will demonstrate three different techniques for developing network graphs in r.\nThis is part 3 of a series which is based on the Stormlight Archive by Brandon Sanderson. This project was originally inspired by the work of Thu Vu where she created a network mapping of the characters in the Witcher series.\nIn the first part of the project, we scrapped the Coopermind website to create a verified character name list. This scrapping was performed with the rvest package. The list was then cleaned up and saved for further use.\nFor the second part of the project, we read through and analyzed the four books that make up the Stormlight Archive series. The books were read into memory with the readtext package, which fed nicely into the quanteda to create the body of text called a Corpus. Unfortunately, the body of text was so big that we were unable to model all the text, so we divided the Corpus up into smaller documents with the rainette package.\nWith the corpus finally prepped, we feed it into the spacyr package, a frontend for the spaCy python library, to identify the entities. We were able to create a table identifying the entities that were people and filter it by the verified character list. We created a moving window model that would create a connection between two named characters if they were both mentioned within the same window. By aggregating the results of this model, we developed the foundation for a network graph."
  },
  {
    "objectID": "posts/2022-07-12-network-graphs-in-r/index.en.html#initialization",
    "href": "posts/2022-07-12-network-graphs-in-r/index.en.html#initialization",
    "title": "Network Graphs in R",
    "section": "Initialization",
    "text": "Initialization\nThe first step of this process is to load in the necessary packages for the graph generation. The Tidyverse package is always useful for analysis, so I’ve loaded it too. I have read that the different graph packages can interrupt each other, requiring one of them to be loaded at a time. I have not found this to be an issue.\n\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(networkD3)\n\nThe next step is to load in the data that we created in part two of the project. This data represents that relationship between all the verified characters as read through the series of books. Saving and loading data in RDS format is much more convenient than the CSV format, as RDS files are compressed and seem to load faster.\n\ndata <- read_rds(\"StormGraph.RDS\")"
  },
  {
    "objectID": "posts/2022-07-12-network-graphs-in-r/index.en.html#igraph",
    "href": "posts/2022-07-12-network-graphs-in-r/index.en.html#igraph",
    "title": "Network Graphs in R",
    "section": "IGraph",
    "text": "IGraph\nThe first package to explore is the igraph package. This package is not only for plotting graphs, but also includes many tools for network analysis. For our data, we can create a simple network graph with the graph_from_data_frame function. The relationships are not directional, so we pass this information to the function. The graph can then be plotted with the plot function.\n\ngraph <- graph_from_data_frame(data, directed = FALSE)\nplot(graph)\n\n\n\n\nThe graph created is a mess. There are way too many character nodes and way too many relationships created. We need to create a smaller dataset to reduce the amount of information. I reduced the size of the data by taking only the top 98% quantile in relationships. Since the data is stored as a data table, the data table notation is used to create a subset.\n\ndata2 <- data[data$N >= quantile(data$N, p = 0.98),,]\ndata2 %>%\n        graph_from_data_frame(directed = FALSE) %>%\n        plot(layout = layout_with_graphopt)\n\n\n\n\nThe plot created is still difficult to understand, but it much more reasonable. I feel the igraph package is best for graph analysis and exploratory plots. For a more attractive plot, we need to move on to the next package."
  },
  {
    "objectID": "posts/2022-07-12-network-graphs-in-r/index.en.html#tidygraph-and-ggraph",
    "href": "posts/2022-07-12-network-graphs-in-r/index.en.html#tidygraph-and-ggraph",
    "title": "Network Graphs in R",
    "section": "Tidygraph and GGraph",
    "text": "Tidygraph and GGraph\nThe tidygraph and ggraph packages seek to create graphs in the tidyverse-like environment.\n\nlibrary(tidygraph)\n\nCreating a graph with ggraph requires more structure than the previous igraph. The graph requires two data frames, one for nodes and one for edges.\nFor the nodes dataframe, we need a list of all the node names and an ID number for each node. This is achieved by finding the unique values within both columns of data. These values are then passed to the tibble function to create a tibble, a data structure similar to data frames, and then a column for IDs is created with the rowid_to_column function.\n\nnodes <- c(data2$Person1, data2$Person2) %>% \n        unique() %>%\n        tibble(label = .) %>%\n        rowid_to_column(\"id\")\n\nFor the edges dataframe, we need some additional steps. As a reminder, in our subset of data, we have rows with two names and a number to represent the strength of their bond. The character names need to in the form of the node IDs rather than the names. This task is completed with two merges with the node dataframe. The graph can then be created with the tbl_graph function.\n\nedges <- data2 %>%\n        left_join(nodes, by = c(\"Person1\"=\"label\")) %>%\n        rename(from = \"id\") %>%\n        left_join(nodes, by = c(\"Person2\"=\"label\")) %>%\n        rename(\"to\" = \"id\") %>%\n        select(from, to, N)\n\ngraph_tidy <- tbl_graph(nodes = nodes, edges = edges, directed = FALSE)\n\nFor the plotting of the graph, we use the ggraph library. With this package, the graph can act as any other ggplot geom. With an extra step, we can create a centrality feature in our graph. There are a bunch of different centrality measures, but they all represent the level of importance of a node.\n\ngraph_tidy %>%\n        mutate(Centrality = centrality_authority()) %>%\n        ggraph(layout = \"graphopt\") + \n        geom_node_point(aes(size=Centrality, colour = label), show.legend = FALSE) +\n        geom_edge_link(aes(width = N), alpha = 0.8, show.legend = FALSE) + \n        scale_edge_width(range = c(0.2, 2)) +\n        geom_node_text(aes(label = label), repel = TRUE)"
  },
  {
    "objectID": "posts/2022-07-12-network-graphs-in-r/index.en.html#network-d3",
    "href": "posts/2022-07-12-network-graphs-in-r/index.en.html#network-d3",
    "title": "Network Graphs in R",
    "section": "Network D3",
    "text": "Network D3\nThe ggraph has created a better looking plot with a much higher level of customization. It is however a static plot with no level of interaction. I have tried using the ggplotly function from the plotly package it make it more interactive, but many of the ggraph features are not supported.\nTo create an interactive plot, we move to the networkD3 package. This package is based on the D3 JavaScript library to create interactive plots. We can use the same nodes and edges data frames from the ggraph plot. This process does require one adjustment to the node IDs, as the package requires an initial ID of 0 rather than the default r index of 1.\nThe function from the tidygraph, centrality_authority, is only supported for the tidygraph data structure, so we need an alternative function to use with our data frame. This is achieved with the authority.score function from the igraph package. Besides that, we normalize the edge width values, node sizes and set all the parameters for the forceNetwork function.\n\nedges <- edges %>%\n        mutate(from = from -1, to = to - 1) %>%\n        mutate(N = N / 200)\n\nnodes <- nodes %>%\n        mutate(id=id-1) %>%\n        mutate(nodesize = authority.score(graph_tidy)$vector*150)\n        \nforceNetwork(Links = edges, Nodes = nodes, Source = \"from\", Target = \"to\", NodeID = \"label\", Group = \"id\", opacity = 1, fontSize = 14, zoom = TRUE, Value = \"N\", Nodesize = \"nodesize\", opacityNoHover = TRUE)"
  },
  {
    "objectID": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html",
    "href": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html",
    "title": "Quarto: The successor to R Markdown",
    "section": "",
    "text": "RMarkdown has been a staple for any Data Scientist that programs in R. Quarto builds on that, with multiple language support and additional features. Because of its language independent design, Quarto requires an independent installation. (“Quarto,” n.d.)\nI have spent the past week moving my blog from blogdown to quarto. There have been some challenges, but I am pretty happy with the new look. Let’s start with the setup, it’s a little more work than a regular package or module."
  },
  {
    "objectID": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#setup",
    "href": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#setup",
    "title": "Quarto: The successor to R Markdown",
    "section": "Setup",
    "text": "Setup\nThe setup for Quarto is pretty simple. You will need to visit the quarto website to download the Quarto Command Line Interface (CLI). There are step-by-step instructions for your selected text editor. I am most familiar with RStudio for R and VSCode for Python.\nFor Rstudio, it’s pretty much just plug and play now. I did install the Quarto package, but all the commands can be done by the command line interface. Switching from RMarkdown is as simple as saving them as qmd file. The process for Quarto for RStudio can be described by the following process flow:\n\n\nRender qmd in RStudio\n\n\nIt is not much more difficult for VSCode, all you need to do is download the Quarto extension. The process flow is similar to RStudio but uses Jupyter instead of knitr.\n\n\nRender qmd for VSCode\n\n\nWith the setup complete, there should be no differences between text editors."
  },
  {
    "objectID": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#code-chunk-options",
    "href": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#code-chunk-options",
    "title": "Quarto: The successor to R Markdown",
    "section": "Code Chunk Options",
    "text": "Code Chunk Options\nThe first new feature to explore the support for code chuck options within the code chunks. These options would usually live within the code chunk title line. Any supported option can be added with the #| tag. This feature is useful for situations with many options, as it does increase readability.\n```{r}\n#| label: load\n#| include: true\n#| warning: false\n\nlibrary(tidyverse)\ndata(\"msleep\")\n```"
  },
  {
    "objectID": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#code-folding",
    "href": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#code-folding",
    "title": "Quarto: The successor to R Markdown",
    "section": "Code-folding",
    "text": "Code-folding\nOne of the neat new features is code-folding. When this feature is enabled in the qmd YAML, the person viewing the document can hide/unhide code chunks. This can make it easier for them to read the document. Only the code will be hidden, and not the results.\n\nCodeglimpse(msleep)\n\n\nThis feature is enabled by making the following addition to the YAML. You would change the format from HTML to your required format, such as PDF.\n\nCodeformat: \n  html: \n    code-fold: true\n    code-tools: true\n\n\nWith the addition of the code-tools: true parameter, the reader can decide to hide all code chunks from the top of the document."
  },
  {
    "objectID": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#figures",
    "href": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#figures",
    "title": "Quarto: The successor to R Markdown",
    "section": "Figures",
    "text": "Figures\nQuarto provides a bunch of additional tools for displaying figures. You can assign values for captions, sub-captions, width and height. You can even create a figure with multiple plots with separate sub-captions.\n```{r}\n#| label: fig-sleep\n#| fig-cap: \"Sleeping habits of animals\"\n#| fig-subcap:\n#|   - \"Scatter plot of body weight by total sleep\"\n#|   - \"Violin plot of REM sleep by vore\"\n#| layout-ncol: 2\n\nmsleep %>%\n  drop_na(sleep_total, bodywt) %>%\n  ggplot(aes(y= sleep_total, x = bodywt)) +\n  geom_point(color = \"blue\") +\n  theme_minimal()\n\nmsleep %>%\n  group_by(vore) %>%\n  drop_na(sleep_rem, vore) %>%\n  ggplot(aes(y= sleep_rem, x = vore)) +\n  geom_violin(aes(fill = vore)) +\n  theme_minimal()\n```\n\n\n\n\n\n(a) Scatter plot of body weight by total sleep\n\n\n\n\n\n\n(b) Violin plot of REM sleep by vore\n\n\n\n\nFigure 1: Sleeping habits of animals\n\n\nYou can now use cross-referencing for the figure by referencing the figure. This means that in your text, you can refer to the figure number and link to the figure. This will automatically update your figure numbers and is achieved by typing the ‘@’ symbol followed by the figure label. As an example, ‘@fig-sleep’ turns into Figure 1.\nThere is an additional option to let the figures take up the width of the entire page, but I would not recommend using it as it extends beyond the width of the body of your page. It requires the following code:\n```{r}\n#| column: page\n```"
  },
  {
    "objectID": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#code-linking",
    "href": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#code-linking",
    "title": "Quarto: The successor to R Markdown",
    "section": "Code Linking",
    "text": "Code Linking\nA reader may not be familiar with all the functions that you use in your document, so it may be useful to enable code linking. With code linking, a function in a code chunk will have a hyperlink to the documentation for that function. To work in R, this feature requires the xml2 and downlit packages.\n\nCodelm(as.factor(order) ~ sleep_total, data = msleep[complete.cases(msleep),] )"
  },
  {
    "objectID": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#table-of-contents",
    "href": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#table-of-contents",
    "title": "Quarto: The successor to R Markdown",
    "section": "Table of contents",
    "text": "Table of contents\nI think the best feature for Quarto is the floating table of contents. I can’t describe how much time and effort I’ve spent trying to get a floating table of contents in a Blogdown blog. It didn’t work for me, it would require getting deep into the weeds changing the CSS layout for my HUGO theme. It was not worth the effort.\nAdding a floating table of contents in Quarto is simple. Just use the following code in the document YAML:\n\nCodetoc: TRUE\n\n\nOne simple line of code in the YAML and your document has a floating table of contents. There is some additional customization such as the level of headers, location and title.\n\nCodetoc: true\ntoc-depth: 2\ntoc-location: left\ntoc-title: Contents"
  },
  {
    "objectID": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#quarto-vs-blogdown",
    "href": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#quarto-vs-blogdown",
    "title": "Quarto: The successor to R Markdown",
    "section": "Quarto vs Blogdown",
    "text": "Quarto vs Blogdown\nWith my experimentation with Quarto, I decided to move my blogdown blog to Quarto. In theory, this should be a simple switch, with just copying all post from folder to another. Quarto can use rmd files, but they can easily be changed over to qmd files. I decided to switch all my post to the qmd format and include some additional features. The Quarto site has extensive reference information for creating a blog. (“Quarto,” n.d.-)\nI did have an issue with one of my post not rendering correctly. This maybe an issue with compatibility with the stargazer package. In the end, I decided to just remove the post altogether as I could get it to render correctly, and I prefer the gt over the stargazer package for creating good-looking tables."
  },
  {
    "objectID": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#conclusion",
    "href": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#conclusion",
    "title": "Quarto: The successor to R Markdown",
    "section": "Conclusion",
    "text": "Conclusion\nIt is easy to create great looking documents using quarto, whether that be with code in python or R. Quarto supports most of the features in RMarkdown with some fancy new ones. My personal favourite is the floating table of contents. I have also found that rendering a Quarto blog is a much smoother experience than rendering a blogdown blog."
  }
]