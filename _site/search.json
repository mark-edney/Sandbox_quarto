[
  {
    "objectID": "posts/2023-10-30-Custom OpenAI Chatbot Pt2/index.html",
    "href": "posts/2023-10-30-Custom OpenAI Chatbot Pt2/index.html",
    "title": "Custom OpenAI Chatbot Pt2: Fun with Lang Chain",
    "section": "",
    "text": "Photo by Google DeepMind\n\n\n\nIntroduction\nThis is a continuation from a previous post about creating a custom ChatGPT bot with Langchain. From the previous post, we have created a CSV file which contains the text captured from a folder full of different PDF files. We will continue from there, creating a custom ChatGPT bot from that CSV files.\n\n\nLoading packages\nAs with any project, the first step is to load some of the necessary python packages. The load_dotenv function from the dotenv plays a special role in this program. In order to use the Open AI API, we will need to create and Open AI account. With this account, you will receive an API key, which will be connected to any functional calls with your account. You can keep these API keys in your code, but it is better practice to store them in your environmental variables. The environmental variables is a location in windows where you can store some sensitive information that will be only stored on your computer. You can find where your environmental variables are with the windows search, or you can find it under your advanced system properties. Make a new entry in either the user variables or system variables (whichever you prefer) and name it “OPENAI_API_KEY”. The value will be the actual key you get from your Open AI account. After you save your API key, you will need to restart windows. You will need to pay for your API requests for your Open AI account. Using the embedding model turns out to be pretty cheap, while questioning the ChatGPT cost a bit more, usually only a one or two cents per question.\n\n\nCode\nimport os\nimport sys\nimport pandas as pd\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\nFalse\n\n\n\n\nDocument Loading\nThe next step is to load your CSV files into a document format used by Lang Chain. I found that the best way to complete this is with the DirectoryLoader and the CSVLoader functions. I have also found that it is important to specify the encoding used to create the CSV files from the pandas dataframes. Furthermore, I’ve also included an additional step which will add 1 to the row numbers in the metadata. The row value represents the page number within the document, but the default indexing for a pandas dataframe starts at 0 and not at 1. Getting the length of our documents represents the overall number of pages that we have scanned from our PDF files.\n\n\nCode\nfrom langchain.document_loaders import DirectoryLoader, CSVLoader\n\nloader = DirectoryLoader(path='C:/Users/Mark/Desktop/csv_files', glob = '**/*.csv', loader_cls = CSVLoader, use_multithreading = True, show_progress = True, silent_errors = True, loader_kwargs = {'encoding': 'utf-8-sig'})\n\ndocs = loader.load()\n\nfor doc in docs:\n  doc.metadata['row'] +=1\nprint(len(docs))\n\n\n  0%|          | 0/3 [00:00&lt;?, ?it/s]100%|██████████| 3/3 [00:00&lt;?, ?it/s]\n\n\n46\n\n\nJust to get an idea of the data format, we can print the 10 item stored in our documents.\n\n\nCode\ndocs[10]\n\n\nDocument(page_content=\": 7\\n0: Chatbot - Wikipedia https://en.wikipedia.org/wiki/Chatbot\\n\\nThe creation and implementation of chatbots is still a developing area, heavily related to artificial intelligence and machine learning,\\nso the provided solutions, while possessing obvious advantages, have some important limitations in terms of functionalities and use\\ncases. However, this is changing over time.\\n\\nThe most common limitations are listed below:!83]\\n\\n= As the input/output database is fixed and limited, chatbots can fail while dealing with an unsaved query.!52]\\n\\n= A chatbot's efficiency highly depends on language processing and is limited because of irregularities, such as accents and\\nmistakes.\\n\\n= Chatbots are unable to deal with multiple questions at the same time and so conversation opportunities are limited.|83I\\n\\n= Chatbots require a large amount of conversational data to train. Generative models, which are based on deep learning\\nalgorithms to generate new responses word by word based on user input, are usually trained on a large dataset of natural-\\nlanguage phrases. |S]\\n\\n« Chatbots have difficulty managing non-linear conversations that must go back and forth on a topic with a user.|&41\\n\\n= As it happens usually with technology-led changes in existing services, some consumers, more often than not from older\\ngenerations, are uncomfortable with chatbots due to their limited understanding, making it obvious that their requests are being\\ndealt with by machines. |83]\\n\\nChatbots and jobs\\n\\nChatbots are increasingly present in businesses and often are used to automate tasks that do not require skill-based talents. With\\ncustomer service taking place via messaging apps as well as phone calls, there are growing numbers of use-cases where chatbot\\ndeployment gives organizations a clear return on investment. Call center workers may be particularly at risk from AI-driven\\nchatbots.!85]\\n\\nChatbot jobs\\n\\nChatbot developers create, debug, and maintain applications that automate customer services or other communication processes.\\nTheir duties include reviewing and simplifying code when needed. They may also help companies implement bots in their operations.\\n\\nA study by Forrester (June 2017) predicted that 25% of all jobs would be impacted by AI technologies by 2019.!86]\\n\\nSee also\\n\\n= Applications of artificial intelligence =» Autonomous agent\\n\\n8 of 18 2023-11-10, 9:32 a.m.\", metadata={'source': 'C:\\\\Users\\\\Mark\\\\Desktop\\\\csv_files\\\\Chatbot - Wikipedia.csv', 'row': 8})\n\n\n\n\nDocument Transformation\nWe could work with the documents we have just as they are now, but it is sometimes difficult to get the proper level of context from a question from an entire page of information. One way to rectify this is by using the ReqcursiveCharacterTextSplitter function which will break down the data into smaller sections. The function has some built-in breakpoints point like section titles and ends of paragraphs. We could then use the smaller split up documents to makes sure our search’s are more accurate. I have found that using both the overall page documents for overall context and the smaller in-depth documents for smaller details together works best. We see that this drastically increases the number of documents that we need to search through.\n\n\nCode\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size = 512, chunk_overlap = 100)\n\ndocuments = text_splitter.split_documents(docs)\ndocuments = documents + docs\nlen(documents)\n\n\n454\n\n\n\n\nText Embeddings\nText embeddings are the real power behind all natural language processes, including chatbots. A text embedding is a vector that represents the meaning behind a text and not the text itself. In this way, we can search through the meaning of our documents for an answer without needing to match the text exactly. In order to use the Open AI API to answer our questions, we need to utilize the Open AI Embeddings model. The use of it is pretty simple, when send them a document and they return the embedding.\n\n\nCode\nimport openai\nfrom langchain.embeddings import OpenAIEmbeddings\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nembeddings_model = OpenAIEmbeddings(request_timeout = 60)\n\n\n\n\nVector Store\nThe next step is not 100% necessary, but it will make our lives much easier. What we need to do is to create a database that connects each of the documents with their embedding. We can do this by creating a vector store. There are a few different vector stores, the one I chose to us is `FAISS’. All we need to do is define the vector store, send it our stored documents and send the connection to our embedding model. The function will then process each document for us.\n\n\nCode\nfrom langchain.vectorstores import FAISS\n\ndb = FAISS.from_documents(documents, embeddings_model)\n\n\n\n\nMultiQuery Retriever\nWhat we have now is a database that we can search for ourselves if we like. You can query the vector store, convert that query into an embedding, and search the vector store for matches. The return from this query will then be a list of documents in the vector store, which maybe useful, but can be difficult to interpret by itself. What a custom chatbot does is use this basic search results and supply it to a Large Language Model (llm) to make sense of it. We would like our Chatbot to have the entire vector store to look through to answer our questions, but there are some limitations to how much context we can provide to Open Ai.\nFor reference, the Chat GPT3 API has a limit of about 4000 tokens. What are tokens? Well, tokens are small sections of text. They are usually longer than 1 character but smaller than most words. One thing to keep in mind is that the 4000 token limitation covers everything, including context provided, the query and the answer returned. Do to this limitation, we need to do a search through our vector store and limit the context that we send to the Open AI API.\nWe could perform a basic query, but I prefer to create a Multi-query. In this query, we are actually asking ChatGPT to create different variations to our query. We can then search our vector store on multiple queries are return the best results over all queries. There is some expense to this as it requires an additional API call to generate the new queries.\nWe can then send our question to our retriever which will automatically perform our search over the vector store, I use the default prompt as it seems to do a good job. The custom Chat bot will then return the answer to our question, based on the context provided by the vector store search, with the source documents it used to generate an answer.\n\n\nCode\nfrom langchain.chains import RetrievalQA\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\nfrom langchain.llms import OpenAI\n\nretriever_from_llm = MultiQueryRetriever.from_llm(\n  retriever = db.as_retriever(search = 'mmr', search_kwags = {'k': 10}),\n  llm = OpenAI())\n  \nquestion = \"What do Chat bots do?\"\nqa = RetrievalQA.from_chain_type(llm = OpenAI(),\nchain_type = \"stuff\",\nretriever = retriever_from_llm,\nreturn_source_documents = True)\n\nans = qa({\"query\":question}, return_only_outputs = True)\nprint(ans)\n\n\n{'result': ' Chatbots are software applications or web interfaces that aim to mimic human conversation through text or voice interactions. Modern chatbots are typically online and use artificial intelligence (AI) systems that are capable of maintaining a conversation with a user in natural language and simulating the way a human would behave as a conversational partner. They are used for B2C customer service, sales and marketing, as well as in toys, devices, and other applications. They can also be used to fill chat rooms with spam and advertisements, by mimicking human behavior and conversations or to entice people into revealing personal information.', 'source_documents': [Document(page_content='A chatbot (originally chatterbot!!) is a software application or web interface that aims to\\nmimic human conversation through text or voice interactions.l2I[3Il41 Modern chatbots are\\ntypically online and use artificial intelligence (AI) systems that are capable of maintaining a\\nconversation with a user in natural language and simulating the way a human would behave as a\\nconversational partner. Such technologies often utilize aspects of deep learning and natural', metadata={'source': 'C:\\\\Users\\\\Mark\\\\Desktop\\\\csv_files\\\\Chatbot - Wikipedia.csv', 'row': 1}), Document(page_content=': 3\\n0: Chatbot - Wikipedia https://en.wikipedia.org/wiki/Chatbot\\n\\nIn 2016, Facebook Messenger allowed developers to place chatbots on their platform. There were 30,000 bots created for Messenger\\nin the first six months, rising to 100,000 by September 2017.!27]\\n\\nSince September 2017, this has also been as part of a pilot program on WhatsApp. Airlines KLM and Aeroméxico both announced\\ntheir participation in the testing;!281[29l[30I[31] oth airlines had previously launched customer services on the Facebook Messenger\\nplatform.\\n\\nThe bots usually appear as one of the user\\'s contacts, but can sometimes act as participants in a group chat.\\n\\nMany banks, insurers, media companies, e-commerce companies, airlines, hotel chains, retailers, health care providers, government\\nentities and restaurant chains have used chatbots to answer simple questions, increase customer engagement,!32! for promotion, and\\nto offer additional ways to order from them.!33]\\n\\nA 2017 study showed 4% of companies used chatbots.!34] According to a 2016 study, 80% of businesses said they intended to have\\none by 2020,[35]\\n\\nAs part of company apps and websites\\n\\nPrevious generations of chatbots were present on company websites, e.g. Ask Jenn from Alaska Airlines which debuted in 200834 or\\nExpedia\\'s virtual customer service agent which launched in 2011.[361[37] The newer generation of chatbots includes IBM Watson-\\npowered \"Rocky\", introduced in February 2017 by the New York City-based e-commerce company Rare Carat to provide information\\nto prospective diamond buyers.!381[39]\\n\\nChatbot sequences\\n\\nUsed by marketers to script sequences of messages, very similar to an autoresponder sequence. Such sequences can be triggered by\\nuser opt-in or the use of keywords within user interactions. After a trigger occurs a sequence of messages is delivered until the next\\nanticipated user response. Each user response is used in the decision tree to help the chatbot navigate the response sequences to\\ndeliver the correct response message.\\n\\nCompany internal platforms\\n\\nOther companies explore ways they can use chatbots internally, for example for Customer Support, Human Resources, or even in\\nInternet-of-Things (IoT) projects. Overstock.com, for one, has reportedly launched a chatbot named Mila to automate certain simple\\nyet time-consuming processes when requesting sick leave.[4°] Other large companies such as Lloyds Banking Group, Royal Bank of\\n\\n4 of 18 2023-11-10, 9:32 a.m.', metadata={'source': 'C:\\\\Users\\\\Mark\\\\Desktop\\\\csv_files\\\\Chatbot - Wikipedia.csv', 'row': 4}), Document(page_content='of chatbots). Chatbots can also be designed or customized to further target even more specific\\nsituations and/or particular subject-matter domains.!Z]', metadata={'source': 'C:\\\\Users\\\\Mark\\\\Desktop\\\\csv_files\\\\Chatbot - Wikipedia.csv', 'row': 1}), Document(page_content=': 2\\n0: Chatbot - Wikipedia https://en.wikipedia.org/wiki/Chatbot\\n\\n(though the program as released would not have been capable of doing so).{15]\\n\\nFrom 197816] to some time after 1983,!7] the CYRUS project led by Janet Kolodner constructed a chatbot simulating Cyrus Vance\\n(57th United States Secretary of State). It used case-based reasoning, and updated its database daily by parsing wire news from\\nUnited Press International. The program was unable to process the news items subsequent to the surprise resignation of Cyrus Vance\\nin April 1980, and the team constructed another chatbot simulating his successor, Edmund Muskie.8]57]\\n\\nOne pertinent field of AI research is natural-language processing. Usually, weak AI fields employ specialized software or\\nprogramming languages created specifically for the narrow function required. For example, A.L.I.C.E. uses a markup language called\\nAIML,3] which is specific to its function as a conversational agent, and has since been adopted by various other developers of, so-\\ncalled, Alicebots. Nevertheless, A.L.I.C.E. is still purely based on pattern matching techniques without any reasoning capabilities, the\\nsame technique ELIZA was using back in 1966. This is not strong AI, which would require sapience and logical reasoning abilities.\\n\\nJabberwacky learns new responses and context based on real-time user interactions, rather than being driven from a static database.\\nSome more recent chatbots also combine real-time learning with evolutionary algorithms that optimize their ability to communicate\\nbased on each conversation held. Still, there is currently no general purpose conversational artificial intelligence, and some software\\ndevelopers focus on the practical aspect, information retrieval.\\n\\nChatbot competitions focus on the Turing test or more specific goals. Two such annual contests are the Loebner Prize and The\\nChatterbox Challenge (the latter has been offline since 2015, however, materials can still be found from web archives)./291\\n\\nChatbots may use neural networks as a language model. For example, generative pre-trained transformers (GPT), which use the\\ntransformer architecture, have become common to build sophisticated chatbots. The \"pre-training\" in its name refers to the initial\\ntraining process on a large text corpus, which provides a solid foundation for the model to perform well on downstream tasks with\\nlimited amounts of task-specific data. An example of a GPT chatbot is ChatGPT.[2°] Despite criticism of its accuracy, ChatGPT has\\ngained attention for its detailed responses and historical knowledge. Another example is BioGPT, developed by Microsoft, which\\nfocuses on answering biomedical questions.!21I[22]\\n\\nDBpedia created a chatbot during the GSoC of 2017./231[241I25] Tt can communicate through Facebook Messenger.\\n\\nApplication\\nMessaging apps\\nMany companies’ chatbots run on messaging apps or simply via SMS. They are used for B2C customer service, sales and\\n\\nmarketing. [26]\\n\\n3 of 18 2023-11-10, 9:32 a.m.', metadata={'source': 'C:\\\\Users\\\\Mark\\\\Desktop\\\\csv_files\\\\Chatbot - Wikipedia.csv', 'row': 3}), Document(page_content=': 2\\n0: Chatbot - Wikipedia https://en.wikipedia.org/wiki/Chatbot\\n\\n(though the program as released would not have been capable of doing so).{15]', metadata={'source': 'C:\\\\Users\\\\Mark\\\\Desktop\\\\csv_files\\\\Chatbot - Wikipedia.csv', 'row': 3}), Document(page_content=\": 6\\n0: Chatbot - Wikipedia https://en.wikipedia.org/wiki/Chatbot\\n\\nIn India, the state government has launched a chatbot for its Aaple Sarkar platform,!Z] which provides conversational access to\\ninformation regarding public services managed.!721I73]\\n\\nToys\\nChatbots have also been incorporated into devices not primarily meant for computing, such as toys.[74]\\n\\nHello Barbie is an Internet-connected version of the doll that uses a chatbot provided by the company ToyTalk,!75] which previously\\nused the chatbot for a range of smartphone-based characters for children.!76] These characters' behaviors are constrained by a set of\\nrules that in effect emulate a particular character and produce a storyline.!77]\\n\\nThe My Friend Cayla doll was marketed as a line of 18-inch (46 cm) dolls which uses speech recognition technology in conjunction\\nwith an Android or iOS mobile app to recognize the child's speech and have a conversation. It, like the Hello Barbie doll, attracted\\ncontroversy due to vulnerabilities with the doll's Bluetooth stack and its use of data collected from the child's speech.\\n\\nIBM's Watson computer has been used as the basis for chatbot-based educational toys for companies such as CogniToys!74] intended\\nto interact with children for educational purposes.!78]\\n\\nMalicious use\\n\\nMalicious chatbots are frequently used to fill chat rooms with spam and advertisements, by mimicking human behavior and\\nconversations or to entice people into revealing personal information, such as bank account numbers. They were commonly found on\\nYahoo! Messenger, Windows Live Messenger, AOL Instant Messenger and other instant messaging protocols. There has also been a\\npublished report of a chatbot used in a fake personal ad on a dating service's website.[79]\\n\\nTay, an AI chatbot that learns from previous interaction, caused major controversy due to it being targeted by internet trolls on\\nTwitter. The bot was exploited, and after 16 hours began to send extremely offensive Tweets to users. This suggests that although the\\nbot learned effectively from experience, adequate protection was not put in place to prevent misuse.[8°]\\n\\nIf a text-sending algorithm can pass itself off as a human instead of a chatbot, its message would be more credible. Therefore,\\nhuman-seeming chatbots with well-crafted online identities could start scattering fake news that seems plausible, for instance\\nmaking false claims during an election. With enough chatbots, it might be even possible to achieve artificial social proof.[81J[82]\\n\\nLimitations of chatbots\\n\\n7 of 18 2023-11-10, 9:32 a.m.\", metadata={'source': 'C:\\\\Users\\\\Mark\\\\Desktop\\\\csv_files\\\\Chatbot - Wikipedia.csv', 'row': 7}), Document(page_content='Chatbots and jobs\\n\\nChatbots are increasingly present in businesses and often are used to automate tasks that do not require skill-based talents. With\\ncustomer service taking place via messaging apps as well as phone calls, there are growing numbers of use-cases where chatbot\\ndeployment gives organizations a clear return on investment. Call center workers may be particularly at risk from AI-driven\\nchatbots.!85]\\n\\nChatbot jobs', metadata={'source': 'C:\\\\Users\\\\Mark\\\\Desktop\\\\csv_files\\\\Chatbot - Wikipedia.csv', 'row': 8}), Document(page_content='Many high-tech banking organizations are looking to integrate automated AI-based solutions such as chatbots into their customer\\nservice in order to provide faster and cheaper assistance to their clients who are becoming increasingly comfortable with technology.\\nIn particular, chatbots can efficiently conduct a dialogue, usually replacing other communication tools such as email, phone, or SMS.', metadata={'source': 'C:\\\\Users\\\\Mark\\\\Desktop\\\\csv_files\\\\Chatbot - Wikipedia.csv', 'row': 5}), Document(page_content='Chatbot jobs\\n\\nChatbot developers create, debug, and maintain applications that automate customer services or other communication processes.\\nTheir duties include reviewing and simplifying code when needed. They may also help companies implement bots in their operations.\\n\\nA study by Forrester (June 2017) predicted that 25% of all jobs would be impacted by AI technologies by 2019.!86]\\n\\nSee also\\n\\n= Applications of artificial intelligence =» Autonomous agent\\n\\n8 of 18 2023-11-10, 9:32 a.m.', metadata={'source': 'C:\\\\Users\\\\Mark\\\\Desktop\\\\csv_files\\\\Chatbot - Wikipedia.csv', 'row': 8})]}\n\n\nIf you are wondering what the default query looks like, you can use the following code. You can create your own prompt if you like, but the default seems to work well for me. With a custom prompt you can customize the output, like make the results return summarized in a Markdown table if you like.\n\n\nCode\nprint(retriever_from_llm.llm_chain.prompt)\n\n\ninput_variables=['question'] template='You are an AI language model assistant. Your task is \\n    to generate 3 different versions of the given user \\n    question to retrieve relevant documents from a vector  database. \\n    By generating multiple perspectives on the user question, \\n    your goal is to help the user overcome some of the limitations \\n    of distance-based similarity search. Provide these alternative \\n    questions separated by newlines. Original question: {question}'\n\n\n\n\nConclusion\nWell, there are all the steps needed to create your very own custom ChatGPT bot with Lang chain. We loaded our CSV files, split the documents into smaller section, created a vector store of our documents, search through our vector store with a multi-query and forward the context and our question to the Open AI API. The result is an answer that is limited to the data stored in our PDF documents.\nPhoto by Google DeepMind"
  },
  {
    "objectID": "posts/2022-10-03-Tree Based Methods/index.html",
    "href": "posts/2022-10-03-Tree Based Methods/index.html",
    "title": "Tree Based Methods: Exploring the Forest",
    "section": "",
    "text": "Forest: Generated by Nightcafe AI\n\nIntroduction\nI was recently reading my copy of “An Introduction to Statistical Learning” (my Amazon affiliate link) and got the chapter about the different tree based methods. I am pretty familiar with Random Forest, but a few of the other methods are new to me. Let’s explore these different techniques.\nFor these examples, I will explore the glass dataset from the openml site. This dataset has 9 different features used to predict the type of glass. The dataset has 214 observations.\nThe dataset is downloaded with the following code. This requires the farff package to open the arff files used on the openml site.\n\nCodelibrary(tidyverse)\ndownload.file(\"https://www.openml.org/data/download/41/dataset_41_glass.arff\", \"data.arff\")\ndf &lt;- farff::readARFF(\"data.arff\")\n\n\nFrom there, we can start to explore the dataset and set up a train and testing split for the data.\n\nCodedf %&gt;% str()\n\n'data.frame':   214 obs. of  10 variables:\n $ RI  : num  1.52 1.52 1.52 1.51 1.53 ...\n $ Na  : num  12.8 12.2 13.2 14.4 12.3 ...\n $ Mg  : num  3.5 3.52 3.48 1.74 0 2.85 3.65 2.84 0 3.9 ...\n $ Al  : num  1.12 1.35 1.41 1.54 1 1.44 0.65 1.28 2.68 1.3 ...\n $ Si  : num  73 72.9 72.6 74.5 70.2 ...\n $ K   : num  0.64 0.57 0.59 0 0.12 0.57 0.06 0.55 0.08 0.55 ...\n $ Ca  : num  8.77 8.53 8.43 7.59 16.19 ...\n $ Ba  : num  0 0 0 0 0 0.11 0 0 0.61 0 ...\n $ Fe  : num  0 0 0 0 0.24 0.22 0 0 0.05 0.28 ...\n $ Type: Factor w/ 7 levels \"build wind float\",..: 1 3 1 6 2 2 3 1 7 2 ...\n\nCodedf &lt;- droplevels(df)\n\nset.seed(1234)\ntest &lt;- sample(1:nrow(df), floor(nrow(df)*0.2))\n\ntestdf &lt;- df[test,]\ntraindf &lt;- df[-test,]\n\n\nDecision Tree\nThe most basic model is the decision tree. In this model, all classes are stored in a container at the root of the tree. The root is split by certain feature values into two smaller containers. These smaller containers are called leafs. After these initial two leafs, we perform an additional split resulting in 4 new leafs. At this point, there should be some better sorting on the response value in the new leafs. Splitting and creating new leafs is continued until the desire results are obtained.\nThe main disadvantage of decision trees is that they are very prone to over fitting. It is pretty easy to imagine a tree that has split so many times that each leaf now represents a single observation. This tree would have 100% accuracy on the training set, but would not perform well on a test set. We prevent over fitting by cutting off or pruning the number of leafs to a certain value.\nFor our example, the decision tree is created with the tree package. It is pretty simple to use, you just supply it with the function and the data.\n\nCodemdltree &lt;- tree::tree(Type ~., data = traindf)\n\n\nA tree visualization is easily created with the print command. With an extra line, we can add the text that shows where each leaf is split. It does get difficult to understand the splits when getting to the lower level leafs.\n\nCodemdltree %&gt;%\nplot(mdltree)\ntext(mdltree, cex =.4)\n\n\n\n\nWe can now test the decision tree’s performance on the test data. Since the response is a factor, the resulting prediction is a matrix with a probability assigned to each response for every observation. We can summarize the mostly likely response with the max.col function.\n\nCodetreevalues &lt;- predict(mdltree, newdata = testdf)\ntreepred &lt;- colnames(treevalues)[max.col(treevalues)]\nAcctree &lt;- mean(treepred == testdf$Type)\nAcctree\n\n[1] 0.5\n\n\nFrom this prediction, we can see that the decision tree didn’t provide a high level of accuracy (50% Accuracy). For a higher level of accuracy, let’s explore some additional methods.\nBagging\nBootstrapping is a useful technique when we have a limited number of observations. Ordinarily, we take each training observation out of a bag and train our model with it. In bootstrapping, after we train our model with an observation, we put the observation back in the bag, allowing repeated selection. The repeated selection creates a smoother distribution of observations.\nBagging takes this process one step further. We take multiple models from bootstrapped training data and average their values. Each bootstrapped model is unique because the training observations are randomly selected.\nBagging can be performed with the randomForest function. By default, the function creates trees from a subset of features, but we can create bagged trees by using all features. The default behaviour for a random forest is to take the majority vote for constructed trees or average their prediction.\n\nCodelibrary(randomForest)\nmdlbag &lt;- randomForest(Type ~., data = droplevels(traindf), mtry = ncol(df)-1, n.trees = 500)\nbagpreds &lt;- predict(mdlbag, newdata = testdf)\nAccbag &lt;- mean(bagpreds == testdf$Type)\nAccbag\n\n[1] 0.6666667\n\n\nRandom Forest\nA random forest is an ensemble model, meaning the prediction is created by combining multiple models together. It is very similar to the previous example with Bagging. The main difference is that the Random Forest selects a random set of features when creating each tree.\nThe rationale for a random feature selection is to create more unique trees. If there is a strong feature in bagging, then most of the trees will use it to create their first split. This creates trees that appear very similar.\n\nCodemdlforest &lt;- randomForest(Type ~., data = droplevels(traindf), n.trees = 500)\nrfpreds &lt;- predict(mdlforest, newdata = testdf)\nAccrf &lt;- mean(rfpreds == testdf$Type)\nAccrf\n\n[1] 0.6666667\n\n\nBoosting\nBoosting is another multiple use technique, much like bagging. Instead of effecting the train data to create different trees, boosting builds trees in sequential order with the default training set. Rather than fitting trees to the response, trees are fit to their residuals. Each tree then learns slowly with the residuals from the previous tree.\nFor this model, we will to use two libraries, caret and gbm. The gbm package is the package that provides the actual model, but a warning is displayed when using the gbm function. To get around the issue, we can use the caret package with the train function. This function can accept many different models and will automatically go through tuning parameters.\n\nCodelibrary(caret)\nlibrary(gbm)\nmdlboost &lt;- caret::train(Type ~., data = traindf, method = 'gbm', verbose = FALSE)\nboostpreds &lt;- predict(mdlboost, newdata = testdf)\nAccboost &lt;- mean(boostpreds == testdf$Type)\nAccboost\n\n[1] 0.6904762\n\n\nBayesian Additive Regression\nBayesian Additive Regression Trees (BART) are similar to the previously mentioned models, creating trees with some random elements that model the signal not captured in previous trees.\nIn the first iteration of the model, all trees are initialized to the same root node. After other iteration updates each of the kth trees one at a time. During the bth iteration, we subtract the predicted values from each response to produce a partial residual. i represents each iteration.\n\\[\nr_i = y_i - \\Sigma_{k'&lt;k}f^b_{k'}(x_i) - \\Sigma_{k'&gt;k}f^{b-1}_{k'}(x_i)\n\\] Rather than fitting a new tree to this residual, BART randomly chooses a tree from the previous iteration (\\(f^{b-1}_k\\)), favouring trees that improve fit.\nUnfortunately, I have not been able to get any BART model to work for this multi-class classification example. The bartMachine package doesn’t have an implementation, and the BART package’s implementation doesn’t seem to work for me. All the values for me were returning NA. There is also very little resources on the BART package to troubleshoot.\nConclusion\nThe decision tree as an easy-to-understand model, but they are prone to over fitting data. Pruning is required to reduce this, but will also reduce accuracy(50% Accuracy for this example).\nEnsemble models are then introduced with Bagging which averages a bunch of trees created by a selection of different observations. This does increase our accuracy(66.67%). This is very similar to the Random Forest model, but instead we increase the random element to include the features selected (66.67%).\nThe final models create trees that build of previously generated trees with the boosting and the Bayesian Additive Regression Tree. Unfortunately, there are no results for the BART model, but boosting does produce good results (69.05%).\nSo there is no clear answer which model is the best, they follow pretty similar methods and produce similar results. I would recommend training them all and finding which performs best on your data."
  },
  {
    "objectID": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html",
    "href": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html",
    "title": "Quarto: The successor to R Markdown",
    "section": "",
    "text": "RMarkdown has been a staple for any Data Scientist that programs in R. Quarto builds on that, with multiple language support and additional features. Because of its language independent design, Quarto requires an independent installation. (“Quarto,” n.d.)\nI have spent the past week moving my blog from blogdown to quarto. There have been some challenges, but I am pretty happy with the new look. Let’s start with the setup, it’s a little more work than a regular package or module."
  },
  {
    "objectID": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#introduction",
    "href": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#introduction",
    "title": "Quarto: The successor to R Markdown",
    "section": "",
    "text": "RMarkdown has been a staple for any Data Scientist that programs in R. Quarto builds on that, with multiple language support and additional features. Because of its language independent design, Quarto requires an independent installation. (“Quarto,” n.d.)\nI have spent the past week moving my blog from blogdown to quarto. There have been some challenges, but I am pretty happy with the new look. Let’s start with the setup, it’s a little more work than a regular package or module."
  },
  {
    "objectID": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#setup",
    "href": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#setup",
    "title": "Quarto: The successor to R Markdown",
    "section": "Setup",
    "text": "Setup\nThe setup for Quarto is pretty simple. You will need to visit the quarto website to download the Quarto Command Line Interface (CLI). There are step-by-step instructions for your selected text editor. I am most familiar with RStudio for R and VSCode for Python.\nFor Rstudio, it’s pretty much just plug and play now. I did install the Quarto package, but all the commands can be done by the command line interface. Switching from RMarkdown is as simple as saving them as qmd file. The process for Quarto for RStudio can be described by the following process flow:\n\n\nRender qmd in RStudio\n\nIt is not much more difficult for VSCode, all you need to do is download the Quarto extension. The process flow is similar to RStudio but uses Jupyter instead of knitr.\n\n\nRender qmd for VSCode\n\nWith the setup complete, there should be no differences between text editors."
  },
  {
    "objectID": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#code-chunk-options",
    "href": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#code-chunk-options",
    "title": "Quarto: The successor to R Markdown",
    "section": "Code Chunk Options",
    "text": "Code Chunk Options\nThe first new feature to explore the support for code chuck options within the code chunks. These options would usually live within the code chunk title line. Any supported option can be added with the #| tag. This feature is useful for situations with many options, as it does increase readability.\n```{r}\n#| label: load\n#| include: true\n#| warning: false\n\nlibrary(tidyverse)\ndata(\"msleep\")\n```"
  },
  {
    "objectID": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#code-folding",
    "href": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#code-folding",
    "title": "Quarto: The successor to R Markdown",
    "section": "Code-folding",
    "text": "Code-folding\nOne of the neat new features is code-folding. When this feature is enabled in the qmd YAML, the person viewing the document can hide/unhide code chunks. This can make it easier for them to read the document. Only the code will be hidden, and not the results.\n\nCodeglimpse(msleep)\n\n\nThis feature is enabled by making the following addition to the YAML. You would change the format from HTML to your required format, such as PDF.\n\nCodeformat: \n  html: \n    code-fold: true\n    code-tools: true\n\n\nWith the addition of the code-tools: true parameter, the reader can decide to hide all code chunks from the top of the document."
  },
  {
    "objectID": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#figures",
    "href": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#figures",
    "title": "Quarto: The successor to R Markdown",
    "section": "Figures",
    "text": "Figures\nQuarto provides a bunch of additional tools for displaying figures. You can assign values for captions, sub-captions, width and height. You can even create a figure with multiple plots with separate sub-captions.\n```{r}\n#| label: fig-sleep\n#| fig-cap: \"Sleeping habits of animals\"\n#| fig-subcap:\n#|   - \"Scatter plot of body weight by total sleep\"\n#|   - \"Violin plot of REM sleep by vore\"\n#| layout-ncol: 2\n\nmsleep %&gt;%\n  drop_na(sleep_total, bodywt) %&gt;%\n  ggplot(aes(y= sleep_total, x = bodywt)) +\n  geom_point(color = \"blue\") +\n  theme_minimal()\n\nmsleep %&gt;%\n  group_by(vore) %&gt;%\n  drop_na(sleep_rem, vore) %&gt;%\n  ggplot(aes(y= sleep_rem, x = vore)) +\n  geom_violin(aes(fill = vore)) +\n  theme_minimal()\n```\n\n\n\n\n\n(a) Scatter plot of body weight by total sleep\n\n\n\n\n\n(b) Violin plot of REM sleep by vore\n\n\n\nFigure 1: Sleeping habits of animals\n\n\nYou can now use cross-referencing for the figure by referencing the figure. This means that in your text, you can refer to the figure number and link to the figure. This will automatically update your figure numbers and is achieved by typing the ‘@’ symbol followed by the figure label. As an example, ‘@fig-sleep’ turns into Figure 1.\nThere is an additional option to let the figures take up the width of the entire page, but I would not recommend using it as it extends beyond the width of the body of your page. It requires the following code:\n```{r}\n#| column: page\n```"
  },
  {
    "objectID": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#code-linking",
    "href": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#code-linking",
    "title": "Quarto: The successor to R Markdown",
    "section": "Code Linking",
    "text": "Code Linking\nA reader may not be familiar with all the functions that you use in your document, so it may be useful to enable code linking. With code linking, a function in a code chunk will have a hyperlink to the documentation for that function. To work in R, this feature requires the xml2 and downlit packages.\n\nCodelm(as.factor(order) ~ sleep_total, data = msleep[complete.cases(msleep),] )"
  },
  {
    "objectID": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#table-of-contents",
    "href": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#table-of-contents",
    "title": "Quarto: The successor to R Markdown",
    "section": "Table of contents",
    "text": "Table of contents\nI think the best feature for Quarto is the floating table of contents. I can’t describe how much time and effort I’ve spent trying to get a floating table of contents in a Blogdown blog. It didn’t work for me, it would require getting deep into the weeds changing the CSS layout for my HUGO theme. It was not worth the effort.\nAdding a floating table of contents in Quarto is simple. Just use the following code in the document YAML:\n\nCodetoc: TRUE\n\n\nOne simple line of code in the YAML and your document has a floating table of contents. There is some additional customization such as the level of headers, location and title.\n\nCodetoc: true\ntoc-depth: 2\ntoc-location: left\ntoc-title: Contents"
  },
  {
    "objectID": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#quarto-vs-blogdown",
    "href": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#quarto-vs-blogdown",
    "title": "Quarto: The successor to R Markdown",
    "section": "Quarto vs Blogdown",
    "text": "Quarto vs Blogdown\nWith my experimentation with Quarto, I decided to move my blogdown blog to Quarto. In theory, this should be a simple switch, with just copying all post from folder to another. Quarto can use rmd files, but they can easily be changed over to qmd files. I decided to switch all my post to the qmd format and include some additional features. The Quarto site has extensive reference information for creating a blog. (“Quarto,” n.d.-)\nI did have an issue with one of my post not rendering correctly. This maybe an issue with compatibility with the stargazer package. In the end, I decided to just remove the post altogether as I could get it to render correctly, and I prefer the gt over the stargazer package for creating good-looking tables."
  },
  {
    "objectID": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#conclusion",
    "href": "posts/2022-08-01-quarto-the-successor-to-r-markdown/index.html#conclusion",
    "title": "Quarto: The successor to R Markdown",
    "section": "Conclusion",
    "text": "Conclusion\nIt is easy to create great looking documents using quarto, whether that be with code in python or R. Quarto supports most of the features in RMarkdown with some fancy new ones. My personal favourite is the floating table of contents. I have also found that rendering a Quarto blog is a much smoother experience than rendering a blogdown blog."
  },
  {
    "objectID": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html",
    "href": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html",
    "title": "Relationship Extraction with Spacyr",
    "section": "",
    "text": "This is the continuation of the previous project, where we scrapped the Cooper Mind website with the rvest package. Please refer to that posting for the necessary steps to obtain the verified character names.\nAs a reminder, this project was inspired by the work of Thu Vu where she created a network mapping of the characters in the Witcher series. I thought it would be interesting to do some recreation of this project, but in R and with the Stormlight Archive book series.\nFor those unfamiliar with the series, it is an epic fantasy story sprawling over four main books at the time of the publishing of this post. Sanderson is a fantastic author and I feel that the Stormlight Archive is his best work."
  },
  {
    "objectID": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#introduction",
    "href": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#introduction",
    "title": "Relationship Extraction with Spacyr",
    "section": "Introduction",
    "text": "Introduction\nSo in a previous post, we created a list of characters which will represent the nodes in our network graph. The next step in the project is to create the edges. The edges represent the relationships between characters. In our graph, we are going to have the edges represent that strength of the relationships between characters. In order to determine these edge values, we will need to perform relationship extraction from the text with the spacyr package.\nThe spacyr package is simply a wrapper for the python spaCy library, with the following functionality:\n\ntokenization\nlemmatizing tokens\nparsing dependencies (to determine grammatical structure)\nextracting form named entities\n\nIt uses the reticulate to create the python environment. I have previously written a post about using the reticulate package for using python code in RMarkdown."
  },
  {
    "objectID": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#initialization",
    "href": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#initialization",
    "title": "Relationship Extraction with Spacyr",
    "section": "Initialization",
    "text": "Initialization\nWe start with the loading of the necessary libraries to complete the project.\n\nlibrary(spacyr)\nlibrary(tidyverse)\nlibrary(data.table)\n#necessary to create a corpus object\nlibrary(readtext)\nlibrary(quanteda)\nlibrary(rainette)\n\nIf you have an environment of python with a version a spaCy, you can pass the destination into the spacy_intialize function. If not, you need to use the spacy_install function to create a Conda environment that will also include the spaCy package. For this project, I let spacyr create the Conda environment for me. This process did take a while for me, so don’t be surprised if it is the same for you.\n\nspacy_install()\n\nI have the name list from the web scraping post saved as a RDS files. RDS files are compressed text files which load quicker and take up much less space than a CSV file.\n\nnames &lt;- read_rds(\"data/names.RDS\")"
  },
  {
    "objectID": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#text-reading",
    "href": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#text-reading",
    "title": "Relationship Extraction with Spacyr",
    "section": "Text Reading",
    "text": "Text Reading\nThe first step is to read all the text files into the system. I found this interesting little snippet of code that allows you to create a list of all the text files in a specific folder. For this project, all the books were stored in a single data folder.\n\nlist_of_files &lt;- list.files(path = \".\", recursive = TRUE,\n                            pattern = \"\\\\.txt$\", \n                            full.names = TRUE)\n\nWith the list of files, we can use the map_df function from the purr package. The purr package is part of the tidyverse package, so we don’t need to load it separately. The map series of functions allows use to pass a vector of values and a function. Each value will then be passed to that function. The _df part of the function is just the requirement that the output be in the format of a dataframe.\nThe same task can be completed with a for loop, but it is much faster in the map function as it utilize vectorization. Vectorization is the strategy of performing multiple operations rather than a single operation at the same time. I am not very familiar with the purr package, so I plan to write a new article on the topic in the near future.\nAfter all the books are read into memory, we need to create a corpus. A corpus is a large body of text, much like a library, for the sorting and organization of books. This is completed with the corpus function from the quanteda package. This corpus structure is necessary to utilize functions from the spacyr package.\nThe organizational structure in the Corpus is why I needed to load the books in with the readtext function from the readtext package. I’ve tried many methods to read the text(readlines, read_Lines, readfile) but none of them performed the proper way for the corpus function. There were plenty of issues, hours of difficulty, which resulted in me referring to the quanteda package website. There I learnt about the readtext function and it worked flawlessly on the first time. Well, I did find an issue with the default encoding not interpreting characters correctly, but this was corrected easily.\nWhen the time came to modelling, issues arose with the size of the Corpus. There is a limitation in spaCy, it will only work with text files less than 100,000,000 characters long. I think that each book was a little over twice that size. So I needed to batch the process by breaking the corpus up into smaller sections. This was done with the split_segments function from the rainette package. The function only accepts a split based on number of sentences, so I arrived at a value of 100,000 sentences per document.\n\ncorpus &lt;- list_of_files %&gt;% \n        map_df(readtext, encoding='utf-8') %&gt;%\n        corpus() %&gt;%\n        rainette::split_segments(segment_size = 100000)\n\n  Splitting...\n\n\n  Done.\n\n\nWith the books read into file, the corpus created and the corpus split into sections, we now have 18 documents. We can proceed to entity modelling with the spaCy functions.\nUnfortunately, we still have size issues, as passing the entire Corpus to be parsed is unaffected by the number of documents. So I needed to create a simple for loop to analyze each document one at a time and bind the results to a data table. Data tables are like data frames, but they have some unique notation and increased performance.\nThe Corpus is parsed with the spacy_parse function. Setting pos and lemma to false should reduce performance time as the function doesn’t need to return dependency POS tag set and lemmatized tokens. The POS tag set refers to the type of word such as Noun, while the lemmatized token is the base of a word, such as for the word “am” would be lemmatized to “be”. The parsing of the corpus takes a very long time.\n\ndf &lt;- corpus[[1]] %&gt;%\n        spacy_parse(pos = FALSE, lemma = FALSE)\n\nFound 'spacy_condaenv'. spacyr will use this environment\n\n\nsuccessfully initialized (spaCy Version: 3.1.3, language model: en_core_web_sm)\n\n\n(python options: type = \"condaenv\", value = \"spacy_condaenv\")\n\nfor (i in 2:length(corpus)){\n        temp &lt;- corpus[[i]] %&gt;%\n        spacy_parse(pos = FALSE, lemma = FALSE) \n        \n        df &lt;- rbind(df,temp)}\nrm(temp)\n\nThe parsing creates an object that acts very similarly as a data table. There is an entry for each word, which is more than what is required for this project. The original data table is preserved, in case we would like to reference a sentence in the corpus, and we create a filtered data table. The data table is filtering the tokens in the names list and by the identified entity, making sure it starts with person.\n\ndfclean &lt;- df %&gt;% \n        filter(token %in% names,\n               str_starts(entity, \"PERSON\"))"
  },
  {
    "objectID": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#relationship-modelling",
    "href": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#relationship-modelling",
    "title": "Relationship Extraction with Spacyr",
    "section": "Relationship modelling",
    "text": "Relationship modelling\nThe final step is to create a model that will connect people in the data table. I have decided to use a sentence windows that creates a connection when two names are mentioned within that window.\nThis is another very time-consuming tasks the requires two for loops. The first loop goes through all 30025 rows and its sentence id. A second for loop that excludes all rows already used in the first loop is used to compare a second sentence id. If the difference between the sentence ids is less than the windows, the tokens for these rows are added to an empty data table. If the difference is greater than the window size, we break the second for loop as all the sentence ids are incremental. It is not a very clean or smooth method, but it works.\n\nwindow_size &lt;- 5\nrelated &lt;- data.table(\"Person1\" = character(), \"Person2\" = character())\n\nfor(i in 1: (nrow(dfclean)-1)){\n        for(j in (i + 1):nrow(dfclean)){\n                if((dfclean$sentence_id[j] - dfclean$sentence_id[i]) &lt; window_size){\n                        \n                        related &lt;- rbindlist(list(related, list(dfclean$token[i], dfclean$token[j])))\n                }\n                \n                else{\n                        break\n                }\n        }\n}\n\nThe following is a sample of the data table we have created to build the relationships.\n\nrelated %&gt;% head()\n\n   Person1 Person2\n1: Shallan  Kabsal\n2: Jezrien Jezrien\n3: Jezrien Jezrien\n4: Jezrien Jezrien\n5: Jezrien Jezrien\n6: Jezrien   Kalak\n\n\nWe can identify two issues with this sample. The first issue is when two of the same names are within the same window. We will have to filter out when ‘Person1’ is equal to ‘Person2’. The second issue is that we would actually like to aggregate the data. We would like a count of when two different names are in the same window. Both of these tasks are easy enough to solve using the built-in data table notation. For more information on data tables, please refer to my previous post on the topic.\n\nrelatedagg &lt;- related[Person1 != Person2,.N,by = c(\"Person1\", \"Person2\")]\n\nrelatedagg %&gt;% head()\n\n   Person1 Person2  N\n1: Shallan  Kabsal  8\n2: Jezrien   Kalak 10\n3:   Kalak Jezrien  9\n4:   Szeth Dalinar 98\n5:   Szeth  Jasnah 11\n6:   Szeth Elhokar  3\n\n\nThe final issue is for the relationships for ‘Person1’ and ‘Person2’ when their places are switched, but that will be dealt with in the next post."
  },
  {
    "objectID": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#conclusion",
    "href": "posts/2022-07-04-relationship-analysis-with-spacyr/index.html#conclusion",
    "title": "Relationship Extraction with Spacyr",
    "section": "Conclusion",
    "text": "Conclusion\nWith some hard-work, we were able to create an organized Corpus of all the current 4 Stormlight Archive books. We were able to split this Corpus into smaller sized documents, making them easier to manage. The spacyr library was then used to model entities within the Corpus, identifying the tokens that represent people. The next step was to clean up the results, keeping only the verified characters names as tokens. We then used a model to developed relationships using a window. A relationship was created whenever two character names were mentioned in the same window. We then filtered out characters relationships to themselves and aggregated the data. The clear next step is to actually build the graphs with the characters as nodes and their relationships as edges. But that is a post for another day.\nPhoto by Alec Favale on Unsplash"
  },
  {
    "objectID": "posts/2022-06-08-text-prediction-shiny-app-pt-2/index.html#description",
    "href": "posts/2022-06-08-text-prediction-shiny-app-pt-2/index.html#description",
    "title": "Text Prediction Shiny App pt 2",
    "section": "Description",
    "text": "Description\nThis is the second part for the creation of a text prediction Shiny Application. From the previous post, we have developed and Corpus of text to start creating text prediction applications.\nWe have also explored the corpus, looking at the frequency of words in the vocabulary. It is now time to start to develop ngram models."
  },
  {
    "objectID": "posts/2022-06-08-text-prediction-shiny-app-pt-2/index.html#n-gram-models",
    "href": "posts/2022-06-08-text-prediction-shiny-app-pt-2/index.html#n-gram-models",
    "title": "Text Prediction Shiny App pt 2",
    "section": "N-gram models",
    "text": "N-gram models\nA ngram is a continuous sequence of tokens, where the order is determined by how many tokens are in the sequence. For our purpose, a token is created for each word in a sentence. Other tokens can be created, such as sentence in a paragraph or letters in a word. It really depends on your application needs.\nA line of text can be broken down into ngrams in many ways. For example, the following text:\n“The quick brown fox”\ncan be broken down to the following unigrams:\n\n(“the”)(“quick”)(“brown”)(“fox”)\n\nor to the following bigrams:\n\n(“the quick”)(“quick brown”)(“brown fox”)\n\nor to the following trigrams:\n\n(“the quick brown”)(“quick brown fox”)\n\nor to the single tetragram:\n\n(“the quick brown fox”)\n\nThe process for creating tokens from text, tokenization, drops the text to lower case and removes all punctuation. For this application, I would recommend the unnest_tokens function from the tidytext package.\nNgrams can be used for predictive text by reserving the last word in the ngram as the predicted word."
  },
  {
    "objectID": "posts/2022-06-08-text-prediction-shiny-app-pt-2/index.html#models",
    "href": "posts/2022-06-08-text-prediction-shiny-app-pt-2/index.html#models",
    "title": "Text Prediction Shiny App pt 2",
    "section": "Models",
    "text": "Models\n\nStupid Back-off\nA higher level of n-gram should provide a better predictive quality for our models. However, these higher n-grams have lower levels of occurrences. Each additional word included in the created n-grams, reduce the different possible solutions but should have a higher level of accuracy as there is more context provided to the model.\nWe need to create some shiny functions to help use determine the highest possible ngram model that we can use. The first function, turns the user input in unigram tokens, which does a lot of pre-processing for us. For words not in the vocabulary, we change the values to the ‘’ token, which the models already have included in the ngrams.\nThe final function simply finds the minimum between the length of the user input and the highest level of ngram models. The result will be the highest degree of ngram that we can use. This is often refereed to as the “Stupid Back-off” method, as a higher order ngram is “backed-offed” to a lower level ngram.\n\ntruetext &lt;- reactive({\n        truetext &lt;- input$text %&gt;%\n                tibble(text=.) %&gt;%\n                unnest_tokens(word, text, token=\"ngrams\", n=1)\n        \n        truetext[!truetext$word %in% voc$word,] &lt;- \"unk\"\n        truetext})\n        \n        maxuse &lt;- reactive({\n                min(nrow(truetext()) + 1,maxn)\n                })\n\n\n\nMaximum Likelihood Estimation\nThe maximum likelihood estimation (MLE) is the simplest model to examine. We simply count all the occurrence where the all values from the user input match with the ngrams to the final word in the n-gram. The final for in the ngram is reserved for the predicted estimation.\n\\[p_x = \\frac{C_x}{C}\n\\]\nWhere \\(p_x\\) is the probability that the word x will be predicted, \\(C_x\\) is the count of the word x occurring, and \\(C\\) is the count of all words.\nThe MLE model produces an unbalanced model, where there a many values from the vocabulary that have zero probability of being predicted. We would like to address this issue by developing more complicated models.\nThe following plot is a sample distribution created with the MLE model. The predicted values are sorted into bins based on the first letter of the predicted value. Some bins/letters have no value and therefore will have no probability assigned to them.\n\ndf &lt;- ngrams$three %&gt;%\n        filter(word1 ==\"what\", word2 == \"is\") %&gt;%\n        select(word3, n) %&gt;% \n        right_join(voc, by = c(\"word3\" = \"word\")) %&gt;%\n        mutate(bin = substr(word3,1,1)) %&gt;% \n        group_by(bin)\n\ndf$n[is.na(df$n)] &lt;- 0\n\ndf %&gt;%\n        ggplot(aes(x = bin, y = n)) +\n        geom_bar(stat = \"identity\")\n\n\n\n\n\n\nAdd One Smoothing\nThe simplest way to deal with the issue of zero probability values is to add one to all unseen counts. This is also referred to as Laplace Smoothing.\n\\[p_x = \\begin{cases}\n\\frac{C_x}{C} & C_x &gt; 0 \\\\\n\\frac{1}{C} & C_x = 0\n\\end{cases}\n\\]\nThe plot for the add one model is pretty easy to create from the previous sample. It is clear that there are some values now in each bin, so there is some probability to every word in the vocabulary. The heights of the bins are also increased, as there previously were words in each bin that had 0 occurrences now occurring once.\n\ndf &lt;- ngrams$three %&gt;%\n        filter(word1 ==\"what\", word2 == \"is\") %&gt;%\n        select(word3, n) %&gt;% \n        right_join(voc, by = c(\"word3\" = \"word\")) %&gt;%\n        mutate(bin = substr(word3,1,1)) %&gt;% \n        group_by(bin)\n\ndf$n[is.na(df$n)] &lt;- 1\n\ndf %&gt;%\n        ggplot(aes(x = bin, y = n)) +\n        geom_bar(stat = \"identity\")\n\n\n\n\n\n\nGood Turing\nIn order to understand the Good Turing Smoothing, we need to introduce some new notation, \\(N_C\\), to represent the frequency of frequencies. The frequency of frequencies represents how often a number of occurrences will happen in or distribution. For example, \\(N_0\\) represents the word count in our vocabulary where there are no occurrences of that word in the distribution. \\(N_1\\) then represents the count of the words that have one occurrence. The frequency of frequencies is a one layer of abstraction from our counts. It is helpful to consider our previous plots where we created bins based on the first letter of the predicted word, but instead we are creating bins one how often our predicted words occur.\nTo create these \\(N_C\\) values, we can use the count function. The original values for ‘n’ were created with the count function, we can repeat it over the values of ‘n’ to create a count of counts which I have called ‘nn’. The plot is as expected, there are many words with a low number of counts and a few high count values.\n\ndf &lt;- ngrams$three %&gt;%\n        filter(word1 ==\"what\", word2 == \"is\") %&gt;%\n        select(word3, n) %&gt;% \n        right_join(voc, by = c(\"word3\" = \"word\"))\n\ndf$n[is.na(df$n)] &lt;- 0\n\nNr &lt;- count(df, n, name = \"nn\")\n\n\nNr %&gt;%\n        head()\n\n# A tibble: 6 × 2\n      n    nn\n  &lt;dbl&gt; &lt;int&gt;\n1     0 64330\n2     2    51\n3     3    28\n4     4    13\n5     5     2\n6     6     7\n\n\nThe first intuition of the Good Turing is that the probability of something new, a word with a count of zero, should be assigned the probability for an event that occurred once. For this example, we have the very unlikely event that there are no counts of words that appear once, so we use the next available count(X). This will give the probability of all words with zero count, we will later divide it by the number of words with the count 0.\n\\[P_0 = \\frac{C_1}{C} = \\frac{C_x\\cdot N_x}{\\Sigma C_N\\cdot N_N}\n\\]\nSince we have grouped the words by frequencies, we can use the product of all frequency of the frequencies by their count.\n\ntotal &lt;- sum(Nr$nn*Nr$n)\ntotal\n\n[1] 1449\n\n\nGood Turing requires some additional calculations, so it is beneficial to add some columns to the dataframe at this point.\n\nNr &lt;- Nr %&gt;%\n        arrange(n) %&gt;% \n        mutate(c= 0) %&gt;%\n        mutate(sc = 0) %&gt;%\n        mutate(GT = 0)\n\nThis snippet of code is used to determine the probability for a word with zero count.\n\n#the probability for unseen matches is set to the next value probability\nNr$GT[Nr$n==0] &lt;- Nr$nn[2]*Nr$n[2]/total\n\nAll other counts are to be adjusted. The Good Turing Smoothing is defined by the following equation:\n\\[C^*=\\frac{(C+1)N_{C+1}}{N_C}\\]\nWhere \\(C^*\\) is the adjusted count number. Since the general trend is that the frequencies decrease as the count increases, the term \\(\\frac{N_{C+1}}{N_C}\\) will decrease the value for the count. This is the desired behaviour, as we want that probability to be distributed to zero counts.\nOne major issue that need to be addressed is that the frequency table is not continuous. There are holes as not all counts exist. To overcome this obstacle, we can create a regression model to fill in the missing values. A logistics regression model fits the values much better than a linear model.\n\nZn &lt;- Nr[-1,] %&gt;% add_row(n=Nr$n[nrow(Nr)]+1)\nZr &lt;- Nr[-1,] %&gt;% lm(log(nn)~log(n), data=.) %&gt;% predict(newdata=Zn)\nZr &lt;- exp(Zr)\n\nThe next code chunk can look quite complicated. In this chunk, the corrected count, \\(C^*\\), are calculated. The variable j is used to control whether the regression model is used to substitute the value for \\(N_{C+1}\\).\n\n#creates the new adjusted counts\nj &lt;- 0 \nfor (i in 2:nrow(Nr)) {\n        Nr$c[i] &lt;-  (Nr$n[i]+1)*Nr$nn[i+1]/Nr$nn[i]\n        Nr$c[i][is.na(Nr$c[i])] &lt;- 0\n        Nr$sc[i] &lt;-  (Nr$n[i]+1)*Zr[i]/Zr[i-1]\n        if(Nr$n[i+1]-Nr$n[i] &gt; 1 | i == nrow(Nr)){\n                j &lt;- 1}\n        Nr$GT[i] &lt;-  Nr$c[i]*(1-j) + Nr$sc[i]*j\n        }\n\nThe probabilities at this time need two additional modifications, they need to be normalized as the regression model skews the overall probability and the probabilities need to be divided by the frequency counts to get a word specific probability.\n\n#the specific prop from words with the same count\nNr$GT[Nr$GT &lt; 0] &lt;- Nr$nn[2]/total\nNr$GT &lt;- Nr$GT/sum(Nr$GT)\nNr$GT2 &lt;- Nr$GT/Nr$nn\n\nWe can now plot the completed ngram prediction for the Good Turing Smoothing. The plot looks similar to previous plots, but we plot the probabilities rather than the count values ‘n’.\n\ndf &lt;- ngrams$three %&gt;%\n        filter(word1 ==\"what\", word2 == \"is\") %&gt;%\n        select(word3, n) %&gt;% \n        right_join(voc, by = c(\"word3\" = \"word\")) %&gt;%\n        mutate(bin = substr(word3,1,1)) %&gt;% \n        group_by(bin)\n\ndf$n[is.na(df$n)] &lt;- 0\n\ndf %&gt;%\n        left_join(select(Nr,n,GT2), by = \"n\") %&gt;%\n        ggplot(aes(x = bin, y = GT2)) +\n        geom_bar(stat = \"identity\")\n\n\n\n\n\n\nAbsolute Discounting\nGood Turing Smoothing is an effective model, but man can it be complicated. One observation that you can make when looking at the values for \\(C\\) and \\(C^*\\) is that there is nearly constant discounting. The distribution in our example is skewed, but we can see that the most common value is between 0 and 1.\n\nNr %&gt;%\n        select(c,sc) %&gt;%\n        mutate(diff = c-sc) %&gt;%\n        ggplot(aes(x=diff)) +\n        geom_histogram()\n\n\n\n\nThis would suggest that we could significantly simplify the adjusted counts calculations by subtracting a constant value. The algorithm is described by the following equation:\n\\[p_x = \\frac{C_x - d}{C} + \\lambda \\cdot p_{unigram}\n\\]\nwhere ‘d’ is the discounting amount, \\(\\lambda\\) is the Interpolation rate and \\(p_{unigram}\\) is the unigram probability based on the MLE.\n\ndiscount &lt;- 0.75\nADI &lt;- df %&gt;%\n        ungroup() %&gt;%\n        select(word3, n) %&gt;%\n        mutate(ADI = (n - discount)/sum(n))\n                \nADI$ADI[ADI$ADI &lt; 0 ] &lt;- 0\n\nAs previously mentioned, the unigram probability is calculated by applying the MLE to the unigram counts.\n\nunigram.prop &lt;- ngrams$one %&gt;%\n        mutate(prop = n / sum(n))\n\nThe interpolated weight (\\(\\lambda\\)) can be found by finding the probability that was discounted.\n\nuni.wt &lt;- 1 - sum(ADI$ADI)\n\nADI &lt;- ADI %&gt;% \n        add_column(uni = unigram.prop$prop*uni.wt) %&gt;% \n        mutate(ADI = ADI + uni, .keep = \"unused\")\n\nWe can see that the plot of the probabilities for the absolute discounting is very similar to the Good Turing plot, but it was much easier to understand and calculate.\n\nADI %&gt;%\n        mutate(bin = substr(word3,1,1)) %&gt;% \n        group_by(bin) %&gt;%\n        ggplot(aes(x = bin, y = ADI)) +\n        geom_bar(stat = \"identity\")\n\n\n\n\n\n\nKneser-Ney\nThe issue with Absolute Discounting is the reliance on the unigram probabilities. The unigram probability doesn’t provide any contextual information. We would rather rely on the continuation probability. Rather than looking at how often the word occurs, the continuation probability looks at how many bigrams the word completes. The Kneser-Ney model follows this equation:\n\\[p_x = \\frac{max(C_x - d, 0)}{C} + \\lambda \\cdot p_{continuation}\n\\]\nThe next chunk of code is very similar to the code used in the absolute discounting model.\n\nKNS &lt;- df %&gt;%\n        ungroup %&gt;%\n        select(word3, n) %&gt;%\n        mutate(KNS = (n - discount)/sum(n))\n\nKNS$KNS[KNS$KNS &lt; 0 ] &lt;- 0\ncont.wt &lt;- 1 - sum(KNS$KNS)\n\n\nContinuation Probabilities\nThe following code is used to determine the continuation probabilities. Since the highest order ngram is six, the continuation probability needs to be calculated for six different ngram series.\n\ncont.prop.func &lt;- function(word, ngrams){\n        out &lt;- ngrams %&gt;% \n                filter(.[,ncol(ngrams)-1] == word) %&gt;%\n                nrow() \n        out / nrow(ngrams)\n}\ncont.prop &lt;- list()\ncont.prop$one &lt;- tibble(word=voc$word, prop = ngrams$one$n/sum(ngrams$one$n))\ncont.prop$two &lt;- tibble(word=voc$word, prop = map_dbl(word, cont.prop.func, ngrams=ngrams$two))\ncont.prop$three &lt;- tibble(word=voc$word, prop = map_dbl(word, cont.prop.func, ngrams=ngrams$three))\ncont.prop$four &lt;- tibble(word=voc$word, prop = map_dbl(word, cont.prop.func, ngrams=ngrams$four))\ncont.prop$five &lt;- tibble(word=voc$word, prop = map_dbl(word, cont.prop.func, ngrams=ngrams$five))\ncont.prop$six &lt;- tibble(word=voc$word, prop = map_dbl(word, cont.prop.func, ngrams=ngrams$six))\nsaveRDS(cont.prop, \"cont.prop.rds\")\n\nThe difficulty is with finding the continuation probability. After they are found, it is pretty easy to add them to the model.\n\nKNS$KNS &lt;- KNS$KNS + cont.prop$three$prop*cont.wt\n\nKNS %&gt;%\n        mutate(bin = substr(word3,1,1)) %&gt;% \n        group_by(bin) %&gt;%\n        ggplot(aes(x = bin, y = KNS)) +\n        geom_bar(stat = \"identity\")"
  },
  {
    "objectID": "posts/2022-06-08-text-prediction-shiny-app-pt-2/index.html#shiny-app",
    "href": "posts/2022-06-08-text-prediction-shiny-app-pt-2/index.html#shiny-app",
    "title": "Text Prediction Shiny App pt 2",
    "section": "Shiny App",
    "text": "Shiny App\nWith all the models created, we can bundle it together in a single Shiny Application. This Shiny Application retrieves the user’s input and attempts to predict the next word. A table is generated to summarize the most highly predicted word. Since there are five different models, there are five different rows. A plot is generated for each model where the predicted words are in bins with other words with the same first letter.\n\n\n\n\n\n\nPhoto by Jaredd Craig on Unsplash"
  },
  {
    "objectID": "posts/2022-05-09-formatting-our-outout-with-python-s-f-strings/index.html",
    "href": "posts/2022-05-09-formatting-our-outout-with-python-s-f-strings/index.html",
    "title": "Formatting our output with Python’s F strings",
    "section": "",
    "text": "I have recently been on a tear of different challenges on the site HackerRank. I am about halfway through their 30 days of code and 10 days of statistics. These challenges often require to output number to a certain a number of significant digits. I’ve always thought that the round function can be used for this, but I am wrong. The F string seems to be a powerful tool to accomplish this, and worth your time learning if you are unfamiliar."
  },
  {
    "objectID": "posts/2022-05-09-formatting-our-outout-with-python-s-f-strings/index.html#structure-of-an-f-string",
    "href": "posts/2022-05-09-formatting-our-outout-with-python-s-f-strings/index.html#structure-of-an-f-string",
    "title": "Formatting our output with Python’s F strings",
    "section": "Structure of an F string",
    "text": "Structure of an F string\nThe formatting of an F string starts with a f prior to quotations, whether they be single or double quotes. Any variable can then be included within a series of {}. This formatting can make it easier than turning values into strings and concatenating all strings into a single line of text. This is easily demonstrated with a large mix of values and strings.\n\nx = 1/3\ny = 1/6\n\nprint(\"The value is \" + str(x) + \" is greater than \" + str(y))\nprint(f\"The value is {x} is greater than {y}\")\n\nThe value is 0.3333333333333333 is greater than 0.16666666666666666\nThe value is 0.3333333333333333 is greater than 0.16666666666666666\n\n\nThe values can then be formatted with : after the variable name. The number of digits prior and post the decimal can then be specified. The f is added after the decimal formatting to ensure the value is treated as a float.\n\nprint(f\"The value is {x:.3f} is greater than {y:.2f}\")\n\nThe value is 0.333 is greater than 0.17\n\n\nThe values passed are not specific to the number of digits, but the minimum number of spaces. This means you can ensure specific space aligned, such as for a table, by including these values.\n\nz = [10000, 500, 10, 0.001, .1]\nfor i in z:\n        print(f\"the value is: {i:5}\")\n\nthe value is: 10000\nthe value is:   500\nthe value is:    10\nthe value is: 0.001\nthe value is:   0.1\n\n\nAdditionally, we can add leading zeros by adding zero prior to the number of digits.\n\nfor i in z:\n        print(f\"the value is: {i:05}\")\n\nthe value is: 10000\nthe value is: 00500\nthe value is: 00010\nthe value is: 0.001\nthe value is: 000.1"
  },
  {
    "objectID": "posts/2022-05-09-formatting-our-outout-with-python-s-f-strings/index.html#alternative-formatting",
    "href": "posts/2022-05-09-formatting-our-outout-with-python-s-f-strings/index.html#alternative-formatting",
    "title": "Formatting our output with Python’s F strings",
    "section": "Alternative formatting",
    "text": "Alternative formatting\nThere are a few alternative methods for f strings. From my understanding, they are not as fast when it comes to performance. I don’t think that is of particular importance. If your script needs a high level of performance, than you probably don’t want many print statements.\n\nFormat Method()\nThe format method is very similar to f strings with the use of the {}. The string is not preceded by f and the {} can remain empty or contain position indexing. The values are then added in the .format function after the string. The order of the variable in the string will correspond with the number used in the {}, if used at all.\n\nprint(\"The value is {} is greater than {}\".format(1/3, 1/6))\n\nThe value is 0.3333333333333333 is greater than 0.16666666666666666\n\n\n\n\nOld % Method\nThe Old % operator (modulo) replaces the value in the string. Formatting details, such as those previously discussed, are entered after the %. The variables or values are then entered after the string when preceded by another %. Multiple values can be passed.\n\nprint(\"The value is %5.3f is greater than %5.3f\" %(x,y))\n\nThe value is 0.333 is greater than 0.167"
  },
  {
    "objectID": "posts/2022-05-09-formatting-our-outout-with-python-s-f-strings/index.html#conclusions",
    "href": "posts/2022-05-09-formatting-our-outout-with-python-s-f-strings/index.html#conclusions",
    "title": "Formatting our output with Python’s F strings",
    "section": "Conclusions",
    "text": "Conclusions\nWhichever method you decide, it probably won’t make a huge difference. The important part is to understand is the actual formatting. F strings also seem to make it easier to understand the code, as the actual values are inline with the string and the formatting.\n\nPhoto by Sigmund on Unsplash"
  },
  {
    "objectID": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html",
    "href": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html",
    "title": "Dashboards in R with Shiny Dashboard",
    "section": "",
    "text": "In a previous post, I explore the Flex dashboard library for the creation of a clean and interactive dashboard. That post can be found here. Unknown to me at the time, but I sort of skipped over the more natural progression of creating a dashboard with R Shiny. This is my attempt to recreate that dashboard and compare the ease of creation and functionality of the Shinydashboard library."
  },
  {
    "objectID": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#application-structure",
    "href": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#application-structure",
    "title": "Dashboards in R with Shiny Dashboard",
    "section": "Application Structure",
    "text": "Application Structure\nMy shiny dashboard will at heart be a shiny application. I will use the single App file structure rather than the separate UI/Server file structure. The R Studio site has a very good outline on the basic structure for a Shiny Dashboard.\nMY application will then be broken down into the following structure:\n\nData Collection\nPlot Creation\nUI\nServer\n\nBefore starting to write the Shiny App, the raw data needs to be cleaned and setup to increase application performance."
  },
  {
    "objectID": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#data-preparation",
    "href": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#data-preparation",
    "title": "Dashboards in R with Shiny Dashboard",
    "section": "Data Preparation",
    "text": "Data Preparation\nThe data is the same from the previous dashboard, which was the level of donations for Canadians to charities. The raw data can be found here.\nThe following code is used to clean up the raw data and is not included in the Shiny App. This code is just used to create an RDS file (compressed data file) that the Shiny App will more easily load.\n\nlibrary(tidyverse)\n# Download the data and unzip\ndownload.file(\"https://www150.statcan.gc.ca/n1/tbl/csv/45100007-eng.zip\", \"donordata.zip\")\nunzip(\"donordata.zip\")\n\n# Read the data into R\ndata = read_csv(\"45100007.csv\")\n\n# Clean up the data\ndata = data %&gt;%\n        filter(`Donation statistics (UOM)`=='Average annual donations') %&gt;%\n        filter(!GEO==\"Canada\") %&gt;%\n        filter(!Education == \"All education levels\") %&gt;% rename(Province = GEO)\ndata$Province[data$Province == \"Newfoundland and Labrador\"] &lt;- \"Newfoundland\"\ndata$Province[data$Province == \"Prince Edward Island\"] &lt;- \"P.E.I.\"\n\n# Saved the clean data as an RDS file\nsaveRDS(data, 'data.rds')"
  },
  {
    "objectID": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#data-collection",
    "href": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#data-collection",
    "title": "Dashboards in R with Shiny Dashboard",
    "section": "Data Collection",
    "text": "Data Collection\nWith the data preparation completed, we can now start with writing the application. Shiny Application can be broken down to two parts, reactive and nonreactive. It is important to keep our calculations in the nonreactive part if their values do not change because it will be very demanding on system resources otherwise.\n\n# Loading the libraries\nlibrary(shiny)\nlibrary(shinydashboard)\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(rgdal)\nlibrary(gt)\n\n# Data is loaded into the shiny app from the previously generated RDS file\ndata = readRDS('data.rds')\n\n# Summary data is created from the loaded data and saved as data2\ndata2 &lt;- data %&gt;%\n        group_by(Province) %&gt;%\n        summarise(Donations = sum(VALUE))\n\n# For regional information for mapping, the rgdal library is used.\nlibrary(rgdal)\n\n# The following code will download a regional outlines for maps if the file doesn't exist on the system \nif (!file.exists(\"./src/ref/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.dbf\")){\n        download.file(file.path('https://www.naturalearthdata.com/http/',\n                                'www.naturalearthdata.com/download/50m/cultural',\n                                'ne_50m_admin_1_states_provinces_lakes.zip'), \n                      f &lt;- tempfile())\n        unzip(f, exdir = \"./src/ref/ne_50m_admin_1_states_provinces_lakes\")\n        rm(f)\n}\n\n# The regional data is then loaded into R and some data is edited to make it more inline with the regional data\nregion &lt;- readOGR(\"./src/ref/ne_50m_admin_1_states_provinces_lakes\", 'ne_50m_admin_1_states_provinces_lakes', encoding='UTF-8', verbose = FALSE\n                  )\ndata2$Province &lt;- c(\"Alberta\", \"British Columbia\", \"Manitoba\", \"New Brunswick\", \"Newfoundland and Labrador\", \"Nova Scotia\", \"Ontario\", \"Prince Edward Island\", \"QuÃ©bec\", \"Saskatchewan\")"
  },
  {
    "objectID": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#plot-creation",
    "href": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#plot-creation",
    "title": "Dashboards in R with Shiny Dashboard",
    "section": "Plot Creation",
    "text": "Plot Creation\nJust as the data was collected in the nonreactive section, so should the plot creations. This doesn’t mean that the plots won’t be interactive, just that their designs will remain static.\n\nbar_plot &lt;- data %&gt;%\n        group_by(Province) %&gt;%\n        summarise(Donations = sum(VALUE)) %&gt;%\n        ggplot(aes(x = Province, y = Donations, fill = Province)) +\n        geom_bar(stat = \"identity\", show.legend = FALSE) +\n        theme(axis.text.x = element_text(angle = 90), legend.position='none')\n\n# This call was added for illustration\nbar_plot\n\n\n\nedu_plot &lt;- data %&gt;%\n        group_by(Education) %&gt;%\n        rename(Donations = VALUE) %&gt;%\n        ggplot(aes(y= Donations, x = Education, fill = Education)) +\n        geom_boxplot() +\n        theme(axis.title.x=element_blank(),\n              axis.text.x=element_blank(),\n              axis.ticks.x=element_blank())\n\n# This call was added for illustration\nedu_plot\n\n\n\npie_plot &lt;- data %&gt;%\n        group_by(Province) %&gt;%\n        summarise(Donations = sum(VALUE)) %&gt;%\n        ggplot(aes(x = '', y = Donations, fill = Province)) +\n        geom_bar(stat = \"identity\", width = 1) +\n        coord_polar(\"y\", start = 0) +\n        theme_void()\n\n# This call was added for illustration\npie_plot\n\n\n\nmap_leaf &lt;- leaflet() %&gt;% \n        addTiles() %&gt;% \n        setView(-74.09, 45.7,  zoom = 2) %&gt;% \n        addPolygons(data = subset(region, name %in% data2$Province), color = \"#444444\", opacity = 1.0, fillOpacity = 0.75,\n                    fillColor = ~colorQuantile(\"Greens\", data2$Donations)(data2$Donations),\n                    weight = 1)\n\n# This call was added for illustration\nmap_leaf\n\n\n\n\ngt_table &lt;- data2 %&gt;% \n        pivot_wider(names_from = Province, values_from = Donations) %&gt;%\n        gt()\n\n# This call was added for illustration\ngt_table\n\n\n\n\n\n\n\n\nAlberta\nBritish Columbia\nManitoba\nNew Brunswick\nNewfoundland and Labrador\nNova Scotia\nOntario\nPrince Edward Island\nQuÃ©bec\nSaskatchewan\n\n\n\n\n3350\n2411\n2928\n1359\n1486\n1580\n2084\n1982\n1039\n2637"
  },
  {
    "objectID": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#ui",
    "href": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#ui",
    "title": "Dashboards in R with Shiny Dashboard",
    "section": "UI",
    "text": "UI\nThe following code creates the UI interface that the user interacts with. For those familiar to Shiny Apps, the structure will be similar. The actual functions used will be different, but usually their names will just have the ‘dashboard’ prefix. For those not familiar with Shiny Apps, the UI is created by a nesting of a series of functions. The notation can get a little difficult as you need to remember to use ‘,’ when passing multiple functions. Though not used in this dashboard, UI interfaces can be used to control elements of the display. These controls create variables that need to be saved as input$\"variable_name\". This allows the transfer of information from the UI to the server side. Likewise, plots and figures from the server side need to be saved as output$\"variable_name\". When an output variable is referenced in the UI, such as the plotOutput function for displaying plots, they are referenced as “variable_name” in quotes.\nAgain, the syntax can be difficult to learn and use, but the R Studio post on Shiny Dashboards provide the basic skeleton to learn from. My biggest issue is keeping track of all the ‘)’ and ‘,’ used.\n\n# The main page structure is the dashboardPage\nui &lt;- dashboardPage(\n        \n# The header is simple the header for the dashboard\n        dashboardHeader(title = \"Donations in Canada Dashboard\"),\n\n# The sidebar functions adds the controls for flipping through dashboard pages        \n        dashboardSidebar(\n                \n# Each menu item is a link that will apear on the sidebar\n# There are a bunch of quality icons that the icon function can return \n                menuItem(\"Dashboard\", tabName = \"gauge\", icon = icon(\"dashboard\")),\n                menuItem(\"Education\", tabName = \"education\", icon = icon(\"school\")),\n                menuItem(\"Map\", tabName = \"map\", icon = icon(\"map\"))),\n        \n# The next section includes the body or the main section of the dashboard\n        dashboardBody(\n# In the main body, each of the tabs in the side bar need to be referenced. The tabItems function is the container for all the tabs\n                tabItems(\n                        \n# Each previous mentioned tab, will have a separate tabItem.\n                        tabItem(tabName = \"dashboard\",\n\n# The fluidRow function is used to align the elements on the page. Through trial and error, I have found this to be the element for changing the size of plots.\n                                fluidRow(\n                                        \n# A box is drawn around each plot. Each plot/figure/element is referenced here with the plotOutput function or a likewise function. In the server side, I will transform the ggplots into plotly plots to create interactive plots so the plotlyOutput function is used. \n                box(plotlyOutput(\"plot1\"), width = 6),\n                \n# The pie plot is a regular ggplot object, so it can be passed to the plotOutput function\n                box(plotOutput(\"pie\"), width = 6))),\n                \n                tabItem(tabName = \"education\",\n                        fluidRow(\n                                box(plotlyOutput(\"plot3\"), width = 12))),\n                \n# For the map tab, the leafletOutput function is used to display the leaflet map created in the server side. \n                tabItem(tabName = \"map\", \n                        fluidRow(\n                                box(leafletOutput(\"plot2\"), width = 8)\n                                ),\n                        \n# On a second row, under the map, a GT table requires the gt_output function\n                        fluidRow(\n                                box(gt_output(\"table\"), width = 12)\n                        ),\n                        )\n                )\n        )\n)"
  },
  {
    "objectID": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#server",
    "href": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#server",
    "title": "Dashboards in R with Shiny Dashboard",
    "section": "Server",
    "text": "Server\nThe server is the second major function in the Shiny App. Rather than using the output series of functions, it requires the paired version of the render function. Any controls from the UI would need to be passed as part of the input variable.\n\n# The same structure for any other Shiny App\nserver &lt;- function(input, output) {\n\n# Any data to pass back to the UI, such as figures or plots, will need to be saved as part of the output. Anything within the {} brackets becomes an interactive element. If they are not present, the plot will not change.\n        output$plot1 &lt;- renderPlotly({\n                \n# The ggplot for the barplot is converted to an interactive plotly plot.\n                        ggplotly(bar_plot)\n                })\n\n        output$plot2 &lt;- renderLeaflet({map_leaf})\n        \n        output$plot3 &lt;- renderPlotly({ggplotly(edu_plot)})\n        \n# The GT table requires the render_gt function which pairs with the gt_output function in the UI.\n        output$table &lt;- render_gt({gt_table})\n        \n        output$pie &lt;- renderPlot({pie_plot})\n}"
  },
  {
    "objectID": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#finalized-dashboard",
    "href": "posts/2022-04-20-dashboards-in-r-with-shiny-dashboard/index.en.html#finalized-dashboard",
    "title": "Dashboards in R with Shiny Dashboard",
    "section": "Finalized Dashboard",
    "text": "Finalized Dashboard\nThe finalized dashboard looks pretty good. The sidebar adds a good level of control to the user and makes the data better organized. I also enjoy the default styling, but I’m sure it can be changed.\nWhen compared to the dashboard created with flexdashboard, it is much nicer looking. The flex dashboard, however, has the advantage of being much easier to create with r markdown. It is also much more flexible as it produces an HTML file which is easy to manage. The shiny dashboard needs to be hosted, which makes it slightly clunky.\nSpeaking of the clunky nature of Shiny Apps, you also need to be conscious of what needs to be reactive and what doesn’t. It is very easy to slow down a Shiny Dashboard with a bunch of unnecessary calculations. The writing of the application itself is also quite clunky. It doesn’t compare to the ease of writing in R markdown.\nSo which dashboard do I prefer? Well I think for a quick solution, flex dashboard is great but the amount of control and styling of a shiny dashboard is just better. I do think that there is a place for both, but if I had to choose, I would go for a Shiny Dashboard."
  },
  {
    "objectID": "posts/2022-04-11-r-bloggers-site/index.html",
    "href": "posts/2022-04-11-r-bloggers-site/index.html",
    "title": "R-Bloggers site",
    "section": "",
    "text": "I would like to take the time to mention the r-bloggers site. It is a vast collection of Blogs on everything that has to do will the R language. I would very much like to contribute to their work with this blog.\nJust to keep it interesting, I would like to recommend this specific post by The Jumping Rivers Blog about upcoming R conferences. I’ve attended a few digital R Conferences, hosted by R-Studio, and they are very rewarding experiences. They have the ability to both inspire and educate. Unfortunately, there are no conferences in my native Canada.\n\nPhoto by Andrew Neel on Unsplash"
  },
  {
    "objectID": "posts/2022-03-31-underrated-cran-packages/index.html",
    "href": "posts/2022-03-31-underrated-cran-packages/index.html",
    "title": "Underrated CRAN Packages",
    "section": "",
    "text": "I sit here looking for inspiration, nothing interesting to write about. Perhaps there are some popular R packages on CRAN that I don’t know about? You can explore the data on downloads from CRAN with the cranlogs package."
  },
  {
    "objectID": "posts/2022-03-31-underrated-cran-packages/index.html#top-cran-downloads",
    "href": "posts/2022-03-31-underrated-cran-packages/index.html#top-cran-downloads",
    "title": "Underrated CRAN Packages",
    "section": "Top CRAN downloads",
    "text": "Top CRAN downloads\nWith the following code, we can get the most popular packages from CRAN. The CRAN directory doesn’t represent all R packages, but a good amount of them.\n\nlibrary(tidyverse)\nlibrary(cranlogs)\ntop100 &lt;- cran_top_downloads(when = 'last-month', count = 100)\ntop100 %&gt;% head()\n\n  rank  package   count       from         to\n1    1  ggplot2 2502873 2022-07-17 2022-08-15\n2    2    rlang 2039913 2022-07-17 2022-08-15\n3    3 devtools 1844834 2022-07-17 2022-08-15\n4    4       sf 1756905 2022-07-17 2022-08-15\n5    5      cli 1672799 2022-07-17 2022-08-15\n6    6     glue 1624788 2022-07-17 2022-08-15\n\n\nFrom this list, we can see that the tidyverse represents a large amount of the top downloads with ggplot2, rlang and dplyr. The list includes the sf package for geospacial data, the glue package for string manipulation and the cli package which is used to create a command line interface for packages. Most of these packages I already have a good understanding of, so I need to narrow down the search."
  },
  {
    "objectID": "posts/2022-03-31-underrated-cran-packages/index.html#packages-installed",
    "href": "posts/2022-03-31-underrated-cran-packages/index.html#packages-installed",
    "title": "Underrated CRAN Packages",
    "section": "Packages installed",
    "text": "Packages installed\nYou can get a list of your installed packages with the installed_packages function. You can then filter the top 100 list and remove anything you already have installed to find new packages.\n\nmine &lt;- installed.packages() %&gt;%\n        data.frame() %&gt;%\n        select(Package)\nnew &lt;- top100 %&gt;%\n        filter(!package %in% mine$Package)\nnew\n\n   rank     package   count       from         to\n1     3    devtools 1844834 2022-07-17 2022-08-15\n2     7        ragg 1589393 2022-07-17 2022-08-15\n3     8 textshaping 1583922 2022-07-17 2022-08-15\n4    10       rgeos 1524077 2022-07-17 2022-08-15\n5    11         rgl 1499311 2022-07-17 2022-08-15\n6    18     pkgdown 1162782 2022-07-17 2022-08-15\n7    19  enrichwith 1158675 2022-07-17 2022-08-15\n8    20      brglm2 1154128 2022-07-17 2022-08-15\n9    31         zoo  873095 2022-07-17 2022-08-15\n10   50       Hmisc  706994 2022-07-17 2022-08-15\n11   60     rstatix  648757 2022-07-17 2022-08-15\n12   62      nloptr  637641 2022-07-17 2022-08-15\n13   63        lme4  623554 2022-07-17 2022-08-15\n14   68    corrplot  601404 2022-07-17 2022-08-15\n15   72       rJava  577002 2022-07-17 2022-08-15\n16   75      ggpubr  544839 2022-07-17 2022-08-15\n17   88     cowplot  493972 2022-07-17 2022-08-15\n18   94         car  461659 2022-07-17 2022-08-15\n\n\nFrom some quick research, I have found the following about the new packages:\n\nragg - a 2D library as an alternative to the RStudio default\nrgl - functions for 3D interactive graphics\nrgeos - a geometry package, but is currently planned to be retired at the end of 2023 for the sf package\nzoo - a library to deal with time series\npkgdown - a library fOR building a blog website, I use blogdown\nnloptr - a library for solving non-linear optimization problems\nHmisc - an assortment of different data analysis tools\nlme4 - for fitting linear and generalized linear mixed-effects models\nRcppEigen - integration of the eigen library in R for linear algebra"
  },
  {
    "objectID": "posts/2022-03-31-underrated-cran-packages/index.html#take-away",
    "href": "posts/2022-03-31-underrated-cran-packages/index.html#take-away",
    "title": "Underrated CRAN Packages",
    "section": "Take-away",
    "text": "Take-away\nHopefully your take-way is a simple method to explore R library that you have never heard about. I know that a few of the libraries seem interesting and worth further exploring.\nWhile we are at it, might as well find the daily values for the new packages and plot them for the last month.\n\nnew$package %&gt;%\n        cran_downloads(when = \"last-month\") %&gt;%\n        ggplot(aes(x = date, y = count, color = package)) +\n        geom_line()"
  },
  {
    "objectID": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html",
    "href": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html",
    "title": "Making the Connection with Crosstalk",
    "section": "",
    "text": "I recently wrote a post about creating dashboards in R which was based on the Flexdashboard library. My largest criticism was the lack of communication between visualizations on the same dashboard. This was before I learned about the Crosstalk package which adds this feature to html widgets, such as the Flexdashboard, to at least a limited degree."
  },
  {
    "objectID": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html#initialization",
    "href": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html#initialization",
    "title": "Making the Connection with Crosstalk",
    "section": "Initialization",
    "text": "Initialization\nThe Crosstalk package is available on CRAN and is loaded along with other important packages for this demonstration.\n\ninstall.packages(\"crosstalk\")\nlibrary(crosstalk)\nlibrary(tidyverse)\nlibrary(flexdashboard)\nlibrary(plotly)\n\nI have decided to use a Toronto Open dataset about city audits for apartment buildings. I limited the features to only the ones that I feel will be interesting to look at. More information about the data set can be found here.\n\ndownload.file(\"https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/4ef82789-e038-44ef-a478-a8f3590c3eb1/resource/979fb513-5186-41e9-bb23-7b5cc6b89915/download/Apartment%20Building%20Evaluation.csv\", \"data.csv\")\ndf &lt;- read_csv(\"data.csv\") %&gt;%\n        select(lng = LONGITUDE, \n               lat = LATITUDE, \n               SCORE, \n               YEAR_BUILT, \n               SITE_ADDRESS, \n               PROPERTY_TYPE) %&gt;% \n        slice_sample(n = 200)\n\nThe key to the crosstalk library is the SharedData functions. An object is created when a Data Frame is passed to the SharedData$new function. This is what enables communication between plots.\n\nshared_df &lt;- SharedData$new(df)"
  },
  {
    "objectID": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html#dashboard-creation",
    "href": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html#dashboard-creation",
    "title": "Making the Connection with Crosstalk",
    "section": "Dashboard Creation",
    "text": "Dashboard Creation\nThe dashboard is created pretty much as previous mentioned in my dashboard post, with the exception that the shared Data Frame object is passed rather than the Data Frame.\nThe dashboard can include filters that are very similar to the Shiny Apt filters, with the filter_* family of functions.\n\nfilter_slider(\"Score\", \"SCORE\", shared_df, ~SCORE, round = TRUE)\nfilter_checkbox(\"Property Type\", \"PROPERTY_TYPE\", shared_df, ~PROPERTY_TYPE, inline = TRUE)"
  },
  {
    "objectID": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html#conclusion",
    "href": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html#conclusion",
    "title": "Making the Connection with Crosstalk",
    "section": "Conclusion",
    "text": "Conclusion\nThe Crosstalk package does add some significant connectivity to Flex Dashboards. It is relatively simple to use with some basic functions. It does have the issue of not working with aggregating data. The utility of finding the mean value of a selection is something Tableu and PowerBI are still superior at. I think that it is still a welcome improvement."
  },
  {
    "objectID": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html#final-dashboard",
    "href": "posts/2022-03-18-making-the-connection-with-crosstalk/index.html#final-dashboard",
    "title": "Making the Connection with Crosstalk",
    "section": "Final Dashboard",
    "text": "Final Dashboard\n\n\n\n\n\nPhoto by Jason Goodmanon Unsplash"
  },
  {
    "objectID": "posts/2022-03-03-python-in-r-markdown/index.html",
    "href": "posts/2022-03-03-python-in-r-markdown/index.html",
    "title": "Python in R Markdown",
    "section": "",
    "text": "The main advantage of using the R Markdown format is the utility of running R code within the text. This is clearly more advantageous than just writing code in a Markdown file. R Markdown is however limited to R code, unable to run Python scripts. The R library reticulate looks to add this capability."
  },
  {
    "objectID": "posts/2022-03-03-python-in-r-markdown/index.html#initial-setup",
    "href": "posts/2022-03-03-python-in-r-markdown/index.html#initial-setup",
    "title": "Python in R Markdown",
    "section": "Initial Setup",
    "text": "Initial Setup\nThe initial setup requires the installation of the reticulate library, after installation you shouldn’t need to call it, but I do in the preceding code. I have loaded the trees dataset as a test dataset and the tidyverse library just to explore the data a bit.\n\nlibrary(reticulate)\nlibrary(tidyverse)\ndata(trees)\nglimpse(trees)\n\nRows: 31\nColumns: 3\n$ Girth  &lt;dbl&gt; 8.3, 8.6, 8.8, 10.5, 10.7, 10.8, 11.0, 11.0, 11.1, 11.2, 11.3, …\n$ Height &lt;dbl&gt; 70, 65, 63, 72, 81, 83, 66, 75, 80, 75, 79, 76, 76, 69, 75, 74,…\n$ Volume &lt;dbl&gt; 10.3, 10.3, 10.2, 16.4, 18.8, 19.7, 15.6, 18.2, 22.6, 19.9, 24.…\n\n\nNow, R Studio will use your local version of Python when you write any code in a code chuck labelled with the “{Python}” header. If you don’t have any local version, R Studio will ask if you would like to install Miniconda. From here, you will need to start downloading the required Python modules.\nModules can be downloaded with the pip python package installer from the terminal or command line. The easiest method in R Studio is within the terminal window next to the console window. The command used is pip install \"module name\". Some modules can be tricky and won’t work if not installed after other modules.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "posts/2022-03-03-python-in-r-markdown/index.html#multiple-environments",
    "href": "posts/2022-03-03-python-in-r-markdown/index.html#multiple-environments",
    "title": "Python in R Markdown",
    "section": "Multiple Environments",
    "text": "Multiple Environments\nAfter the setup, you should see some additional options in the environment in R Studio. You should see that you have the option to switch between the R and Python environments.\nData is transitioned from the R environment to the Python environment with the r variable. This method should pretty similar to the Shiny Apt’s use of input\\output. It is not only data that can move between environments, but functions too.\nThe following code takes data from the R environment and creates a plot in Seaborn. The mean values of the columns are calculated in python to be imported into the R environment. A simple linear model is created with the SKlearn module.\n\ndata = r.trees\nmeans = np.mean(data, axis = 0)\ndata[\"big\"] = data.Height &gt; means.Height \nsns.scatterplot(data = data, x= \"Girth\", y= \"Height\", hue = \"big\")\nsns.set_theme(color_codes=True)\nplt.show()\n\n\n\nfrom sklearn.linear_model import LinearRegression\nmdl = LinearRegression()\nmdl.fit(data[[\"Girth\"]], data[[\"Height\"]])\n\nLinearRegression()\n\nprint(mdl.coef_)\n\n[[1.05436881]]\n\n\nData is transitioned from Python to, R similarly with the variable py. Information on models can be passed but not the models themselves. This is important if you are more comfortable creating models in Python.\n\nprint(py$means)\n\n   Girth   Height   Volume \n13.24839 76.00000 30.17097 \n\nprint(py$mdl$intercept_)\n\n[1] 62.03131\n\npy$data %&gt;%\n        ggplot(aes(x = Girth, y = Height, colour = big)) +\n        geom_point()"
  },
  {
    "objectID": "posts/2022-02-23-new-features-in-r/index.html",
    "href": "posts/2022-02-23-new-features-in-r/index.html",
    "title": "New features in R",
    "section": "",
    "text": "&gt; Photo by Clint Patterson on Unsplash\nRecently I had updated my RStudio client and with it came a new update to R. This is an exploration of some of the most interesting changes from R 4.0 to R 4.1."
  },
  {
    "objectID": "posts/2022-02-23-new-features-in-r/index.html#native-pipe-function",
    "href": "posts/2022-02-23-new-features-in-r/index.html#native-pipe-function",
    "title": "New features in R",
    "section": "Native Pipe Function",
    "text": "Native Pipe Function\nDue to the extreme popularity of the magrittr pipe (‘%&gt;%’), R has developed its own native pipe (‘|&gt;’).\n\nlibrary(tidyverse)\ndata(\"morley\")\nmorley |&gt;\n        group_by(Expt) |&gt;\n        summarise(mean = mean(Speed, na.rm=TRUE))\n\n# A tibble: 5 × 2\n   Expt  mean\n  &lt;int&gt; &lt;dbl&gt;\n1     1  909 \n2     2  856 \n3     3  845 \n4     4  820.\n5     5  832.\n\n\nFrom this example, it is apparent that the behaviour of the native pipe is the same as the magrittr pipe.\nSome of the differences I have found is that the native pipe requires the brackets for functions, while the magrittr pipe will usually accept just the function name.\n\n2 %&gt;% sqrt()\n\n[1] 1.414214\n\n2 |&gt; sqrt()\n\n[1] 1.414214\n\n2 %&gt;% sqrt\n\n[1] 1.414214\n\n\n\n2 |&gt; sqrt\n\nError: The pipe operator requires a function call as RHS\n\n\nOne disadvantage of the native pipe is that it doesn’t support the placeholder operator (.) which helps refer to the data in the function. This is a useful function of the magrittr pipe when the data isn’t the first argument in the function, such as the lm function.\n\nmorley %&gt;% lm(Speed~Run, data = .)\n\n\nCall:\nlm(formula = Speed ~ Run, data = .)\n\nCoefficients:\n(Intercept)          Run  \n   856.0947      -0.3519  \n\nmorley |&gt; lm(Speed~Run, data = .)\n\nError in is.data.frame(data): object '.' not found\n\n\nOne advantage is there is no performance penalty as it acts the same as the function call. This is shown with the microbenchmark function, which shows not only the same level of performance as the regular call, but even the results themselves are shown as the function call.\n\nlibrary(microbenchmark)\nmicrobenchmark(sqrt(3),\n               4 |&gt; sqrt(),\n               5 %&gt;% sqrt())\n\nUnit: nanoseconds\n         expr  min   lq mean median   uq   max neval\n      sqrt(3)    0   50  146    100  100  6800   100\n      sqrt(4)    0    0   42      0  100   300   100\n 5 %&gt;% sqrt() 2300 2400 2730   2400 2500 26100   100\n\n\nSo when should we use the native vs the magrittr pipe? Well, it looks like not all the functionality of the magrittr pipe is carried over, so it should still be continued to be used. The native pipe, however, provides a good performance boost, which makes it a better option for code written in functions and libraries. I think that the major application should be to increase the readability of library and function code."
  },
  {
    "objectID": "posts/2022-02-23-new-features-in-r/index.html#lambda-functions",
    "href": "posts/2022-02-23-new-features-in-r/index.html#lambda-functions",
    "title": "New features in R",
    "section": "Lambda Functions",
    "text": "Lambda Functions\nThere has been a simplification in the creation of lambda functions. The notation is simplified, while the results are the same.\n\nlibrary(tidyverse)\nx &lt;- 0:10/10\ny1 &lt;- function(x) x + 0.5\ny2 &lt;- \\(x) x^2 +1\ng &lt;- ggplot(data.frame(x=x)) +\n        geom_function(fun = y1, aes(color = \"blue\")) +\n        geom_function(fun = y2, aes(color = \"red\"))\ng"
  },
  {
    "objectID": "posts/2022-02-23-new-features-in-r/index.html#other-minor-changes",
    "href": "posts/2022-02-23-new-features-in-r/index.html#other-minor-changes",
    "title": "New features in R",
    "section": "Other minor changes",
    "text": "Other minor changes\n\nThe default has been changed for ‘stringsAsFactors = FALSE’. Previously, when using the data.frame() or the read.table() the default option would turn strings into factors. This was an annoying feature that would always create headaches.\nIntroduction of an experimental implementation of hash tables. This development should be watched for people keen on program performance.\nc() can now combine factors to create a new factor. I am not familiar with the original behaviour, but this seems intuitive."
  },
  {
    "objectID": "posts/2022-02-11-fancy-tables-in-r/index.html#introduction",
    "href": "posts/2022-02-11-fancy-tables-in-r/index.html#introduction",
    "title": "Fancy Tables in R",
    "section": "Introduction",
    "text": "Introduction\nAs a continuation from my previous post exploring the use of the Stargazer library to create better looking tables, I thought I would look into the GT library. The GT library takes a different approach by creating an object class with the GT function. It is still able to create great looking tables in html or latex, but also adds support for RTF."
  },
  {
    "objectID": "posts/2022-02-11-fancy-tables-in-r/index.html#gt-library-intuition",
    "href": "posts/2022-02-11-fancy-tables-in-r/index.html#gt-library-intuition",
    "title": "Fancy Tables in R",
    "section": "GT Library Intuition",
    "text": "GT Library Intuition\nThe use of the GT library is pretty simple and starts with the creation of a GT object. For this example, I will use the ChickenWeight database that looks at the weights of different chickens.\n\nlibrary(gt)\nlibrary(tidyverse)\ndata(\"ChickWeight\")\n\nThe dataframe or tibble is passed into the GT function, which can be passed into additional modifier functions. This is better outlined and easier understood using the piping operator from the magrittr library, which is also included in the tidyverse library. The piping operator is incredibly useful and common.\n\nChickWeight %&gt;% head() %&gt;% gt()\n\n\n\n\n\n\n\n\nweight\nTime\nChick\nDiet\n\n\n\n\n42\n0\n1\n1\n\n\n51\n2\n1\n1\n\n\n59\n4\n1\n1\n\n\n64\n6\n1\n1\n\n\n76\n8\n1\n1\n\n\n93\n10\n1\n1\n\n\n\n\n\n\n\nThe result is a simple a clean table the displays the required data. There is a set structure for the GT objects, which is outlined in the following diagram:"
  },
  {
    "objectID": "posts/2022-02-11-fancy-tables-in-r/index.html#function-chaining",
    "href": "posts/2022-02-11-fancy-tables-in-r/index.html#function-chaining",
    "title": "Fancy Tables in R",
    "section": "Function Chaining",
    "text": "Function Chaining\nAs previously mentioned, by chaining GT functions together, we can add elements to the table. These elements can include titles, footnotes, source notes and conditional formatting similar to what you would use in an Excel table.\n\nChickWeight %&gt;%\n        head() %&gt;%\n        gt() %&gt;%\n        tab_header(\n                title = \"Chicken Weight data\", \n                subtitle = \"remember to weight your chickens!\"\n        ) %&gt;%\n        tab_footnote(footnote = \"measured in seconds\",\n                     locations = cells_column_labels(Time)) %&gt;%\n        tab_source_note(source_note = \"From ChickenWeight Database\") %&gt;%\n        tab_style(style = cell_fill(color = \"red\"),\n                  locations = cells_body(\n                          columns = weight,\n                          rows = weight &gt; 50\n                  )) %&gt;%\n        summary_rows(columns = c(weight,Time),\n                           fns = list(\n                                   avg = ~mean(., na.rm = TRUE),\n                                   total = ~sum(., na.rm = TRUE),\n                                   s.d. = ~sd(., na.rm = TRUE))\n                           )\n\n\n\n\n\n  \n    \n      Chicken Weight data\n    \n    \n      remember to weight your chickens!\n    \n  \n  \n    \n      \n      weight\n      Time1\n      Chick\n      Diet\n    \n  \n  \n    \n42\n0\n1\n1\n    \n51\n2\n1\n1\n    \n59\n4\n1\n1\n    \n64\n6\n1\n1\n    \n76\n8\n1\n1\n    \n93\n10\n1\n1\n    avg\n64.17\n5.00\n—\n—\n    total\n385.00\n30.00\n—\n—\n    s.d.\n18.24\n3.74\n—\n—\n  \n  \n    \n      From ChickenWeight Database\n    \n  \n  \n    \n      1 measured in seconds"
  },
  {
    "objectID": "posts/2022-02-11-fancy-tables-in-r/index.html#helper-functions",
    "href": "posts/2022-02-11-fancy-tables-in-r/index.html#helper-functions",
    "title": "Fancy Tables in R",
    "section": "Helper functions",
    "text": "Helper functions\nThe previous code includes smaller helper functions within the element functions. These functions (ex. cells_body) provide targeting information for locations or conditioning. There is a learning curve for these functions, I would recommend looking them up as you work on your table rather than trying to learn them all. The everything() function seems to be of particular usefulness, as it allows you to use all values, such as all columns."
  },
  {
    "objectID": "posts/2022-02-11-fancy-tables-in-r/index.html#additional-notes",
    "href": "posts/2022-02-11-fancy-tables-in-r/index.html#additional-notes",
    "title": "Fancy Tables in R",
    "section": "Additional notes",
    "text": "Additional notes\nThe summary_rows function can create summary rows for each grouping if the grouping is defined in the function or if the data is grouped itself. You can then use the grand_summary_rows function to create a grand summary.\nIt is always good practice to pass the na.rm = TRUE for your summary functions. Without it, you can create undesirable results.\nA useful resource for learning the GT library is an article on R studio found here. It goes through more in depth on the topics that I have skimmed over."
  },
  {
    "objectID": "posts/2022-01-29-fitness-tracker-modeling-ml/index.html#initialization",
    "href": "posts/2022-01-29-fitness-tracker-modeling-ml/index.html#initialization",
    "title": "Fitness Tracker Modeling: ML",
    "section": "Initialization",
    "text": "Initialization\nThe following code was used to initialize the required R libraries, as well as downloading the required data and store it into memory. There are some columns of the data that were not required for modelling which were excluded (ex. usernames).\n\nlibrary(caret)\nlibrary(gbm)\nlibrary(dplyr)\nlibrary(randomForest)\nlibrary(ggplot2)\nset.seed(90210)\nNtree &lt;- 200\n\ndownload.file(\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\", \"training.csv\")\ndownload.file(\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\", \"testing.csv\")\ntrain &lt;- read.csv2(\"training.csv\", sep = \",\")[,-c(1:7)]\ntest &lt;- read.csv2(\"testing.csv\", sep = \",\")[,-c(1:7)]"
  },
  {
    "objectID": "posts/2022-01-29-fitness-tracker-modeling-ml/index.html#reducing-predictors",
    "href": "posts/2022-01-29-fitness-tracker-modeling-ml/index.html#reducing-predictors",
    "title": "Fitness Tracker Modeling: ML",
    "section": "Reducing predictors",
    "text": "Reducing predictors\nThe data contains way too many predictors (153 in total) to produce accurate and simple models. Some trimming is required. The first trim is performed with the near zero variance function from the caret library, which finds the predictors that exhibit near zero variation. These predictors would add little benefit to include in models.\n\nnz &lt;- nearZeroVar(train)\ntrain &lt;- train[,-nz]\ntest &lt;- test[-nz]\n\nFrom this step, the number of predictors is reduced to 94. There remains numerous NA values in the data. These values are examined in the next chunk of code.\n\nmaxi &lt;- length(train) - 1\nvalna &lt;- 1:maxi\n\nfor (i in 1:maxi) {\n        train[,i] &lt;- as.numeric(train[,i])\n        test[,i] &lt;- as.numeric(test[,i])\n        valna[i] &lt;- mean(is.na(train[,i]))\n}\n\ntable(valna)\n\nvalna\n                0 0.979308938946081 \n               52                41 \n\n\nThe code shows that there are 52 predictors that have no missing data and 41 predictors that are mostly missing values. These predictors would add little value to the modelling and are removed with the following code\n\ntrain &lt;- train[, valna == 0]\ntest &lt;- test[, valna == 0]\n\nThe training was then divided to create a validation set which will be used for cross validation. Note that the random forest algorithm has built in cross validation with the “out of bag error”. About 1/3 of the data is used in a random forest.\n\nValid &lt;- createDataPartition(train$classe, p = 0.3)[[1]]\nvalid &lt;- train[Valid,]\ntrain &lt;- train[-Valid,]\n\nThe next step is to utilize the variable of importance function in the caret library to reduce the number of predictors even further. The train data is still very large, but by making a sample set from the training data and modelling from that we can get a reasonable approximation of the variables of importance.\n\nstrain &lt;- rbind(sample_n(train[train$classe == \"A\",],round(mean(train$classe == \"A\")*200,0)),\n                sample_n(train[train$classe == \"B\",],round(mean(train$classe == \"B\")*200,0)),\n                sample_n(train[train$classe == \"C\",],round(mean(train$classe == \"C\")*200,0)),\n                sample_n(train[train$classe == \"D\",],round(mean(train$classe == \"D\")*200,0)),\n                sample_n(train[train$classe == \"E\",],round(mean(train$classe == \"E\")*200,0))\n)\n\nThe sample set was set to ensure an accurate representation of the ‘classe’ variable in the training data. Two models were completed and their variables of importance were added together.\n\nmdl1 &lt;- train(classe~., data = strain, method = \"rf\", ntree = Ntree)\nmdl2 &lt;- train(classe~., data = strain, method = \"gbm\", verbose = FALSE)\nvar &lt;- varImp(mdl1)$importance &gt; 50 | varImp(mdl2)$importance &gt; 50\nvarorder &lt;- order(varImp(mdl1)$importance, decreasing = TRUE)\nVarimp &lt;- row.names(varImp(mdl1)$importance)[varorder[1:2]]\n\nA value of 50 was used for a cut-off value. The total number of predictors has been reduced to 4.\n\nvalid &lt;- valid[,var]\ntrain &lt;- train[,var] %&gt;% slice_sample(prop = 0.75)\ntest &lt;- test[,var]"
  },
  {
    "objectID": "posts/2022-01-29-fitness-tracker-modeling-ml/index.html#modelling",
    "href": "posts/2022-01-29-fitness-tracker-modeling-ml/index.html#modelling",
    "title": "Fitness Tracker Modeling: ML",
    "section": "Modelling",
    "text": "Modelling\nWith the reduced predictors, the models can now be trained. Since these model will look at the entire training data set, it will require a lot of time. The models include:\n- Random forest\n- Generalized Boosted\n- Linear Discriminant\n- Combined\nThe randomForest function is used as it is more efficient than the train function. The data method is also more efficient than using the formula method.\n\nmdl11 &lt;- randomForest(x = train[,1:(ncol(train) - 1)], y = as.factor(train[,ncol(train)]), ntree = Ntree, proximity = TRUE)\nmdl21 &lt;- train(classe~., data = train, method = \"gbm\", verbose = FALSE)\nmdl31 &lt;- train(classe~., data = train, method = \"lda\")\n\nThe following code constructs the combined model\n\npmdl11 &lt;- predict(mdl11, valid)\npmdl21 &lt;- predict(mdl21, valid)\npmdl31 &lt;- predict(mdl31, valid)\njoin &lt;- data.frame(pmdl11, pmdl21, pmdl31, classe = valid$classe)\njmdl &lt;- randomForest(x = join[,1:3], y = as.factor(join$classe), ntree = Ntree)"
  },
  {
    "objectID": "posts/2022-01-29-fitness-tracker-modeling-ml/index.html#model-evaluation",
    "href": "posts/2022-01-29-fitness-tracker-modeling-ml/index.html#model-evaluation",
    "title": "Fitness Tracker Modeling: ML",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nThe new models will need to be examined against the validation data set. The out of bag error for the random forest models were not used, as the validation data provides a uniform comparison for all models. The following function was used to test the models:\n\nExacc &lt;- function(mdl, test){\n        mean(predict(mdl,test) == test$classe)\n}\n\nThe model’s accuracy are summarized in the following dataframe when they are used to predict the results in the validation set:\n\n\n  Model  accuracy\n1 mdl11 0.8935303\n2 mdl21 0.7989472\n3 mdl31 0.3625403\n4 joint 0.8897945"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Custom OpenAI Chatbot Pt2: Fun with Lang Chain\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\n\n\n\n\n\nCustom OpenAI Chatbot Pt1: PDF scanning\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\n\n\n\n\n\nTree Based Methods: Exploring the Forest\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\n\n\n\n\n\nCreating Posts for Quarto Blog\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\n\n\n\n\n\nQuarto: The successor to R Markdown\n\n\n\n\n\n\n\n\nAug 1, 2022\n\n\n\n\n\n\n\nRelationship Extraction with Spacyr\n\n\n\n\n\n\n\n\nJul 4, 2022\n\n\n\n\n\n\n\nWebscraping in R with Rvest\n\n\n\n\n\n\n\n\nJun 22, 2022\n\n\n\n\n\n\n\nText Prediction Shiny App pt 2\n\n\n\n\n\n\n\n\nJun 8, 2022\n\n\n\n\n\n\n\nThe beauty of List comprehensions in Python\n\n\n\n\n\n\n\n\nMay 16, 2022\n\n\n\n\n\n\n\nFormatting our output with Python’s F strings\n\n\n\n\n\n\n\n\nMay 9, 2022\n\n\n\n\n\n\n\nMerging PDFs with Python\n\n\n\n\n\n\n\n\nApr 14, 2022\n\n\n\n\n\n\n\nBenchmarking Data Tables\n\n\n\n\n\n\n\n\nApr 13, 2022\n\n\n\n\n\n\n\nR-Bloggers site\n\n\n\n\n\n\n\n\nApr 11, 2022\n\n\n\n\n\n\n\nUnderrated CRAN Packages\n\n\n\n\n\n\n\n\nMar 31, 2022\n\n\n\n\n\n\n\nSimple Neural Networks in Python\n\n\n\n\n\n\n\n\nMar 20, 2022\n\n\n\n\n\n\n\nMaking the Connection with Crosstalk\n\n\n\n\n\n\n\n\nMar 18, 2022\n\n\n\n\n\n\n\nCreating Dashboards in R\n\n\n\n\n\n\n\n\nMar 10, 2022\n\n\n\n\n\n\n\nPython in R Markdown\n\n\n\n\n\n\n\n\nMar 3, 2022\n\n\n\n\n\n\n\nBike shares in Toronto\n\n\n\n\n\n\n\n\nFeb 27, 2022\n\n\n\n\n\n\n\nNew features in R\n\n\n\n\n\n\n\n\nFeb 23, 2022\n\n\n\n\n\n\n\nSpeed cameras in Toronto\n\n\n\n\n\n\n\n\nFeb 16, 2022\n\n\n\n\n\n\n\nFancy Tables in R\n\n\n\n\n\n\n\n\nFeb 11, 2022\n\n\n\n\n\n\n\nJob posting analysis\n\n\n\n\n\n\n\n\nFeb 6, 2022\n\n\n\n\n\n\n\nFitness Tracker Modeling: ML\n\n\n\n\n\n\n\n\nJan 29, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The Data Sandbox is a collection of Data Science projects and discussions on Data Science topics. These projects are based on R or Python code. The Sandbox was started in January 2022.\nMark Edney is the sole writer for all articles. He performs his own research, and all projects are those that he has completed from a selection of different Data Science certifications. He also manages the website.\nMark is a Engineer in Training located in Toronto Ontario Canada. He has studied data science since 2019 but has programmed for years before that. He is proficient in R, Python and Matlab but has previously learned some C and C++.\nThis website is created in R with the blogdown package. All assets are stored on Github and generously hosted by Netlify. For any inquires, please refer to the contact page.\n\nBlogroll\nA collection of interesting and related blogs:\n\nr-bloggers\npython-bloggers"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Data Sandbox",
    "section": "",
    "text": "Custom OpenAI Chatbot Pt2: Fun with Lang Chain\n\n\n\n\n\n\nHow-to\n\n\nPython\n\n\nAI\n\n\n\nThe final steps for the creation of a custom Chat GPT Chat bot\n\n\n\n\n\nNov 29, 2023\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nCustom OpenAI Chatbot Pt1: PDF scanning\n\n\n\n\n\n\nHow-to\n\n\nPython\n\n\nAI\n\n\n\nA PDF OCR reader for the creation of an chatbot.\n\n\n\n\n\nOct 30, 2023\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nTree Based Methods: Exploring the Forest\n\n\n\n\n\n\nGeneral\n\n\nR\n\n\nML\n\n\n\nA study of the different tree based methods in machine learning\n\n\n\n\n\nNov 15, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Posts for Quarto Blog\n\n\n\n\n\n\nHow-to\n\n\nR\n\n\nRmarkdown\n\n\nQuarto\n\n\nShiny App\n\n\n\nA guide in creating a template for Quarto Blog posts.\n\n\n\n\n\nAug 15, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto: The successor to R Markdown\n\n\n\n\n\n\nHow-to\n\n\nR\n\n\nRmarkdown\n\n\nPython\n\n\nQuarto\n\n\n\nA review of Quarto, the potenial successor to R Markdown\n\n\n\n\n\nAug 1, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork Graphs in R\n\n\n\n\n\n\nHow-to\n\n\nProject\n\n\nNLP\n\n\nR\n\n\nGGPlot\n\n\n\nA look at the diffrent options for exploring network graphs in R.\n\n\n\n\n\nJul 12, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nRelationship Extraction with Spacyr\n\n\n\n\n\n\nHow-to\n\n\nProject\n\n\nNLP\n\n\nR\n\n\n\nA tutorial of the Spacyr packaged for the realationship extraction through the stormlight archieve book series.\n\n\n\n\n\nJul 4, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping in R with Rvest\n\n\n\n\n\n\nHow-to\n\n\nProject\n\n\nNLP\n\n\nR\n\n\n\nA tutorial on the Rvest package in R\n\n\n\n\n\nJun 22, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nText Prediction Shiny App pt 2\n\n\n\n\n\n\nProject\n\n\nR\n\n\nNLP\n\n\nShiny App\n\n\n\nThe second part of the creation of a test prediction shiny app.\n\n\n\n\n\nJun 8, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nThe beauty of List comprehensions in Python\n\n\n\n\n\n\nHow-to\n\n\nPython\n\n\n\nList Comprehensions are an elegent and useful tool in Python, that every Python coder should know about.\n\n\n\n\n\nMay 16, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nFormatting our output with Python’s F strings\n\n\n\n\n\n\nHow-to\n\n\nPython\n\n\n\nUsing F string to format the output from Python\n\n\n\n\n\nMay 9, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nLevel up your programming skills\n\n\n\n\n\n\nGeneral\n\n\nPython\n\n\nR\n\n\n\nA look into the Hacker Rank website as a means to increase ones programming skills.\n\n\n\n\n\nMay 1, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nDashboards in R with Shiny Dashboard\n\n\n\n\n\n\nHow-to\n\n\nGGPlot\n\n\nLeaflet\n\n\nR\n\n\nShiny App\n\n\n\nAn outline to creating great looking dashboards with Shiny Dashboards\n\n\n\n\n\nApr 20, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nMerging PDFs with Python\n\n\n\n\n\n\nHow-to\n\n\nPython\n\n\n\nA simple script for merging PDFs.\n\n\n\n\n\nApr 14, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmarking Data Tables\n\n\n\n\n\n\nHow-to\n\n\nR\n\n\n\nA look into the claim performance of Data Tables in the R language.\n\n\n\n\n\nApr 13, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nR-Bloggers site\n\n\n\n\n\n\nGeneral\n\n\nR\n\n\n\nA collection of R blogs\n\n\n\n\n\nApr 11, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nUnderrated CRAN Packages\n\n\n\n\n\n\nProject\n\n\nR\n\n\n\nA search for popular R packages, that I would would otherwise miss.\n\n\n\n\n\nMar 31, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Neural Networks in Python\n\n\n\n\n\n\nHow-to\n\n\nPython\n\n\nNN\n\n\n\nAn outline for creating a simple Neural Network in Python\n\n\n\n\n\nMar 20, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nMaking the Connection with Crosstalk\n\n\n\n\n\n\nHow-to\n\n\nR\n\n\nShiny App\n\n\nGGPlot\n\n\nRmarkdown\n\n\nLeaflet\n\n\n\nAn investigation of the crosstalk library in R with Toronto Apartment data\n\n\n\n\n\nMar 18, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Dashboards in R\n\n\n\n\n\n\nHow-to\n\n\nR\n\n\nShiny App\n\n\nGGPlot\n\n\nRmarkdown\n\n\nLeaflet\n\n\n\nA demostration of the Flexdashboard package\n\n\n\n\n\nMar 10, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nPython in R Markdown\n\n\n\n\n\n\nHow-to\n\n\nR\n\n\nPython\n\n\nRmarkdown\n\n\n\nAn R package for writting Python code in R Markdown\n\n\n\n\n\nMar 3, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nBike shares in Toronto\n\n\n\n\n\n\nProject\n\n\nShiny App\n\n\nR\n\n\n\nAnalysis of a bike sharing app in Toronto\n\n\n\n\n\nFeb 27, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nNew features in R\n\n\n\n\n\n\nGeneral\n\n\nR\n\n\n\nA look at the new features introduced in the 4.0 version of R\n\n\n\n\n\nFeb 23, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nSpeed cameras in Toronto\n\n\n\n\n\n\nProject\n\n\nR\n\n\nLeaflet\n\n\n\nCreating a Leaflet map of all the speed cameras in Toronto\n\n\n\n\n\nFeb 16, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nFancy Tables in R\n\n\n\n\n\n\nHow-to\n\n\nR\n\n\n\nCreating fancy tables with the GT library\n\n\n\n\n\nFeb 11, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nJob posting analysis\n\n\n\n\n\n\nProject\n\n\nR\n\n\nShiny App\n\n\n\nA project used to study the occurance of keywords in a job posting.\n\n\n\n\n\nFeb 6, 2022\n\n\nMark Edney\n\n\n\n\n\n\n\n\n\n\n\n\nFitness Tracker Modeling: ML\n\n\n\n\n\n\nProject\n\n\nGGPlot\n\n\nML\n\n\nR\n\n\n\nAn analysis of Data collected by Fitness Trackers\n\n\n\n\n\nJan 29, 2022\n\n\nMark Edney\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-01-30-job-posting-analysis/index.html",
    "href": "posts/2022-01-30-job-posting-analysis/index.html",
    "title": "Job posting analysis",
    "section": "",
    "text": "Recently, there was a post on medium about the use of Natural Language Processing (NLP) to study a job posting for keywords. I found that this article was very similar to R shiny App that I created a while ago. 1"
  },
  {
    "objectID": "posts/2022-01-30-job-posting-analysis/index.html#introduction",
    "href": "posts/2022-01-30-job-posting-analysis/index.html#introduction",
    "title": "Job posting analysis",
    "section": "Introduction",
    "text": "Introduction\nTechnology has changed the job application process, making it easier and quicker to apply to jobs. As a result, the average job posting will receive around 250 resumes. 2 So how can hiring managers handle spending their time looking through that many resumes for one posting? That’s easy, they cheat.\nHiring Managers no longer look at individual resumes, but use automatic software called applicant tracking system (ATS). These programs filter resumes by a set of keywords, reducing the amount of resumes to a more manageable amount. So how can a job applicant make sure their resume is looked at? Well, they should cheat.\nThe medium article I mentioned uses Python and Natural Language Processing (NLP) to skim through the job posting to look for the most common words used. This is useful information, but not necessarily the keywords used by the ATS software. I propose the use of an R Shiny App to filter a job posting by a list of common keywords.\nAn R Shiny App is an interactive web based application that runs R code. The syntax for a Shiny App is a little different from R and requires some additional understanding. The product will be a basic, interactive program that can be hosted online. One free Shiny App hosting site that I recommend is shinyapps.io."
  },
  {
    "objectID": "posts/2022-01-30-job-posting-analysis/index.html#inialization",
    "href": "posts/2022-01-30-job-posting-analysis/index.html#inialization",
    "title": "Job posting analysis",
    "section": "Inialization",
    "text": "Inialization\nThe shiny App will require the following libraries.\n\nlibrary(shiny)\nlibrary(wordcloud2)\nlibrary(tidyverse)\nlibrary(XML)\nlibrary(rvest)\nlibrary(tidytext)\n\nThe Shiny App will use a csv files which contains a set of keywords that ATS will look for. This list was found online, but I have modified by adding additional keywords as I see fit. The file can be downloaded here from my GitHub site. Here is a sample of some keywords:\n\nKeywords &lt;- read_csv(\"Keywords.csv\") \nKeywords$Keys %&gt;% head()\n\n[1] \".NET\"                \"account management\"  \"accounting\"         \n[4] \"accounts payable\"    \"accounts receivable\" \"acquisition\""
  },
  {
    "objectID": "posts/2022-01-30-job-posting-analysis/index.html#app-structure",
    "href": "posts/2022-01-30-job-posting-analysis/index.html#app-structure",
    "title": "Job posting analysis",
    "section": "App Structure",
    "text": "App Structure\nOne issue I found when developing this application was the use of keywords that are a combination of multiple words. This creates some complications, as a simple search of keywords would use only the first word and lose the context.\nThis challenge was met by breaking the website down into ngrams. An over simplification of a ngram is a group of n number of words. Wikipedia has a very good page that better explains ngrams.3 The website can then be split into ngrams of different lengths and the keywords searched for.\nAs a example, the phrase:\n\nThe quick brown fox\n\nfor a ngram of length 1 would return:\n\n(The) (quick) (brown) (fox)\n\nfor a ngram of length 2 would return:\n\n(The quick) (quick brown) (brown fox)\n\nand for a ngram of length 3 would return:\n\n(The quick brown) (quick brown fox)"
  },
  {
    "objectID": "posts/2022-01-30-job-posting-analysis/index.html#application-coding",
    "href": "posts/2022-01-30-job-posting-analysis/index.html#application-coding",
    "title": "Job posting analysis",
    "section": "Application Coding",
    "text": "Application Coding\n\nshinyApp(\n#This is the standard format for a shiny app\n        \n#The UI function controls all the frontend for the app\n        ui = fluidPage(\n                titlePanel(\"Job Posting Word Cloud\"),\n                sidebarLayout(\n                        sidebarPanel(\n#The user is asked for a url\n                                textInput(\"url\", \"input URL\", value = \"https://www.google.com/\")\n                                ),\n                        mainPanel(\n#The word cloud plot is displayed\n                                h4(\"Key-Word Cloud\"),\n                                wordcloud2Output(\"plot\")\n                                )\n                        )\n                ),\n        \n#The server function controls the backend for the app\n        server = function(input, output){\n                \n#The keywords are loaded and an index of how many words per keyword is created\n                Keywords &lt;- read_csv(\"Keywords.csv\")\n                Keywords$Keys &lt;- str_to_lower(Keywords$Keys) \n                index &lt;- Keywords$Keys %&gt;% str_count(\" \")\n                \n#The { brackets are used to create reactive functions which continuously run \n                data &lt;- reactive({\n#The input variable is how the server side receives data from the ui side\n                url &lt;- input$url\n#The text is read from the url provide by the user\n                data &lt;- text %&gt;%\n                        data.frame(text = .)\n#Since there are ngrams of length 1-3, there a three search's that are concatenated together\n                rbind(data %&gt;%\n#the unnest_tolkens from the tidytext library is used to create the ngrams\n                              unnest_tokens(word, text, token = \"ngrams\", n= 1) %&gt;%\n#A count is performed on each ngram in the website to find the most common ngrams \n                              count(word, name = 'freq', sort = TRUE) %&gt;%\n#The ngram count is then filtered by the keywords of the same ngram length\n                              filter(word %in% Keywords$Keys[index == 0]),\n#The steps are repeated for bigrams (ngrams of length 2) and trigrams(ngrams of length 3)\n                      data %&gt;%\n                              unnest_tokens(word, text, token = \"ngrams\", n= 2) %&gt;%\n                              count(word, name = 'freq', sort = TRUE) %&gt;%\n                              filter(word %in% Keywords$Keys[index == 1]),\n                      data %&gt;%\n                              unnest_tokens(word, text, token = \"ngrams\", n= 3) %&gt;%\n                              count(word, name = 'freq', sort = TRUE) %&gt;%\n                              filter(word %in% Keywords$Keys[index == 2]))\n                        })\n                \n#The plot/wordcloud needs to be saved as an output value\n#The output variable is how the server sends data back to the UI\n                output$plot &lt;- renderWordcloud2({\n#One part of the strange syntax of a shiny app is that the since the data is reactive\n#and changes with the user input, it is passed in a function so it needs to be called\n#as data ()\n                        wordcloud2(data())\n                        })\n        },\n\n  options = list(height = 500)\n)"
  },
  {
    "objectID": "posts/2022-01-30-job-posting-analysis/index.html#shiny-app",
    "href": "posts/2022-01-30-job-posting-analysis/index.html#shiny-app",
    "title": "Job posting analysis",
    "section": "Shiny App",
    "text": "Shiny App"
  },
  {
    "objectID": "posts/2022-01-30-job-posting-analysis/index.html#footnotes",
    "href": "posts/2022-01-30-job-posting-analysis/index.html#footnotes",
    "title": "Job posting analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nuse-python-and-nlp-to-boost-your-resume↩︎\nResume Screening: A How-To Guide For Recruiters↩︎\nWiki: ngrams↩︎"
  },
  {
    "objectID": "posts/2022-02-16-/index.html#objective",
    "href": "posts/2022-02-16-/index.html#objective",
    "title": "Speed cameras in Toronto",
    "section": "Objective",
    "text": "Objective\nThis report plots the speed cameras in the Greater Toronto Area from the data provided by Open Toronto, which can be found here."
  },
  {
    "objectID": "posts/2022-02-16-/index.html#initialization",
    "href": "posts/2022-02-16-/index.html#initialization",
    "title": "Speed cameras in Toronto",
    "section": "Initialization",
    "text": "Initialization\nThe following code is used to initialize the required libraries.\n\ninstall.packages(\"opendatatoronto\", repos = \"https://cran.us.r-project.org\", dependencies = TRUE)\n\npackage 'opendatatoronto' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\Mark\\AppData\\Local\\Temp\\RtmpglvqBO\\downloaded_packages\n\nlibrary(opendatatoronto)\nlibrary(dplyr)\nlibrary(leaflet)\n\nThe following code is provided by the Open Toronto site to download the dataset.\n\n# get package\npackage &lt;- show_package(\"a154790c-4a8a-4d09-ab6b-535ddb646770\")\n\n# get all resources for this package\nresources &lt;- list_package_resources(\"a154790c-4a8a-4d09-ab6b-535ddb646770\")\n\n# identify datastore resources; by default, Toronto Open Data sets datastore resource format to CSV for non-geospatial and GeoJSON for geospatial resources\ndatastore_resources &lt;- filter(resources, tolower(format) %in% c('csv', 'geojson'))\n\n# load the first datastore resource as a sample\ndata &lt;- filter(datastore_resources, row_number()==1) %&gt;% get_resource()"
  },
  {
    "objectID": "posts/2022-02-16-/index.html#plotting-the-data",
    "href": "posts/2022-02-16-/index.html#plotting-the-data",
    "title": "Speed cameras in Toronto",
    "section": "Plotting the Data",
    "text": "Plotting the Data\nThe geometry in the dataset can be used directly with Leaflet, and the longitude and latitude do not need to be separated.\n\ndf &lt;- data$geometry\n\nCustom icons for the speed cameras can be used with the following code:\n\ncameraicon &lt;- makeIcon(\n        iconUrl = \"https://www.flaticon.com/svg/static/icons/svg/2164/2164608.svg\",\n        iconWidth = 35, iconHeight = 35\n)\n\nFinally, all the data and options can be passed to the leaflet function.\n\nplt &lt;- df %&gt;%\n        leaflet() %&gt;%\n        addTiles() %&gt;%\n        addMarkers(icon = cameraicon, clusterOptions = markerClusterOptions(), popup = data$location)"
  },
  {
    "objectID": "posts/2022-02-27-bike-shares-in-toronto/index.html#bike-rental-shiny-app",
    "href": "posts/2022-02-27-bike-shares-in-toronto/index.html#bike-rental-shiny-app",
    "title": "Bike shares in Toronto",
    "section": "Bike Rental Shiny App",
    "text": "Bike Rental Shiny App\nThis application use the data collected from the Toronto Open Data to generate a histogram of the usage of rental bikes in Toronto during the month of June in 2020.\n\ninstall.packages(\"opendatatoronto\", \n                 repos = \"https://cran.us.r-project.org\",\n                 dependencies = TRUE)\nlibrary(opendatatoronto)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(shiny)"
  },
  {
    "objectID": "posts/2022-02-27-bike-shares-in-toronto/index.html#ui",
    "href": "posts/2022-02-27-bike-shares-in-toronto/index.html#ui",
    "title": "Bike shares in Toronto",
    "section": "UI",
    "text": "UI\nThere are two user inputs on the UI side:\n\nA slider that limits the maximum and minimum of the displayed values\nA checkbox that excludes users with a annual bike pass\n\n\n        sidebarPanel(\n            sliderInput(\"dur\",\n                        \"Trip Duration:\",\n                        min = 0,\n                        max = 500,\n                        value = c(0,500)),\n            checkboxInput(\"freq\",\n                        \"Exclude annual users:\",\n                        value = FALSE))"
  },
  {
    "objectID": "posts/2022-02-27-bike-shares-in-toronto/index.html#server",
    "href": "posts/2022-02-27-bike-shares-in-toronto/index.html#server",
    "title": "Bike shares in Toronto",
    "section": "Server",
    "text": "Server\nThe following code is used for the server side logic, this includes downloading the data from the ‘opendatatoronto’ library.\n\n # get package\n    package &lt;- show_package(\"7e876c24-177c-4605-9cef-e50dd74c617f\")\n    \n    # get all resources for this package\n    resources &lt;- list_package_resources(\"7e876c24-177c-4605-9cef-e50dd74c617f\")\n    # identify datastore resources; by default, Toronto Open Data sets datastore resource format to CSV for non-geospatial and GeoJSON for geospatial resources\n    datastore_resources &lt;- filter(resources, tolower(format) %in% c('zip', 'geojson'))\n    # load the first datastore resource as a sample\n    data &lt;- filter(datastore_resources, name == \"Bike share ridership 2020\") %&gt;% get_resource()\n    data2 &lt;-  data$`2020-06.csv`\n    data2[grepl(\"Time\",names(data2))] &lt;- \n        lapply(data2[grepl(\"Time\",names(data2))], parse_date_time, orders = \"mdy HM\")\n    data2$Dur &lt;- as.numeric(data2$End.Time - data2$Start.Time,units=\"mins\")"
  },
  {
    "objectID": "posts/2022-02-27-bike-shares-in-toronto/index.html#application",
    "href": "posts/2022-02-27-bike-shares-in-toronto/index.html#application",
    "title": "Bike shares in Toronto",
    "section": "Application",
    "text": "Application\nThe final application takes a while to load as the data needs to be downloaded and sorted through. In future iterations, I would save the data locally as an RDS file."
  },
  {
    "objectID": "posts/2022-03-10-creating-dashboard-in-r/index.html",
    "href": "posts/2022-03-10-creating-dashboard-in-r/index.html",
    "title": "Creating Dashboards in R",
    "section": "",
    "text": "Dashboards are a great way to demonstrate knowledge and engage decision makers. Their utility has made PowerBI and Tableau household names. And while these solutions do support R and Python scripts and visualizations, the Flexdashboard package seeks to compete. The Flexdashboard packages does this all in R with the simplicity of writing a R Markdown file."
  },
  {
    "objectID": "posts/2022-03-10-creating-dashboard-in-r/index.html#initial-setup",
    "href": "posts/2022-03-10-creating-dashboard-in-r/index.html#initial-setup",
    "title": "Creating Dashboards in R",
    "section": "Initial Setup",
    "text": "Initial Setup\nThe setup is simple, you just need to download and load the Flexdashboard package. With the package installed, the easiest way to start is by creating a new R Markdown file using the Flexdashboard template. Loading the Shiny package is useful if you would like to use interactive plots, but it is not necessary.\nThe dashboard can be laid out by either columns or by rows, it doesn’t really make a difference. Just change the text of columns with rows in the following walk-through. A column is set up with “## Column” as the header. The size of the plot region can then be modified with “{data-width=”500”}” in the same header line. The next line should be the plot/area title, which is included with “### Plot Title” header. All that is left is to include a code chunk with your plot.\n\nlibrary(flexdashboard)\nlibrary(shiny)"
  },
  {
    "objectID": "posts/2022-03-10-creating-dashboard-in-r/index.html#sample-data",
    "href": "posts/2022-03-10-creating-dashboard-in-r/index.html#sample-data",
    "title": "Creating Dashboards in R",
    "section": "Sample Data",
    "text": "Sample Data\nI decided to demonstrate different dashboard features with a data set from Open Canada about charitable donations. More information can be found here\n\ndownload.file(\"https://www150.statcan.gc.ca/n1/tbl/csv/45100007-eng.zip\", \"donordata.zip\")"
  },
  {
    "objectID": "posts/2022-03-10-creating-dashboard-in-r/index.html#sample-dashboard-1",
    "href": "posts/2022-03-10-creating-dashboard-in-r/index.html#sample-dashboard-1",
    "title": "Creating Dashboards in R",
    "section": "Sample Dashboard 1",
    "text": "Sample Dashboard 1\nThe first dashboard was set up with the default columns layout. It includes an interactive bar chart, an interactive box plot and a pie chart. All the plot were created with GGplot2, the two plot were made interactive with the GGplotly function from the Plotly package. I created a pie chart to demonstrate the use of regular ggplots and because I recently read a complaint about GGplot2 for the creation of Pie charts on Reddit. In my opinion, Pie charts are not very good very conveying information."
  },
  {
    "objectID": "posts/2022-03-10-creating-dashboard-in-r/index.html#sample-dashboard-2",
    "href": "posts/2022-03-10-creating-dashboard-in-r/index.html#sample-dashboard-2",
    "title": "Creating Dashboards in R",
    "section": "Sample Dashboard 2",
    "text": "Sample Dashboard 2\nFor the second dashboard, I used the row layout. The process is that same with no additional complications. The dashboard features an interactive Leaflet plot, an interactive histogram and data table using the GT package. The table was transformed with Pivot_Wider function to better fill the space."
  },
  {
    "objectID": "posts/2022-03-10-creating-dashboard-in-r/index.html#conclusions",
    "href": "posts/2022-03-10-creating-dashboard-in-r/index.html#conclusions",
    "title": "Creating Dashboards in R",
    "section": "Conclusions",
    "text": "Conclusions\nThe Flexdashboard package can be used to create nice looking dashboards with a great level of control. The plots can also include interactive elements. When compared to PowerBi or Tableau, there remains one major deficiency. These other dashboards contain a smart interactive filter which ties all the plots together. If you select a specific element in one plot for filtering, all other plots have the same filter applied to them. This is a major boon for understanding data and not a simple feature to develop in Flexdashboard. It remains an interesting package, but I would still rely on PowerBI or Tableau to create dashboards.\n\nPhoto by Luke Chesser on Unsplash"
  },
  {
    "objectID": "posts/2022-03-20-simple-neural-networks-in-python/index.html",
    "href": "posts/2022-03-20-simple-neural-networks-in-python/index.html",
    "title": "Simple Neural Networks in Python",
    "section": "",
    "text": "Neural Networks (NN) have become incredibly popular due to their high level of accuracy. The creation of a NN can be complicated and have a high level of customization. I wanted to explore just the simplest NN that you could create. A framework as a workhorse for developing new NN.\nThe SciKitlearn provides the easiest solution with the Multi-Layer Perceptron series of functions. It doesn’t provide a bunch of the more advanced features of TensorFlow, like GPU support, but that is not what I’m looking for."
  },
  {
    "objectID": "posts/2022-03-20-simple-neural-networks-in-python/index.html#initialization",
    "href": "posts/2022-03-20-simple-neural-networks-in-python/index.html#initialization",
    "title": "Simple Neural Networks in Python",
    "section": "Initialization",
    "text": "Initialization\nFor the demonstration, I decided to use a data set on faults found in steel plates from the OpenML website. The data set includes 27 features with 7 binary predictors.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv('https://www.openml.org/data/get_csv/1592296/php9xWOpn')\n\npredictors = ['V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'Class']\ndf['Class'] -= 1 \n\nSince there are multiple binary predictors, I needed to create a single class variable to represent each class. The Class variable doesn’t currently represent this, it represents all faults that don’t fit in the categories of V28 to V33. The single variable class was created with the np.argmax function which returns the index of the highest value between all the predictors.\n\ny = np.argmax(df[predictors].values, axis =1)\nX = df.drop(predictors, axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
  },
  {
    "objectID": "posts/2022-03-20-simple-neural-networks-in-python/index.html#modelling",
    "href": "posts/2022-03-20-simple-neural-networks-in-python/index.html#modelling",
    "title": "Simple Neural Networks in Python",
    "section": "Modelling",
    "text": "Modelling\nThis is the most basic model that I would like to evaluate. I’ve used the GridSearch function, so all combinations of parameters are tested. The only parameter I wanted to examine was the size of the hidden layers. Each hidden layer provided is a tuple, where each number represents the number of nodes in a singled layer. Multiple numbers represent additional layers.\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100), \n(100,100,100,100), \n(100,100,100,100,100), \n(100,100,100,100,100,100), \n(100,100,100,100,100,100,100),\n(100,100,100,100,100,100,100,100),\n(100,100,100,100,100,100,100,100,100),\n(100,100,100,100,100,100,100,100,100,100)]}\nmodel = MLPClassifier(random_state = 1,max_iter = 10000, \nsolver = 'adam', learning_rate = 'adaptive')\n\ngrid = GridSearchCV(estimator = model, param_grid = parameters)\ngrid.fit(X_train, y_train)\nprint(grid.best_score_)\n\n0.4054982817869416\n\n\nThe performance of the best model in the grid is not impressive. It took me awhile to realize that I had forgotten to scale the features. I included this error to show the importance of scaling on model performance."
  },
  {
    "objectID": "posts/2022-03-20-simple-neural-networks-in-python/index.html#feature-scaling",
    "href": "posts/2022-03-20-simple-neural-networks-in-python/index.html#feature-scaling",
    "title": "Simple Neural Networks in Python",
    "section": "Feature Scaling",
    "text": "Feature Scaling\nThe features are simply scaled with the StandardScaler function. The same model is used on the scaled features.\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nscaler = sc.fit(X_train)\nX_train_sc = scaler.transform(X_train)\nX_test_sc = scaler.transform(X_test)\n\nparameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100), \n(100,100,100,100), \n(100,100,100,100,100), \n(100,100,100,100,100,100), \n(100,100,100,100,100,100,100),\n(100,100,100,100,100,100,100,100),\n(100,100,100,100,100,100,100,100,100),\n(100,100,100,100,100,100,100,100,100,100)]}\nmodel = MLPClassifier(random_state = 1,max_iter = 10000, \nsolver = 'adam', learning_rate = 'adaptive')\n\ngrid = GridSearchCV(estimator = model, param_grid = parameters, cv=3)\ngrid.fit(X_train_sc, y_train)\ngrid.best_score_\n\n0.7553264604810996\n\n\nThe performance of the scaled model is much more impressive. After the GridSearch function finds the parameters for the best model, it retrains the model on the entire dataset. This is because the function utilize cross validation, so some data was withheld for comparing the different models on test data."
  },
  {
    "objectID": "posts/2022-03-20-simple-neural-networks-in-python/index.html#conclusion",
    "href": "posts/2022-03-20-simple-neural-networks-in-python/index.html#conclusion",
    "title": "Simple Neural Networks in Python",
    "section": "Conclusion",
    "text": "Conclusion\nWith our model constructed, we can now test its performance on the original test set. It is important to remember to use the scaled test features, as that is what the model is expecting.\n\ngrid.score(X_test_sc, y_test)\n\n0.7304526748971193\n\n\nThe results are pretty satisfactory. A decent level of accuracy without a lot of complicated code. Default values were used, whenever they were appropriate. Additional steps could be taken, but this remains a good foundation for future exploratory analysis.\n\nPhoto by Alina Grubnyak on Unsplash"
  },
  {
    "objectID": "posts/2022-04-08-benchmarking-data-tables/index.html",
    "href": "posts/2022-04-08-benchmarking-data-tables/index.html",
    "title": "Benchmarking Data Tables",
    "section": "",
    "text": "When I started learning R, I heard vague tales of the use of Data Tables. Really just whisperers, of something to consider in the future after I’ve become more proficient. Well now is the time to learn what if anything I’ve been missing out on."
  },
  {
    "objectID": "posts/2022-04-08-benchmarking-data-tables/index.html#introduction",
    "href": "posts/2022-04-08-benchmarking-data-tables/index.html#introduction",
    "title": "Benchmarking Data Tables",
    "section": "Introduction",
    "text": "Introduction\nData Tables are a potential replacement for the common dataframe. It seeks to perform that same role but with improved performance. I would like to see the speed comparison between Data Frames, Data Tables and Tibbles. I will use the microbenchmark package to perform the actual benchmarking.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(microbenchmark)\nlibrary(farff)\n\nFor the benchmark, I will use the ‘credit-g’ dataset, which can be found on the open ml website. I’m pretty sure the last open ml dataset I used was a csv file, but they seem to have moved to a ARFF format. I will need to use the farff package to load the data.\n\ndf &lt;- farff::readARFF('dataset_31_credit-g.arff')\ndt &lt;- setDT(df)\nti &lt;- tibble(df)"
  },
  {
    "objectID": "posts/2022-04-08-benchmarking-data-tables/index.html#syntax",
    "href": "posts/2022-04-08-benchmarking-data-tables/index.html#syntax",
    "title": "Benchmarking Data Tables",
    "section": "Syntax",
    "text": "Syntax\nThe syntax for Data Tables is a little different:\n\nDT[i,j,by]\n\nIn this manner, a data table can be subset by i, to calculate j when grouped with a by. Along with the special syntax, there are some common functions that add some additional simplification.\n\n.()\n\nThe ‘.()’ function can be used as a placeholder for ‘list()’. The list function is useful for subsetting."
  },
  {
    "objectID": "posts/2022-04-08-benchmarking-data-tables/index.html#grouped-aggregate",
    "href": "posts/2022-04-08-benchmarking-data-tables/index.html#grouped-aggregate",
    "title": "Benchmarking Data Tables",
    "section": "Grouped Aggregate",
    "text": "Grouped Aggregate\nAggregating data in Data Tables is simple by using the j and by parameters in the syntax. Again, multiple functions or even multiple groupings can be passed with the ‘.()’ function. For this comparison, we will look at the performance of finding the average age of the credit holders grouped by the class or credit rating.\n\ngroup &lt;- microbenchmark(Data_Frame = df %&gt;% \n                                 group_by(class) %&gt;%\n                       summarise(avg = mean(age)),\n               Data_Table = dt[,.(avg = mean(age)), by = class],\n               Tibble = ti %&gt;% \n                       group_by(class) %&gt;%\n                       summarise(avg = mean(age)))\nprint(group)\n\nUnit: microseconds\n       expr    min     lq     mean  median      uq     max neval\n Data_Frame 5406.5 5564.6 6138.870 5904.40 6306.95 17135.6   100\n Data_Table  623.6  766.8  941.930  885.95  958.05  7471.7   100\n     Tibble 5602.8 5783.1 6586.445 6103.15 6902.65 14356.9   100"
  },
  {
    "objectID": "posts/2022-04-08-benchmarking-data-tables/index.html#taking-counts",
    "href": "posts/2022-04-08-benchmarking-data-tables/index.html#taking-counts",
    "title": "Benchmarking Data Tables",
    "section": "Taking counts",
    "text": "Taking counts\nAnother function of interest is the ‘.N’ function. This function will return the count of rows. The test looks are the number of people with over 5000 in credit and younger than 35.\n\ncounts &lt;- microbenchmark(Data_Frame = df %&gt;% \n                                 filter(credit_amount &gt; 5000, age &lt;35) %&gt;%\n                       nrow(),\n               Data_Table = dt[credit_amount &gt; 5000 & age &lt; 35, .N ,],\n               Tibble = ti %&gt;% \n                       filter(credit_amount &gt; 5000, age &lt;35) %&gt;%\n                       nrow())\nprint(counts)\n\nUnit: microseconds\n       expr    min      lq      mean  median      uq     max neval\n Data_Frame 8404.1 8584.90  9576.638 8927.45 9867.95 17254.5   100\n Data_Table  276.7  309.50   429.832  451.80  466.35  1045.6   100\n     Tibble 8945.1 9204.75 10206.312 9405.45 9942.45 27081.9   100"
  },
  {
    "objectID": "posts/2022-04-08-benchmarking-data-tables/index.html#creating-new-columns",
    "href": "posts/2022-04-08-benchmarking-data-tables/index.html#creating-new-columns",
    "title": "Benchmarking Data Tables",
    "section": "Creating new columns",
    "text": "Creating new columns\nData Tables also contain a very simple syntax for creating a new column with ‘:=’. I compare this to the tidyverse mutate function. Using the base R to create a column is still the fastest method, taking about half the time of the Data Table method.\n\nnew &lt;- microbenchmark(Data_Frame = df %&gt;% mutate(property = paste(property_magnitude, housing)),\n               Data_Table = dt[,property := paste(property_magnitude, housing)],\n               Tibble = ti %&gt;% mutate(property = paste(property_magnitude, housing)))\nprint(new)\n\nUnit: microseconds\n       expr    min      lq     mean median      uq    max neval\n Data_Frame 2071.3 2148.55 2396.203 2259.6 2586.40 3386.1   100\n Data_Table  505.7  580.00  657.802  656.6  694.35  996.0   100\n     Tibble 2585.3 2754.85 3187.554 2922.9 3331.70 8879.8   100"
  },
  {
    "objectID": "posts/2022-04-08-benchmarking-data-tables/index.html#chaining-data-tables",
    "href": "posts/2022-04-08-benchmarking-data-tables/index.html#chaining-data-tables",
    "title": "Benchmarking Data Tables",
    "section": "Chaining Data Tables",
    "text": "Chaining Data Tables\nAnother point of exploration is that Data Tables can be chained together to create more complicated structures\n\ndt[credit_amount &gt; 1000, .(age = mean(age)),by = .(purpose, class)][class == \"good\" & age &lt; mean(age)]\n\n               purpose class      age\n1:            radio/tv  good 35.44865\n2: furniture/equipment  good 33.21930\n3:            used car  good 36.91860\n4:            business  good 34.50000\n5:  domestic appliance  good 35.50000\n6:          retraining  good 34.00000\n\n\nI don’t think this is the most useful feature, as you can already create some very complicated transformation with a single call. Chaining also makes it more difficult to understand."
  },
  {
    "objectID": "posts/2022-04-08-benchmarking-data-tables/index.html#conclusions",
    "href": "posts/2022-04-08-benchmarking-data-tables/index.html#conclusions",
    "title": "Benchmarking Data Tables",
    "section": "Conclusions",
    "text": "Conclusions\nIt is clear that there are significant performance improvements when using Data Tables versus Data Frames (an average decrease of time by -85%). There are also insignificant differences between Data Frames and Tibbles. Also, the syntax for Data Tables is fairly simple and straight forward and yet extremely powerful.\nSo, to answer the most important question, should you change to Data Tables from Data Frames? Probably, they present a significant performance gain and their structure is very flexible.\n\nPhoto by Tyler Clemmensen on Unsplash"
  },
  {
    "objectID": "posts/2022-04-14-merging-pdfs-with-python/index.html",
    "href": "posts/2022-04-14-merging-pdfs-with-python/index.html",
    "title": "Merging PDFs with Python",
    "section": "",
    "text": "I am currently looking for a new job, which means I need to create many resumes and cover letters. When creating a resume, it is good practice to create a PDF file. PDFs cannot be edited, which can make them difficult to work with, but they retain their formatting. It is impossible to tell which version of Microsoft Word a hiring manager is using. So you have to risk a possible formatting error or create a compatible resumes without the latest features.\nOne issue with using PDFs is that employers will sometimes ask for a cover letter and resume to be submitted as a single PDF. This wouldn’t be an issue if they were both stored in the same document, but if you are like me, you have separate documents creating separate PDFs. You can always use free online PDF mergers, but they can have limitations, and it may not be desirable to give away your personal data. So I decided to create a small Python script that will merge my PDFs together.\nThe script will require the PyPDF2 package for dealing with PDFs and the os package. The os package is just used to automatically merge all PDFs in the root folder.\n\nimport PyPDF2, os\n\nThe first step is to create a list of the PDFs in the current folder. It also ensures that the merged PDF is not in the list.\n\npdfiles = []\n\nfor filename in os.listdir('.'):\n        if filename.endswith('.pdf'):\n                if filename != 'merged.pdf':\n                        pdfiles.append(filename)\n                        \npdfiles.sort(key = str.lower)\n\nThe file list is also sorted alphabetically to ensure the results are predictable and easy to control. The merged PDF will contain the PDFs in the same order.\nThe next step is to create a PdfFileMerger object, which will be the destination for all the data in the PDFs. The first step is to open the first PDF file in the PDF file list. The PdfFileMerger object will only accept a file object, so we need to create a PdfFileReader object from the opened PDF. This PdfFileReader object will then be appended to the PdfFileMerger object. We proceed then to the next PDF. After all files are added, the write method is used on the merged object to create a merged PDF.\n\npdfMerge = PyPDF2.PdfFileMerger()\n\nfor filename in pdfiles:\n        pdfFile = open(filename, 'rb')\n        pdfReader = PyPDF2.PdfFileReader(pdfFile)\n        pdfMerge.append(pdfReader)\n\npdfFile.close()\npdfMerge.write('merged.pdf')\n\nAnd that’s everything, a simple Python script that creates a merged PDF from all PDFs in the root folder. It is important to remember that a PDF file needs to be opened, and then a file object can be created. Using the regular PDF file will not work.\n\nPhoto by krakenimages on Unsplash"
  },
  {
    "objectID": "posts/2022-05-01-level-up-your-progamming-skills/index.en.html",
    "href": "posts/2022-05-01-level-up-your-progamming-skills/index.en.html",
    "title": "Level up your programming skills",
    "section": "",
    "text": "How do you become a better programmer? Well, there is strong scientific evidence for the support of the principle of deliberate practice. Deliberate practice is a method of skill development first written by Anders Ericsson in the book “Peak: Secrets from the New Science of Expertise”. I would also recommend reading “Talent Is Overrated: What Really Separates World-Class Performers from Everybody Else” by Geoff Colvin."
  },
  {
    "objectID": "posts/2022-05-01-level-up-your-progamming-skills/index.en.html#deliberate-practice",
    "href": "posts/2022-05-01-level-up-your-progamming-skills/index.en.html#deliberate-practice",
    "title": "Level up your programming skills",
    "section": "Deliberate Practice",
    "text": "Deliberate Practice\nDeliberate Practice can be summarized to the following points:\n\nTalent is not enough, and to become great at a task requires a lot of practice and repetition.\nDeliberate practice is hard-work, in order strengthen your skills through practice you need to be challenged. This means that repetition by itself will not develop skill. This also means you need to constantly increase the challenge of your practice as you become better at it.\nFocus plays a large role in deliberate practice. This connects to the previous point for the required challenge of practice. This can also tie into the principles of flow, as best described by Mihaly Csikszentmihalyi in his book “Flow: The Psychology of Optimal Experience”.\nSetting goals becomes a powerful motivator. With the completion of a goal, there is a release of endorphins, which cause a sense of satisfaction. Goals are can also be utilized to increase the difficulty of practice, making an otherwise easy task a challenge.\nFeedback is important. Feedback provides motivation by comparing current to previous performance."
  },
  {
    "objectID": "posts/2022-05-01-level-up-your-progamming-skills/index.en.html#application-for-programmers",
    "href": "posts/2022-05-01-level-up-your-progamming-skills/index.en.html#application-for-programmers",
    "title": "Level up your programming skills",
    "section": "Application for programmers",
    "text": "Application for programmers\nSo, how can programmers incorporate the principles of deliberate practice? I’ve recently been recommended the site Hacker Rank and I can say it is fantastic. The Hacker Rank site provides a wide array of challenges for programmers of varying skills levels. There is a selection of different topics from Algorithms to Regular Expressions.\nSo how does Hacker Rank fit in with deliberate practice? Well, there certainly are a good level of challenges to work through. There a three different levels per topic, with challenges in the same difficulty having nothing in common.\nThe design of the site is quite simple, with very little to distract you from your challenge. There is the option for a dark theme if you are, like me, a person of sophistication. It also pretty easy to have the problem on one side of your screen with your programming on the other.\nThere are multiple built-in goals to work on. Certifications for each topic to test your current skill level and to advertise to potential employers. There a preparation kits for interviews with time frames between 1 and 12 weeks, which ever is most convenient for you.\nThere is immediate feedback from assignments, with automated program testing. A leader board is provided for the most competitive, who are interested in their global ranking. You also get feedback from the built-in IDE on your programming errors."
  },
  {
    "objectID": "posts/2022-05-01-level-up-your-progamming-skills/index.en.html#site-criticisms",
    "href": "posts/2022-05-01-level-up-your-progamming-skills/index.en.html#site-criticisms",
    "title": "Level up your programming skills",
    "section": "Site Criticisms",
    "text": "Site Criticisms\nI do enjoy the site, but I still have some minor issues. To its credit, the site does support multiple programming languages, even different versions of the same programming language. This does, however, make it difficult to following along with the tutorials if they are done in a language that you are not familiar with.\nAlso, at this time, there doesn’t seem to be support to retake certification exams for some topics. I myself, had failed the R basic certification as I am more used to using the tidyverse package rather than base R.\nI can’t really speak on behalf of the incorporation of potential employers in it. But that does seem like a very promising idea. I still think it is a great to tool for the programming community, and I will continue to utilize it for my personal skill development, as it can easily provide a source of deliberate practice."
  },
  {
    "objectID": "posts/2022-05-16-the-beauty-of-list-comprehensions-in-python/index.html",
    "href": "posts/2022-05-16-the-beauty-of-list-comprehensions-in-python/index.html",
    "title": "The beauty of List comprehensions in Python",
    "section": "",
    "text": "I have spent awhile learning Python, and I was a little perplexed when it came to list comprehensions. Why would you use them? Isn’t there just an easier why?\nAs my proficiency increase, I have found them to be an incredibly useful tool. They save you lines of code, are easy to understand, and are usually better for performance. A good list comprehension, is truly a work of beauty."
  },
  {
    "objectID": "posts/2022-05-16-the-beauty-of-list-comprehensions-in-python/index.html#structure",
    "href": "posts/2022-05-16-the-beauty-of-list-comprehensions-in-python/index.html#structure",
    "title": "The beauty of List comprehensions in Python",
    "section": "Structure",
    "text": "Structure\nThe basic structure of a list comprehension is pretty simple, you contain an expression and an iterable within a set of []. Depending on the type of brackets used, you can create a list, a generator, set or a dictionary.\n\n[i for i in range(5)]\n(i for i in range(5))\n{i for i in range(5)}\n\n{0, 1, 2, 3, 4}\n\n\nIt may appear from first impressions that a list comprehension is a simple one line for loop, but it is much more powerful than that.\n\nConditions\nMuch more complicated lists can be created with an included if statement. The if statement fits right at the end of the statement.\n\n[a for a in range(10) if a % 2 == 0]\n\n[0, 2, 4, 6, 8]\n\n\nBut what if you need to create an even more complicated list, one that requires an else statement along with the if statement. Then the structure of the list comprehension changes a little, the iterable statement is moved to the end.\n\n[a if a % 2 == 0 else 0 for a in range(10)]\n\n[0, 0, 2, 0, 4, 0, 6, 0, 8, 0]\n\n\n\n\nExpressions\nOf course, expressions can be more complicated than returning single values. One common issue I find is when I have a list of a value type and I need them to be of a different type. This conversion is easy with list comprehensions.\n\na = ['0', '1', '2', '3', '4']\n[int(x) for x in a]\n\n[0, 1, 2, 3, 4]\n\n\nThere is nearly an unlimited potential of different expressions you can use.\n\n\nMore Iterables\nList comprehensions are not limited to a single iterable. Far warning, however, increasing the number of iterables will reduce readability. At some level of complication, it will be a better idea to separate steps.\n\na = range(5)\nb = [5,10,15]\n[x*y for x in a for y in b]\n\n[0, 0, 0, 5, 10, 15, 10, 20, 30, 15, 30, 45, 20, 40, 60]\n\n\nThe results are an element-wise evaluation across multiple iterables. These iterables don’t need to be the same size.\n\n\nDictionary Comprehensions\nAs previously mentioned, by changing the structure, we can generate dictionaries.\n\n{char : num for num, char in enumerate(['a','b','c','d','e'])}\n\n{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4}\n\n\nLikewise, you can create a set rather than a list. Sets can be useful if you don’t need the data to be in order, and you don’t want any duplicate values.\n\n\nOther Applications\nThere is great potential in list comprehensions. Often I find that I need to create a list of zeroes or of boolean logic of the same size as a current list. This is easy to create, just don’t refer to the iterable within the expression.\n\na = range(5)\n[True for x in a]\n\n[True, True, True, True, True]\n\n\nWhile it may not be best practice, you can nest a list comprehension within another list comprehension.\n\n[x for x in [b for b in range(20) if b %2 == 0] if x %3 == 0]\n\n[0, 6, 12, 18]\n\n\n\n\nConclusions\nHopefully I have won you over with the beauty of list comprehensions. They are simple and clean to create yet extremely flexible in their design. So take a minute, to really appreciate the beauty of list comprehensions.\n\nPhoto by Kelly Sikkema on Unsplash"
  },
  {
    "objectID": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html",
    "href": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html",
    "title": "Webscraping in R with Rvest",
    "section": "",
    "text": "Web scraping has become an incredibly important tool in data science, as an easy way to generate new data. The main advantage is the automation of some pretty repetitive tasks. Web scrapping can also be a good way of keeping up with new data on a website, assuming it doesn’t have a big change in its HTML structure."
  },
  {
    "objectID": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#introduction",
    "href": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#introduction",
    "title": "Webscraping in R with Rvest",
    "section": "Introduction",
    "text": "Introduction\nThis project is inspired from a YouTube video created by Thu Vu about at data scraping project about the Witcher books series. Her project utilizes python and Selenium. I love the book series and I loved the project idea. I’ve also had it on my backlog to learn the Rvest library for a while, so it seems like a great opportunity to combine these two interests.\nRather than completing the project on the Witcher series, I thought it would be interesting to explore another book series that I love in the Stormlight Archive by Brandon Sanderson. If you are not familiar with the series, it is an epic fantasy story sprawling over four main books at the time of the publishing of this post. Sanderson is a fantastic author and I feel that the Stormlight Archive is his best work.\nFor this project, I will scrap the Coppermind website for all the character names in the series. The Coppermind is a fan made Wiki site that covers the work of Brandon Sanderson. After retrieving all the character names, I will create a graph outlining the relationships between each character. This work will be done in a future post, so please look forward to it."
  },
  {
    "objectID": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#inializaton",
    "href": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#inializaton",
    "title": "Webscraping in R with Rvest",
    "section": "Inializaton",
    "text": "Inializaton\nThe first step is to download the Rvest library. This is done with the following code.\n\ninstall.packages(\"rvest\")\n\nThe rvest package is also installed if you have the tidyverse package installed. Loading the tidyverse package however will not load the rvest package, so they both need to be loaded separately.\n\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")"
  },
  {
    "objectID": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#datascraping",
    "href": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#datascraping",
    "title": "Webscraping in R with Rvest",
    "section": "Datascraping",
    "text": "Datascraping\nTo start the data scraping exercise, we need to save the URL of the website we would like to scrape. This is the URL for the character page for the Stormlight Archives series.\n\nsite &lt;- read_html(\"https://coppermind.net/wiki/Category:Rosharans\")\n\nWhile on the website in your own browser, you right-click on the specific element you’re interested in scrapping and select inspect. This is at least the method used for Firefox, but it should be similar to other browsers.\nFrom here, you have to do a little digging and a little experimentation to determine which HTML elements are important for the character list. It is pretty useful to have a strong understanding of HTML at this point. From my experimentation, I found that the list was contained within a div with the class “mw-category-group”. A div is a generic divider tag in HTML and can represent many things. I selected the elements with the following code:\n\nnames &lt;- site %&gt;% \n        html_elements(\"div.mw-category-group\")\n\nYou use the html_elements command to select the all the elements for a specific HTML tag you pass. The addition of the “.mw-category-group”, specifies the selection to only divs with the specific class. The class is an attribute of the HTML tags, used to identify and group HTML elements together. I have found that this notation is the best way to filter elements.\nWithin the div elements, there is a further sub-structure for an element in the character list. The characters are contained within an &lt;li&gt; tag as a list item and as an &lt;a&gt; tag as a hyperlink within that list item. We can explore further into the HTML structure by selecting these elements. After the final structure is selected, we can use the html_attr function to return an attribute of the selected elements. The ‘title’ attribute stores the character name in the HTML. We could also the html_text2 function to return the text of the hyperlink, but I’ve found that the ‘title’ attribute is better structured.\n\nnames &lt;- names %&gt;%\n        html_elements(\"li\") %&gt;%\n        html_elements(\"a\") %&gt;%\n        html_attr(\"title\")"
  },
  {
    "objectID": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#data-cleaning",
    "href": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#data-cleaning",
    "title": "Webscraping in R with Rvest",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nWe can start exploring the results of the scrapping\n\nprint(head(names))\n\n[1] \"Category:Aimians\"    \"Category:Alethi\"     \"Category:Azish\"     \n[4] \"Category:Emuli\"      \"Category:Herdazians\" \"Category:Iriali\"    \n\n\nOops, the program has captured an additional list that precedes the character list. Through my testing, I have not found a way to distinguish between the two lists from the HTML structure. Thankfully, we can rely on Regular Expressions to complete the job. The unwanted list items all start with “Category:”, so with a single expression of the str_starts from the stringr package we can remove these elements.\n\nnames &lt;- names[!str_starts(names, \"Category:\")]\n\nThe list still requires some additional work, as there are “()” used throughout the list to give additional context. These “()” will not appear in the text, so we need to remove them with a second Regular Expression. Although it is not clear to me, the “(” needs to be double escaped with two “\\” rather than just one.\n\nnames &lt;- str_remove_all(names,\" \\\\(.*\\\\)\")\n\nprint(head(names, 10))\n\n [1] \"Abaray\"        \"Abiajan\"       \"Abrial\"        \"Abrobadar\"    \n [5] \"Abronai\"       \"Abry\"          \"Acis\"          \"Adin\"         \n [9] \"Adis\"          \"Adolin Kholin\"\n\n\nWe can see that the scrapped data is in much better condition. There is still additional work we can do, as the names will sometimes include first and last names. The last names are not particularly important, so we can drop them altogether.\n\nnames &lt;- str_remove_all(names,\" .*\") \n\nprint(head(names, 10))\n\n [1] \"Abaray\"    \"Abiajan\"   \"Abrial\"    \"Abrobadar\" \"Abronai\"   \"Abry\"     \n [7] \"Acis\"      \"Adin\"      \"Adis\"      \"Adolin\"   \n\nnames &lt;- names[!names %in% c(\"Word\",\"She\", \"User:Thurin\")]\n\nNow we can see that the final list is in condition that we can use to better explore the relationships."
  },
  {
    "objectID": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#conclusion",
    "href": "posts/2022-06-22-webscraping-in-r-with-rvest/index.html#conclusion",
    "title": "Webscraping in R with Rvest",
    "section": "Conclusion",
    "text": "Conclusion\nWe have scraped the Stormlight Archive character wiki website with the rvest package. We loaded the website with read_html function. Furthermore, we were then able to sort through the different HTML elements with the html_elements to find where the character list is stored. We then obtained the actual names with the html_attr function. The data collected still contained some unwanted data. We were able to remove an additional list, data in parentheses and the last names of all characters. We can now move forward with scrapping the books to identify the strength of relationships between each character.\nPhoto by Paz Arando on Unsplash"
  },
  {
    "objectID": "posts/2022-07-12-network-graphs-in-r/index.en.html#introduction",
    "href": "posts/2022-07-12-network-graphs-in-r/index.en.html#introduction",
    "title": "Network Graphs in R",
    "section": "Introduction",
    "text": "Introduction\nNetwork graphs are an important tool for network analysis. They illustrate points, referred to as nodes, with connecting lines, referred to as edges. Since network graphs are such useful tools, there are many options for graph generation. In this posting, I will demonstrate three different techniques for developing network graphs in r.\nThis is part 3 of a series which is based on the Stormlight Archive by Brandon Sanderson. This project was originally inspired by the work of Thu Vu where she created a network mapping of the characters in the Witcher series.\nIn the first part of the project, we scrapped the Coopermind website to create a verified character name list. This scrapping was performed with the rvest package. The list was then cleaned up and saved for further use.\nFor the second part of the project, we read through and analyzed the four books that make up the Stormlight Archive series. The books were read into memory with the readtext package, which fed nicely into the quanteda to create the body of text called a Corpus. Unfortunately, the body of text was so big that we were unable to model all the text, so we divided the Corpus up into smaller documents with the rainette package.\nWith the corpus finally prepped, we feed it into the spacyr package, a frontend for the spaCy python library, to identify the entities. We were able to create a table identifying the entities that were people and filter it by the verified character list. We created a moving window model that would create a connection between two named characters if they were both mentioned within the same window. By aggregating the results of this model, we developed the foundation for a network graph."
  },
  {
    "objectID": "posts/2022-07-12-network-graphs-in-r/index.en.html#initialization",
    "href": "posts/2022-07-12-network-graphs-in-r/index.en.html#initialization",
    "title": "Network Graphs in R",
    "section": "Initialization",
    "text": "Initialization\nThe first step of this process is to load in the necessary packages for the graph generation. The Tidyverse package is always useful for analysis, so I’ve loaded it too. I have read that the different graph packages can interrupt each other, requiring one of them to be loaded at a time. I have not found this to be an issue.\n\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(networkD3)\n\nThe next step is to load in the data that we created in part two of the project. This data represents that relationship between all the verified characters as read through the series of books. Saving and loading data in RDS format is much more convenient than the CSV format, as RDS files are compressed and seem to load faster.\n\ndata &lt;- read_rds(\"StormGraph.RDS\")"
  },
  {
    "objectID": "posts/2022-07-12-network-graphs-in-r/index.en.html#igraph",
    "href": "posts/2022-07-12-network-graphs-in-r/index.en.html#igraph",
    "title": "Network Graphs in R",
    "section": "IGraph",
    "text": "IGraph\nThe first package to explore is the igraph package. This package is not only for plotting graphs, but also includes many tools for network analysis. For our data, we can create a simple network graph with the graph_from_data_frame function. The relationships are not directional, so we pass this information to the function. The graph can then be plotted with the plot function.\n\ngraph &lt;- graph_from_data_frame(data, directed = FALSE)\nplot(graph)\n\n\n\n\nThe graph created is a mess. There are way too many character nodes and way too many relationships created. We need to create a smaller dataset to reduce the amount of information. I reduced the size of the data by taking only the top 98% quantile in relationships. Since the data is stored as a data table, the data table notation is used to create a subset.\n\ndata2 &lt;- data[data$N &gt;= quantile(data$N, p = 0.98),,]\ndata2 %&gt;%\n        graph_from_data_frame(directed = FALSE) %&gt;%\n        plot(layout = layout_with_graphopt)\n\n\n\n\nThe plot created is still difficult to understand, but it much more reasonable. I feel the igraph package is best for graph analysis and exploratory plots. For a more attractive plot, we need to move on to the next package."
  },
  {
    "objectID": "posts/2022-07-12-network-graphs-in-r/index.en.html#tidygraph-and-ggraph",
    "href": "posts/2022-07-12-network-graphs-in-r/index.en.html#tidygraph-and-ggraph",
    "title": "Network Graphs in R",
    "section": "Tidygraph and GGraph",
    "text": "Tidygraph and GGraph\nThe tidygraph and ggraph packages seek to create graphs in the tidyverse-like environment.\n\nlibrary(tidygraph)\n\nCreating a graph with ggraph requires more structure than the previous igraph. The graph requires two data frames, one for nodes and one for edges.\nFor the nodes dataframe, we need a list of all the node names and an ID number for each node. This is achieved by finding the unique values within both columns of data. These values are then passed to the tibble function to create a tibble, a data structure similar to data frames, and then a column for IDs is created with the rowid_to_column function.\n\nnodes &lt;- c(data2$Person1, data2$Person2) %&gt;% \n        unique() %&gt;%\n        tibble(label = .) %&gt;%\n        rowid_to_column(\"id\")\n\nFor the edges dataframe, we need some additional steps. As a reminder, in our subset of data, we have rows with two names and a number to represent the strength of their bond. The character names need to in the form of the node IDs rather than the names. This task is completed with two merges with the node dataframe. The graph can then be created with the tbl_graph function.\n\nedges &lt;- data2 %&gt;%\n        left_join(nodes, by = c(\"Person1\"=\"label\")) %&gt;%\n        rename(from = \"id\") %&gt;%\n        left_join(nodes, by = c(\"Person2\"=\"label\")) %&gt;%\n        rename(\"to\" = \"id\") %&gt;%\n        select(from, to, N)\n\ngraph_tidy &lt;- tbl_graph(nodes = nodes, edges = edges, directed = FALSE)\n\nFor the plotting of the graph, we use the ggraph library. With this package, the graph can act as any other ggplot geom. With an extra step, we can create a centrality feature in our graph. There are a bunch of different centrality measures, but they all represent the level of importance of a node.\n\ngraph_tidy %&gt;%\n        mutate(Centrality = centrality_authority()) %&gt;%\n        ggraph(layout = \"graphopt\") + \n        geom_node_point(aes(size=Centrality, colour = label), show.legend = FALSE) +\n        geom_edge_link(aes(width = N), alpha = 0.8, show.legend = FALSE) + \n        scale_edge_width(range = c(0.2, 2)) +\n        geom_node_text(aes(label = label), repel = TRUE)"
  },
  {
    "objectID": "posts/2022-07-12-network-graphs-in-r/index.en.html#network-d3",
    "href": "posts/2022-07-12-network-graphs-in-r/index.en.html#network-d3",
    "title": "Network Graphs in R",
    "section": "Network D3",
    "text": "Network D3\nThe ggraph has created a better looking plot with a much higher level of customization. It is however a static plot with no level of interaction. I have tried using the ggplotly function from the plotly package it make it more interactive, but many of the ggraph features are not supported.\nTo create an interactive plot, we move to the networkD3 package. This package is based on the D3 JavaScript library to create interactive plots. We can use the same nodes and edges data frames from the ggraph plot. This process does require one adjustment to the node IDs, as the package requires an initial ID of 0 rather than the default r index of 1.\nThe function from the tidygraph, centrality_authority, is only supported for the tidygraph data structure, so we need an alternative function to use with our data frame. This is achieved with the authority.score function from the igraph package. Besides that, we normalize the edge width values, node sizes and set all the parameters for the forceNetwork function.\n\nedges &lt;- edges %&gt;%\n        mutate(from = from -1, to = to - 1) %&gt;%\n        mutate(N = N / 200)\n\nnodes &lt;- nodes %&gt;%\n        mutate(id=id-1) %&gt;%\n        mutate(nodesize = authority.score(graph_tidy)$vector*150)\n        \nforceNetwork(Links = edges, Nodes = nodes, Source = \"from\", Target = \"to\", NodeID = \"label\", Group = \"id\", opacity = 1, fontSize = 14, zoom = TRUE, Value = \"N\", Nodesize = \"nodesize\", opacityNoHover = TRUE)"
  },
  {
    "objectID": "posts/2022-08-15-Creating a Quarto Blog Template/index.html#data",
    "href": "posts/2022-08-15-Creating a Quarto Blog Template/index.html#data",
    "title": "Creating Posts for Quarto Blog",
    "section": "Data",
    "text": "Data\nThe first step is the creation of the data that will be passed to create the blog post. This data can be in the form of a Data Frame or a list. This list will contain all the dynamic parameters, including the user input. The only values that I wanted to get user input from was the title. This value is set to variable names which will be passed into the function later. You can also include calculated parameters, such as sys.date() to get the current date.\n\n\nCode\ndata &lt;- list(title = title,\n               author = 'Mark Edney',\n               date = Sys.Date(),\n               categories = list(\"How-to\", \"R\", \"Rmarkdown\"),\n               draft = 'true',\n               description = \"''\",\n               image = \"''\",\n               archives = format(date, \"%Y/%m\"),\n               toc = 'false',\n               fold = \"show\",\n               tools = 'true',\n               link =  'false')"
  },
  {
    "objectID": "posts/2022-08-15-Creating a Quarto Blog Template/index.html#template",
    "href": "posts/2022-08-15-Creating a Quarto Blog Template/index.html#template",
    "title": "Creating Posts for Quarto Blog",
    "section": "Template",
    "text": "Template\nThe creation of the template is pretty easy, you create a character with the desired structure. Again, you include the parameters in the template with the ‘{{}}’ brackets. I also included markdown for an Introduction and Conclusion, as they should probably be included in every new post.\n\n\nCode\nTemplate &lt;- '---\ntitle: {{title}}\nauthor: {{author}}\ndate: {{date}}\ncategories: [{{categories}}]\ndraft: {{draft}}\ndescription: {{description}}\nimage: {{image}}\narchives:\n  - {{archives}}\ntoc: {{toc}}\n\nformat:\n  html: \n    code-fold: {{fold}}\n    code-tools: {{tools}}\n---\n\n# Introduction\n\n# Conclusion\n'"
  },
  {
    "objectID": "posts/2022-08-15-Creating a Quarto Blog Template/index.html#combining-function",
    "href": "posts/2022-08-15-Creating a Quarto Blog Template/index.html#combining-function",
    "title": "Creating Posts for Quarto Blog",
    "section": "Combining Function",
    "text": "Combining Function\nFinally, we create a single function that will tie everything together. By using the dir.create and paste0 functions with the date from the data list, we create a new folder that has the date and the blog post title in its structure. We can then create the blog post with the whisker::render and writeLines functions. The same folder name is required to create the post in the folder.\n\n\nCode\nBlog_post &lt;- function(title){\n  data &lt;- list(...\n  \n  Template &lt;- '---\ntitle: {{title}}...\n\n  dir.create(paste0(\"./posts/\",data$date, \"-\", title))\n  writeLines(whisker.render(Template, data),\n  paste0(\"./posts/\",data$date, \"-\", title, \"/index.qmd\"))\n}\n\n\nThe function can be run by its own, saved to an r script file, or you can add it to your .Rprofile file so that it will be loaded every time you start R Studio. Then to create a post, you would use Blog_post and pass the blog title. If you’re in the root file for your quarto blog, you shouldn’t have an issue. If you are not, you might get an error saying you could not find the directory.\nSo creating a function is a pretty code solution and whisker is simple and easy to use. The one pain point for me is that the function needs to be loaded in the system by running an open r script file, using the source function and passing the r script name, or by including the function in the .Rprofile. None of these seem very intuitive to me, so I decided to proceed with the creation of an Addin."
  },
  {
    "objectID": "posts/2022-08-15-Creating a Quarto Blog Template/index.html#package-creation",
    "href": "posts/2022-08-15-Creating a Quarto Blog Template/index.html#package-creation",
    "title": "Creating Posts for Quarto Blog",
    "section": "Package creation",
    "text": "Package creation\nThe easiest way to create a package is from the template when you create a new project. This will create the basic structure needed for a package. You need to edit the description file to include information on the package. Here is the description for the package I have created:\n\n\nCode\nPackage: Qpostr\nType: Package\nTitle: Create a Quarto Blog Post\nVersion: 0.1.0\nAuthor: Mark Edney\nMaintainer: Mark Edney &lt;m2edney@gmail.com&gt;\nDescription: This package is used to create an addin for creating a new Quarto blog post. This post is created using a default template using the whisker pakage. A Shiny gadget is utilized to get user input prior to the creataion of the blog post. \nLicense: Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License\nEncoding: UTF-8\nLazyData: true\n\n\nUnder the man folder, you need to create a Rd file for each function that will be included in your package. These Rd files will represent the help information when you use the help search for your function. This file will require the function name, an alias, a title for the help page, an example of it usage and a description. Here is a sample for the function that I have created.\n\n\nCode\n\\name{QGadget}\n\\alias{}\n\\title{Create a Qaurto Blog Post}\n\\usage{\nQgadget()\n}\n\\description{\nCreates a Quarto Blog Post in the posts directory from a template. Opens a Shiny Gadget to get user input before creating the post. Gadget is required for creating the addin.\n}\n\n\nTo create the addin, we need some additional folders. From the main project directory, we need to create an ‘inst’ folder and an ‘rstudio’ folder in that. In the new ‘rstudio’ folder, we need to create a file called ‘addins.dcf’. This file will contain about the addin. Here is a sample of mine:\n\n\nCode\nName: Quarto Blog Post\nDescription: Creates a new Quarto Blog post from template\nBinding: QGadget\nInteractive: true\n\n\nIn this file, the binding refers to the function name that will be connected to the addin. The interactive feature determines whether you would like the code to just run or if you like the user to interact first. We keep all the r scripts in the ‘R’ folder from the project main directory. If you are satisfied with the addin being non-interactive, you can stop there and build the package from the build tab. This is a tab near the environment and history tabs. Since I am not satisfied with a non-interactive addin, we need to create a Shiny Gadget and run that as the function."
  },
  {
    "objectID": "posts/2022-08-15-Creating a Quarto Blog Template/index.html#shiny-gadgets",
    "href": "posts/2022-08-15-Creating a Quarto Blog Template/index.html#shiny-gadgets",
    "title": "Creating Posts for Quarto Blog",
    "section": "Shiny Gadgets",
    "text": "Shiny Gadgets\nA shiny gadget is very similar to a shiny app with the UI and Server functions both in the same file. In an R script file, within the R folder under the project directory, we define a function, which will use functions from the shiny and miniUI libraries.\n\n\nCode\nQGadget &lt;- function() {....}\n\n\nSince we are using the miniUI, there are a different series of UI functions we can use. The following creates a basic UI that will ask the user for a title and for an Author name.\n\n\nCode\nui &lt;- miniUI::miniPage(\n  miniUI::gadgetTitleBar(\"Quarto Blog Post\"),\n  miniUI::miniContentPanel(\n    shiny::textInput(\"title\", \"Title\", placeholder = \"Post Title\"),\n    shiny::textInput(\"author\", \"Author\", placeholder = \"\")\n    )\n  )\n\n\nThe server function just uses the observeEvent function to accept the information that the user submitted. The user data is than submitted to our previous Blog_post function to create a new blog post.\nAdditional the Shiny Gadget uses the runGadget function to start the Gadget. The dialogViewer function is passed to create a new window for the gadget. The default behaviour would be to run in the viewer panel.\n\n\nCode\nserver &lt;- function(input, output, session) {\n  \n  shiny::observeEvent(input$done, {\n    Blog_post(input$title, input$author)\n      stopApp(\"Post Created\")\n      })\n  }\n\nshiny::runGadget(ui, server, viewer = shiny::dialogViewer(\"Quarto Blog Post\"))\n\n\nAgain the previous function that we create to make a new Quarto blog post. This function is saved in the same rscript file as the Shiny Gadget, but not in the gadget function itself.\n\n\nCode\nBlog_post &lt;- function(title, author){\n  data &lt;- list(title = title,\n               author = author,\n               date = Sys.Date(),\n               categories = list(\"How-to\", \"R\", \"Rmarkdown\"),\n               draft = 'true',\n               description = \"''\",\n               image = \"''\",\n               archives = format(date, \"%Y/%m\"),\n               toc = 'false',\n               fold = \"show\",\n               tools = 'true',\n               link =  'false')\n\n  Template &lt;- '---\ntitle: {{title}}\nauthor: {{author}}\ndate: {{date}}\ncategories: [{{categories}}]\ndraft: {{draft}}\ndescription: {{description}}\nimage: {{image}}\narchives:\n  - {{archives}}\ntoc: {{toc}}\n\nformat:\n  html:\n    code-fold: {{fold}}\n    code-tools: {{tools}}\n---\n\n# Introduction\n\n# Conclusion\n\n'\n\n  dir.create(paste0(\"./posts/\",data$date, \"-\", title))\n  writeLines(whisker::whisker.render(Template, data), paste0(\"./posts/\",data$date, \"-\", title, \"/index.qmd\"))\n  file.edit(paste0(\"./posts/\",data$date, \"-\", title, \"/index.qmd\"))\n}"
  },
  {
    "objectID": "posts/2023-09-12-Custom OpenAI Chatbot Pt1/index.html",
    "href": "posts/2023-09-12-Custom OpenAI Chatbot Pt1/index.html",
    "title": "Custom OpenAI Chatbot Pt1: PDF scanning",
    "section": "",
    "text": "Photo by Levart_Photographer on Unsplash\n\n\n\nIntroduction\nI have recently completed a project at work, the creation of a custom ChatGPT chatbot. I will break the project into two parts, the first part will scan a folder of PDF files into a dataframe and the second part will pass the data to OpenAI API. This entire project was completed in python.\n\n\nProject outline\nPDFs can be easily scanned in python with the pypdf module. It is easily installed and easily run, but I have found that the quality of the scan to be lacking. Pypdf also seems to have some issues with PDFs that created from scanned documents, not directly created from a text document. For this reason, I have found an alternative method.\nThe first step is to convert all the PDFs in a directory to PNG images. This can be achieved with the convert from path function from the pdf2image library with the poppler application. The poppler program can be downloaded here and unzipped into its own directory. There is no Windows version on the poppler site, but I found a repo with a nearly updated Windows version here. You will need to copy the directory path into your code. We can create a for loop to open each PDF file one at a time. It’s important to remember to change the ‘' to’/’ for Windows users when referring to directory positions.\nThe second step is to then scan through the PNG images with OCR. For this task, we can use Tesseract. Tesseract is a Google project that is easy to use. Like Poppler, you will need to download the application separately. You will also need to install the helper python package pytessseract. The Tesseract application can be found here. I have my program to save the data in a CSV file, but you can store it anyway you want. I decided to save each PDF file as a separate CSV file and assigning each row as a different PNG file or page of the PDF. This was to ensure that may data is easily organized\nThe next stages require getting into the Langchain library. These steps will be included in the follow-up to this post as both post, are quite lengthy and each can stand alone.\n\n\nConverting PDF to PNG with Poppler\nAgain, prior to running this code, you will need to install the Poppler Application. You also need to copy the directory to the location of the Poppler bin folder. The rest of this section is pretty simple, I’ve created a loop to go through every filename that ends with ‘.pdf’ in a specific PDF folder. I also save the PNG file with the page number included into the title. If the results from the OCR scans are inaccurate, you can adjust the resolution of the PNG files with the parameter ‘dpi = 300’ passed to the convert from path function. The default value is 100. Fair warning, increasing the resolution will slow down the entire process and can potentially add additional artifacts into the OCR scan.\n\n\nCode\nimport os\nimport pandas as pd\nfrom PIL import Image\nfrom pdf2image import convert_from_path\n\npoppler_path = 'C:/Program Files/poppler-23.08.0/Library/bin'\nfor pdf_file in [f for f in os.listdir('//Desktop/PDF') if f.endswith('.pdf')]:\n  images = convert_from_path(pdf_path = '//Desktop/PDF/' + pdf_file, poppler_path = poppler_path)\n\n  for count, img in enumerate(images):\n    img_name = f\"{pdf_file[:-4]}_page_{count}.png\"\n    img.save('//Desktop/PDF/' + img_name, \"PNG\")\n\n\n\n\nOCR from PNG files\nThe Tesseract application is required for the next stage. Since every PNG from every PDF will need to go through the process, I’ve recreated the first section and included the Tesseract functions into the same loop. I’ve also included a step to delete each PNG file after it has been scanned, since it will no longer be needed. The final stage is to save all the returned data as a CSV file. I have found that it is useful to specify the encoding used in saving the CSV.\n\n\nCode\nimport os\nimport pandas as pd\nfrom PIL import Image\nfrom pdf2image import convert_from_path\nimport pytesseract\n\npoppler_path = 'C:/Program Files/poppler-23.08.0/Library/bin'\npytesseract.pytesseract.tesseract_cmd = '//Tesseract-OCR/tesseract.exe'\n\nfor pdf_file in [f for f in os.listdir('//Desktop/PDF') if f.endswith('.pdf')]:\n  images = convert_from_path(pdf_path = '//Desktop/PDF' + pdf_file, poppler_path = poppler_path)\n  extracted_text = []\n  for count, img in enumerate(images):\n    img_name = f\"{pdf_file[:-4]}_page_{count}.png\"\n    img.save('//Desktop/PDF' + img_name, \"PNG\")\n    \n    extracted_data.append(pytesseract.image_to_string(Image.open('C:/Users/Mark/Desktop/PDF' + img_name)))\n    os.remove('//Desktop/PDF' + img_name)\n    \n  df = pd.DataFrame(extracted_text)\n  df.to_csv('//Desktop/PDF' + pdf_name[:-4] + '.csv', encoding = 'utf-8-sig')\n\n\n\n\nConclusion\nWe are finally able to create a usable CSV file from a OCR scanned PDF file. The first step was to convert the pdf into PNG files with Poppler. Each png is then scanned with Tesseract. And the returned values are stored in a CSV file. By why would you want to go through all the steps in the first place? Well, we will need to proceed with the next post about creating the ChatGPT chatbot."
  },
  {
    "objectID": "posts/Toggl Data.html",
    "href": "posts/Toggl Data.html",
    "title": "The Data Sandbox",
    "section": "",
    "text": "import pandas as pd\nimport os\nimport base64\nimport requests\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n#key = os.environ['Toggl']\n\nstring=key+':api_token'\nheaders={\n    'Authorization':'Basic '+base64.b64encode(string.encode('ascii')).decode('utf-8'),\n    \"Content-Type\": \"application/json\"   \n}\n\nparams = {\n    'since':'2021-01-01',\n    'until':'2021-12-31',\n    'user_agent': 'm2edney@gmail.com',\n    'workspace_id' : \"3112792\"\n}\n\nresponse = requests.get('https://api.track.toggl.com/reports/api/v2/details', headers = headers, params= params)\nmy_json = json.loads(response.content)\ndf = pd.DataFrame(my_json['data'])\n\n\npage_count = int(my_json['total_count']/my_json['per_page']) + (my_json['total_count'] % my_json['per_page'] &gt; 0)\nfor page in range(1, page_count):\n    params['page']= str(page)\n    response = requests.get('https://api.track.toggl.com/reports/api/v2/details', headers = headers, params= params)\n    my_json = json.loads(response.content)\n    df =pd.concat([df, pd.DataFrame(my_json['data'])])\n    \ndf['dur'] = df['dur']/1000/60/60\ndf = df[~df['project'].isnull()]\n\n\nsns.barplot(x='project', y= 'dur', data = df, estimator = 'sum')\n\n&lt;Axes: xlabel='project', ylabel='dur'&gt;\n\n\n\n\n\n\ndf['project'].unique()\n\narray(['Data Science Study', 'ocwa', 'Commute', 'Job Seeking', 'Article',\n       'Walmart'], dtype=object)"
  }
]